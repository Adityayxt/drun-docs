question,answer,source
How to display audit logs?,"Audit logs can be viewed, listed, cleaned, or exported.",
How to set up authentication in user and access control?,APIServer/SDK authentication settings can be performed.,
How to register GProduct resource quota in the workspace?,GProduct resource quotas can be registered in the workspace.,
How to manage resource quotas in a workspace?,"You can create, edit, delete, view, list, and calculate allocated resource quotas.",
How to deal with unpopulated or incorrect license situations?,Missing or wrong cases can be handled in the license.,
How to get the ESN serial number in the license?,The ESN serial number can be obtained in the license.,
What new features are added in Global Management v0.11.0?,"Added the function of adding, deleting, modifying and checking in-site messages, supporting SMTP mail server settings, supporting user role permission management CRUD, workspace -> life cycle management (create/edit/delete/view/list), audit log -> display (view/ List/cleanup settings/export), identity provider -> docking with LDAP -> user/user group synchronization settings and many other functions.",
What issues are fixed in Global Management v0.11.2?,Turn off the resource group binding cluster function and fix the problem of being unable to create a workspace when there is no workspace.,
What new features are added in Global Management v0.12.0?,"The ""Module"" in the resource group has been changed to ""Source"", the SDK provides binding change notifications for Workspace and Resource, complete integration of Insight metrics and hotel tracing, and Keycloak has been changed to ""Source"".",
What aspects have been optimized in Global Management v0.12.1?,"Pure offline packages are automatically built through CI, and GHippo upgrade documentation is optimized.",
What new features are added in Global Management v0.13.0?,"Added support for deploying a national secret gateway in front of DCE 5.0 and using the national secret browser to access DCE 5.0, supporting the ability to include multiple GProducts in one license, and the ability to bind cluster type resources to resource groups.",
What issues are fixed in Global Management v0.13.2?,"The interface has been supplemented with English copy that lacks permission descriptions, and has fixed an issue where errors may be reported due to database encoding when updating database tables.",
What functions have been added to Global Management v0.14.0?,"Added platform settings->Appearance customization->Advanced customization->Login page customization and post-login page customization, and supports keycloak quarkus architecture running in a dual-stack environment.",
What new features are added in Global Management v0.15.0?,"Added functions such as custom role functions (create/edit/delete/view/list), support for GProduct permission points and custom role function docking, and lost cluster resource processing.",
What features have been added to Global Management v0.16.0?,"Added LDAP support for AD, LDAP support for more parameters, support for modifying the background image of the login page, support for modification of the filing information at the bottom of the homepage and login page, access management - access to client API, reverse generation URL, support for adding Path in front, audit log SDK Directly call functions such as AuditServer API.",
What issues are fixed in Global Management v0.16.1?,Fixed the problem of excessive CPU usage caused by an infinite loop and the problem of incomplete export of audit logs.,
Does the report management module support enabling and disabling?,"Yes, each report supports enabling and disabling, and after disabling, the data collected in the past period will be displayed normally.",
What kinds of reports does the report management module provide?,"The report management module provides the following reports: cluster reports, node reports, Pod reports, workspace reports, namespace reports, audit reports and alarm reports.",
"In step six, why do you need to restart the global management Pod?","Because the modified configuration needs to be reloaded to take effect, restarting the global management Pod can make the modified configuration take effect.",
"In step four, how do you edit and save the file?",You can use the `vim` command to edit and save.,
What is the purpose of a reverse proxy server address?,"The reverse proxy server address is the address used to access the DCE global management service through the reverse proxy, which can implement request forwarding and load balancing functions.",
How to set the DCE 5.0 reverse proxy server address?,You can set it through the following steps:\n1. Set environment variables for easy use below. \n2. Update the global management Helm repository. \n3. Back up --set parameters. \n4. Add your reverse proxy address. \n5. Execute `helm upgrade` to make the configuration take effect. \n6. Use `kubectl` to restart the global management Pod to make the configuration take effect.,
What is DCE global management?,"DCE global management is a central control platform that integrates user management, application deployment, global configuration, load balancing and other functions.",
What are resources?,"Resources generally refer to resources created through various sub-modules on the DCE platform, and are specific data for completing authorization. Typically, a resource describes one or more operation objects, and each submodule has its own resource and corresponding resource definition details. Such as cluster, namespace, gateway, etc.",
How to authorize and manage resources in the service grid?,"Users can manage resources in the service grid through the Admin role, or bind resources (Mesh or Mesh-Namespace) to the workspace, so that users or user groups can obtain permissions for resources in the service grid through the workspace and be bound. Resources added to the workspace will be displayed in the resource group of the workspace. Currently, only the service grid module provides resource binding entry.",
What is a service mesh?,The service grid is a sub-module of the DCE platform. It has its own unique resource characteristics. There are two states of bound and unbound in the service grid.,
How to distinguish resources in different states in container management?,"By binding a resource (cluster or Namespace) to a workspace, the resource is divided into two states: bound to the workspace and unbound to the workspace. Resources bound to a workspace will appear in the workspace-resource group.",
What two resource states exist in container management?,"In container management, resources exist in two states: bound and unbound workspaces.",
What is a container management module?,"Container management is a sub-module of the DCE platform and supports three authorization methods: global management mode, sub-module management mode and resource-based management. In the container management module, users can grant the Kpanda Owner role to other users or user groups, or grant corresponding permissions to a user or user group on a resource through the permission management function of the container management module itself, or by assigning the resource (cluster or Namespace) Binding to a workspace inherits the role permissions of the user or user group in the workspace.",
How to filter nodes?,"In the filter list, you can filter nodes according to clusters and health status, or search through the search box.",
What information can be viewed on the node details page?,"On the node details page, you can view the number of ongoing alarms and the trend chart of resource consumption such as CPU, memory, disk, etc.",
Is insight-agent required to be installed in the cluster for node monitoring?,"Yes, insight-agent needs to be installed in the cluster and the application must be running to perform node monitoring.",
What is node monitoring?,"Node monitoring is a method of selecting the current health status of nodes in the cluster and the number of exceptions corresponding to the container group through an overview. On the current node details page, you can view the number of alarms and the trend chart of resource consumption such as CPU, memory, disk, etc. .",
What fields does Insight provide for custom rendering of email subjects?,"Multiple fields can be used for custom rendering, such as `.status`, `.alertgroup`, `.alertname`, `.severity`, `.target_type`, etc.",
"When Insight sends an email, which fields will the subject use to render?",The theme will use commonLabels content to render the template. The default theme is `[{{ .status }}] [{{ .severity }}] Alert: {{ .alertname }}`.,
"In Insight's threshold template, how to determine whether a namespace exists?","You can use the if statement to determine, the example code is `{{if .Labels.namespace }}`.",
What syntax is used for Insight alert notification templates?,Insight alarm notification template uses Go Template syntax.,
What methods of alarm notification does Insight support?,"Insight supports alert notifications through Webhook, Email, and corporate WeChat.",
"Does the form include testing of functions such as alarm rules, alarm templates, and notification methods? Have these features passed the test?","Yes, the table includes manual testing of multiple functions such as alarm rules, alarm templates, and notification methods, and these functions have all passed the test,✅.",
Is there a test to verify the functionality of the instantaneous indicator api fields? What were the results of this test?,"Yes, there is a manual test to verify the function of the instantaneous indicator api field in this form, and the test passed, ✅.",
What are the test results of adding custom indicator collection?,"The test of adding custom indicator collection passed,✅.",
What are the test results in this form?,"The table shows the test results of each test function on OCP4.10 (k8s 1.23.0). ✅ indicates that the test passed, and ❌ indicates that the test failed.",
What Openshift cluster compatibility features does this table test?,"This form tests multiple functions of Openshift cluster compatibility, including collecting and querying indicators, adding custom indicator collection, real-time indicator query, instantaneous indicator query, querying indicators over a period of time, and batch querying the CPU and memory usage of nodes and clusters. rate and many other functions.",
How to load observability module image into Docker or containerd from local?,"After decompressing the tar archive, use the load command of Docker or containerd to load the image file: Docker: docker load -i images.tar; containerd: ctr image import images.tar.",
Please briefly describe the steps for chart-syncer to synchronize images to the image warehouse.,(1) Create the load-image.yaml file and fill in the relevant parameters; (2) Execute the synchronization command charts-syncer sync --config load-image.yaml.,
Please briefly describe the steps to install or upgrade the observability module.,Installation steps: (1) Load the image according to the image warehouse selection method; (2) Use Helm to install. Upgrade steps: (1) Determine the upgrade method; (2) Back up parameters; (3) Execute Helm upgrade command.,
Where can I download the Observability module installation package?,The observability module installation package can be downloaded from the Download Center (../../../download/dce5.md).,
What is the observability module?,"The observability module is a monitoring tool that can conduct real-time monitoring and data collection of the running status of applications, and provides rich indicators and alarm rules.",
Does this application interface with OpenTelemetry and use the OTLP protocol to export tracking data?,"Yes, this application is connected to OpenTelemetry and uses the OTLP protocol to export tracking data.",
What container resource limits are specified in this deployment file?,The deployment file does not specify any container resource limits.,
What mount points are defined in this deployment file?,"Two mount points are defined in this deployment file, namely `kube-api-access-sp2mz` and `opentelemetry-auto-instrumentation`.",
What environment variables are defined in this deployment file?,"Multiple environment variables are defined in this deployment file, including `OTEL_JAVAAGENT_DEBUG`, `OTEL_INSTRUMENTATION_JDBC_ENABLED`, `SPLUNK_PROFILER_ENABLED`, `JAVA_TOOL_OPTIONS`, `OTEL_TRACES_EXPORTER`, `OTEL_EXPORTER_OTLP_ENDPOINT`, `OTEL_EXPORTER_OTLP_TIMEOUT`, `OTEL_TRA CES_SAMPLER`, `OTEL_TRACES_SAMPLER_ARG`, `SPLUNK_TRACE_RESPONSE_HEADER_ENABLED `, `OTEL_SERVICE_NAME`, `OTEL_RESOURCE_ATTRIBUTES_POD_NAME`, `OTEL_RESOURCE_ATTRIBUTES_POD_UID`, `OTEL_RESOURCE_ATTRIBUTES_NODE_NAME` and `OTEL_PROPAGATORS`.",
What is this code used for?,"This code is a Kubernetes deployment file, used to deploy an application with OpenTelemetry automatic instrumentation to a Kubernetes cluster.",
How to collect indicators in Insight?,The recommended way is through servicemonitor or podmonitor. The corresponding object needs to be created and labeled: "operator.insight.io/managed-by": "insight" to be recognized by the Operator.,
How to check if the indicator is working properly?,You can check if your metrics are working properly by visiting http://localhost:8888/metrics.,
How to add custom indicators?,"For Golang applications, you can refer to the implementation of the exposeClusterMetric method. For Java applications, see opentelemetry-java-docs/prometheus.",
How to expose indicator interface in Java application?,"After using the hotel agent to complete the automatic connection of the link, add the environment variable OTEL_METRICS_EXPORTER=prometheus to directly expose JVM related indicators. To expose custom metrics, see opentelemetry-java-docs/prometheus.",
How to expose indicator interface in Golang application?,"Just add the corresponding method to the application, install the relevant dependencies, use OTel SDK to create the initialization function, and then initialize it in main.go.",
Which languages does Indicator Query support for advanced custom queries?,Indicator query supports using Prom,
What are the differences between ordinary queries and advanced queries?,"Ordinary queries provide visual condition selection and result display on the interface, while advanced queries require manual input of Prom",
How to perform advanced query?,"- In the left navigation bar, click `Data Query` -> `Indicator Query`, click the `Advanced Query` tab to switch to the advanced query page. \n- Enter a Prom",
What are the steps for a normal query?,"In the left navigation bar, click `Data Query` -> `Indicator Query`. \n- After selecting the cluster, type, node, and indicator name query conditions, click `Search`.",
Which resources can be queried for indicator data by indicator query?,Indicator query supports querying the indicator data of each resource of the container.,
How to clone a storage volume?,"To clone a storage volume, you need to log in to the DCE 5.0 platform with global service cluster administrator rights, enter the `kpanda-global-cluster` cluster details, select the left navigation `Workload->Stateful Load`, and find the stateful load of `vmstorage` , click `⋮` on the right side of the target, and select `Status->Stop->OK` in the pop-up menu. Then log in to the `master` node of the `kpanda-global-cluster` cluster on the command line and execute the following command to copy the vm-data directory in the vmstorage container and store the indicator information locally:\n```bash\nkubectl cp -n insight-system vmstorage-insight-victoria-metrics-k8s-stack-1:vm-data ./vm-data\n```\nThen log in to the DCE 5.0 platform and enter the `kpanda-global-cluster` cluster details, select the left Side navigation `Container Storage -> Data Volume (PV)`, click `Clone` in the upper right corner, and modify the capacity of the data volume. After deleting the data volume of the previous vmstorage, wait for a while. After the storage volume declaration is bound to the cloned data volume, execute the following command to import the data exported in step 3 into the corresponding container, and then start the previously suspended `vmstorage` ：\n```bash\nkubectl cp -n insight-system ./vm-data vmstorage-insight-victoria-metrics-k8s-stack-1:vm-data\n```",
How to change the disk capacity of vmstorage?,"To change the disk capacity of vmstorage, you need to log in to the DCE 5.0 platform with global service cluster administrator rights, enter the `kpanda-global-cluster` cluster details, select the left navigation `custom resources`, and after finding the custom resource of `vmcluster`, click Switch to the custom resource details page under the `insight-system` namespace, and select `Edit YAML` from the right menu of `insight-victoria-metrics-k8s-stack`. On the edit YAML page, modify according to the legend and click `OK`. Then select the left navigation `Container Storage -> Data Volume Statement (PVC)` again, find the data volume statement bound to vmstorage and confirm that the modification has taken effect. On a certain PVC details page, click the associated storage source (PV), open the data volume details page and click the `Update` button in the upper right corner, modify `Capacity` and click `OK`, wait a moment until the expansion is successful.",
How to enable storage pool expansion?,"To enable storage pool expansion, you need to log in to the DCE 5.0 platform with global service cluster administrator rights, enter the `kpanda-global-cluster` cluster details, select the left navigation `Container Storage -> Data Volume Statement (PVC)`, and find the vmstorage bound After the data volume is declared, click to enter the details page to confirm the storage pool bound to the PVC. Then select `Container Storage -> Storage Pool (SC)` in the left navigation, find `local-path`, click `⋮` on the right side of the target, and select `Edit` in the pop-up menu. Turn on `Expansion` on the edit page and click `OK`.",
What is vmstorge disk expansion?,"This article introduces the method of vmstorge disk expansion, which includes three steps: enabling storage pool expansion, changing the disk capacity of vmstorage, and cloning the storage volume.",
How do I edit or delete a message template?,"Click `︙` on the right side of the list and select `Edit` or `Delete` in the pop-up menu to modify or delete the message template. Please note that templates cannot be restored after deletion, so please proceed with caution.",
Does Observability come with pre-configured message templates?,"Yes, Observability comes pre-configured with message templates. If you need to define the content of the template, please refer to: [Configuring Notification Template](../../best-practice/notifacation-helper.md)",
How to create a message template?,"1. In the left navigation bar, select `Alarm Center` -> `Message Template`, and click the `New Message Template` button. \n2. Fill in the template content.",
What notification objects does observability provide?,"Observability provides different notification objects such as email, corporate WeChat, DingTalk, and Webhook.",
How long does it take for the fluent-bit pod to take effect after restarting?,"After restarting the fluent-bit pod, you need to wait a few seconds to a few minutes for it to take effect.",
How to update insight-agent configuration?,The insight-agent configuration can be updated in the Helm application management page. Please refer to the documentation for specific steps.,
How to enable the collection of k8s audit logs of the working cluster?,"You need to select the cluster that is connected and needs to enable k8s audit log collection, update the insight-agent configuration on the Helm application management page, and then turn on/off the k8s audit log collection button on the interface. When connected to a cluster, you still need to restart the fluent-bit pod after switching to take effect.",
How to confirm whether the k8s audit log of the working cluster is enabled?,"You can execute the command `ls /var/log/kubernetes/audit` to check whether there are audit logs generated in the `/var/log/kubernetes/audit` directory. If there is, it means that the k8s audit log is successfully opened.",
How to enable the k8s audit log of the working cluster?,"When creating a working cluster through DCE 5.0, confirm that the k8s audit log of the cluster is selected as 'true', so that the k8s audit log of the created working cluster is enabled. You can also manually enable it after the creation is completed. Please refer to the documentation for specific steps.",
How to enable/disable the k8s audit log collection function in the connected cluster?,"This can be achieved by selecting the cluster that needs to turn on/off the function, updating the insight-agent configuration on the helm application management page, and turning on/off the button on the page. At the same time, when connected to a cluster, you still need to restart the fluent-bit pod after switching it on and off to take effect.",
How to enable the k8s audit log collection function of the working cluster?,"When creating a working cluster through DCE 5.0, you need to set this button to the enabled state and enable the k8s audit log collection function. Confirm that the k8s audit log of the working cluster is selected as 'true', so that the k8s audit log of the created working cluster is enabled.",
How to turn off the k8s audit log collection function of the management cluster?,"Similar to the enable function, you only need to set the parameter to false in step 4.",
How to enable the k8s audit log collection function of the management cluster?,You can enable the k8s audit log collection function of the management cluster through the following steps:\n1. Add chartmuseum to the helm repo;\n2. Save the current insight-agent helm value;\n3. Get the current version number and update the helm value configuration;\n n4. Update helm value configuration and restart all fluentBit pods under insight-system.,
How to confirm whether k8s audit log is enabled?,You can check whether there are audit logs generated in the `/var/log/kubernetes/audit` directory by executing the `ls /var/log/kubernetes/audit` command on the node.,
What is k8s audit log?,"The k8s audit log refers to the audit log generated by k8s itself, which records the user's operations in Kubernetes. You can monitor and audit operations in Kubernetes by collecting k8s audit logs.",
How to turn off the audit log function of the Kubernetes cluster?,Just remove the relevant commands in `spec.containers.command`.,
How to configure the API server to enable audit logs?,"Open the API server's configuration file kube-apiserver.yaml, and add the following configuration information:\n1. Add the command under `spec.containers.command`:\n```yaml\n--audit-log-maxage=30 \n--audit-log-maxbackup=1\n--audit-log-maxsize=100\n--audit-log-path=/var/log/audit/kube-apiserver-audit.log\n-- audit-policy-file=/etc/kubernetes/audit-policy/apiserver-audit-policy.yaml\n```\n2. Add under `spec.containers.volumeMounts`:\n```yaml\n- mountPath: /var/log/audit\nname: audit-logs\n- mountPath: /etc/kubernetes/audit-policy\nname: audit-policy\n```\n3. Add under `spec.volumes`: \n```yaml\n- hostPath:\npath: /var/log/kubernetes/audit\ntype: """"\nname: audit-logs\n- hostPath:\npath: /etc/kubernetes/audit-policy\ ntype: """"\nname: audit-policy\n```",
In which folder should the audit log policy file be placed?,The audit log Policy file needs to be placed in the `/etc/kubernetes/audit-policy/` folder and named `apiserver-audit-policy.yaml`.,
How to enable the audit log function of the Kubernetes cluster?,1. Prepare the Policy file for the audit log;\n2. Configure the API server and enable the audit log;\n3. Restart and verify.,
What should I do if the user does not bind an email address?,The system will prompt the user that the email address is not bound and needs to contact the administrator to reset the password.,
How to tell if the username is correct?,"After entering the username and clicking ""Submit"", it will automatically determine whether the username is correct and give corresponding prompts.",
What are the requirements for resetting my password?,The requirements for setting a new password are consistent with the password rules when the user was created.,
"If the user has not set up an email address, how to reset the password?",You can only contact the administrator for password reset.,
What should I do if I don’t receive the email one minute after clicking submit?,Click the "Submit" button again to resend the verification email.,
How to reset password?,"First, click ""Forgot Password"" on the login interface, enter your username and submit, then find the password reset email in your mailbox, and follow the on-screen prompts to set a new password.",
"If resource quotas are not set in the shared cluster, do I need to set resource quotas when creating a namespace?","If resource quotas are not set in the shared cluster, you do not need to set resource quotas when creating the namespace.",
How does a workspace administrator unbind the created namespace from the workspace?,Workspace administrators can unbind the created namespace from the workspace through container management or global management. The corresponding resource quota will be released when unbundled.,
How to set CPU requests when a namespace is bound to a workspace?,"When a namespace is bound to a workspace, if the CPU request has been set in the shared cluster, the namespace should be created such that CPU request ≤ CPU request has been set.",
What is the difference between resource groups and shared resources?,"Resource groups and shared resources both come from container management, but cluster binding and sharing to the same workspace will produce two completely different effects. Binding resources enables users/user groups in the workspace to have full management and usage rights for the cluster. Adding shared resources enables users/user groups in the workspace to have usage rights for the cluster resources. These resources can be created when creating a namespace. when used.",
What are resource quotas?,"Resource quota refers to the function of limiting the maximum usage of a user in shared resources by setting functions such as CPU request, CPU limit, memory request, memory limit, total storage request, and storage volume declaration.",
How can Java applications expose JVM monitoring indicators?,"You can use some frameworks or tools to enable Java applications to expose JVM monitoring indicators. For example, you can use Spring Boot Actuator to expose monitoring data. In addition, you can also use some specialized monitoring tools, such as Prometheus, Grafana, etc., to expose monitoring data.",
How to let Insight collect existing JVM indicators?,"You can let Insight collect existing JVM metrics by adding annotations to the workload. Specifically, the following annotations need to be added: \n```yaml\nannatation: \ninsight.opentelemetry.io/metric-scrape: ""true"" # Whether to collect \ninsight.opentelemetry.io/metric-path: ""/"" # Path to collect metrics\ninsight.opentelemetry.io/metric-port: ""9464"" # Port to collect metrics\n```\nAmong them, `insight.opentelemetry.io/metric-scrape` is used to specify whether to collect, `insight.opentelemetry.io/metric-path` is used to specify the path to collect metrics, and `insight.opentelemetry.io/metric-port` is used to specify the port to collect metrics.",
What information can be found through log query?,"Log query can find the corresponding information generated by nodes, events, and workloads. At the same time, log source information and contextual raw data are combined to assist in locating problems.",
How to view log context?,"Click the icon behind a single log to view the log details and context of the log. It also supports downloading the content of the queried log context, and currently supports exporting files in .csv format.",
How to export logs?,"Click `Download` in the upper right corner of the list to download the log content under the corresponding filter conditions. Currently, log query results are supported to be exported to files in .csv format. For performance reasons, the current limit of downloaded logs is 2,000.",
How to query logs?,"In the left navigation bar, select `Data Query` -> `Log Query`. After selecting the query conditions, click `Search` and the log records in chart form will be displayed. The latest log is shown at the top.",
What query types are supported by log query?,"Log query supports log query of nodes, events, and workloads.",
How to confirm the running status of Helm application `Insight` in the global management cluster?,Check the status of the Helm application `Insight` in the console and confirm that it is in the `Running` state.,
What is the login method for native Grafana?,"Click Login in the lower right corner, and use the default username and password (admin/admin) to log in in the pop-up window.",
How to access native Grafana?,"The access address is `http://ip:access port/ui/insight-grafana`, for example: `http://10.6.10.233:30209/ui/insight-grafana`, use the default user name and password (admin/admin ) to log in.",
How to delete a text message group that has been added?,"On the notification list page, click `︙` on the right side of the list to edit or delete the SMS group.",
How to add one or more SMS groups?,"Click `Alarm Center` -> `Notification Configuration` -> `SMS` in the left navigation bar, click `Add SMS Group`, and enter the name, recipient of the SMS, mobile phone number and notification server. The notification server needs to be added and created in `Notification Configuration` -> `Notification Server` in advance. Currently, two cloud servers, Alibaba Cloud and Tencent Cloud, are supported. For specific configuration parameters, please refer to your own cloud server information.",
"How to send test information, edit or delete webhooks?","After the configuration is completed, it will automatically return to the notification list. Click `︙` on the right side of the list and select the corresponding option.",
How to configure Webhook?,"Click `Alarm Center` -> `Notification Configuration` -> `Webhook` in the left navigation bar, click `New Webhook`, and add one or more Webhooks. For Webhook URLs and more configuration methods, please refer to [webhook documentation](https://github.com/webhooksite/webhook.site).",
"How to send test information, edit or delete DingTalk group bots?","After the configuration is completed, it will automatically return to the notification list. Click `︙` on the right side of the list and select the corresponding option.",
How to configure DingTalk group robot?,"Click `Alarm Center` -> `Notification Configuration` -> `DingTalk` in the left navigation bar, click `Add Group Robot`, and add one or more group robots. For the URL of DingTalk group robots, please refer to [DingTalk Official Document: Custom Robot Access](https://open.dingtalk.com/document/robots/custom-robot-access).",
"How to send test information, edit or delete enterprise WeChat group robot?","After the configuration is completed, it will automatically return to the notification list. Click `︙` on the right side of the list and select the corresponding option.",
How to configure an enterprise WeChat group robot?,"Click `Alarm Center` -> `Notification Configuration` -> `Enterprise WeChat` in the left navigation bar, click `Add Group Robot`, and add one or more group robots. For the URL of the enterprise WeChat group robot, please refer to [Enterprise WeChat official document: How to use the group robot](https://developer.work.weixin.qq.com/document/path/91770).",
How to delete a mailbox group that has been added?,"On the notification list page, click `︙` on the right side of the list to edit or delete the mailbox group.",
How to add one or more email addresses as a mailing group?,"After entering `Observability`, click `Alarm Center` -> `Notification Configuration` in the left navigation bar. By default, it is located in the email notification object. Click `Add Email Group` to add one or more email addresses.",
In what ways can messages be sent to users?,"Messages can be sent to users through email, WeChat Business, DingTalk, Webhook, and SMS.",
"In the service grid, can the Workspace Admin and Workspace Viewer roles be bound to workspaces?","No, only the Admin role can bind a workspace.",
"In a service mesh, which roles can perform system upgrades?","The Admin, Workspace Admin, and Workspace Editor roles can all perform system upgrades. The Workspace Viewer does not have this permission.",
"In a service mesh, which roles have sidecar management rights?","All roles have sidecar management permissions, but different roles have different permissions. Specifically, only the Admin, Workspace Admin, and Workspace Editor roles can enable, disable sidecar injection, and set sidecar resources; the Workspace Viewer role does not have this permission.",
Can the Workspace Editor and Workspace Viewer roles delete service entries?,"No, only Admin and Workspace Admin roles can delete service entries.",
"In a service mesh, which roles can edit and delete the mesh?",Both the Admin and Workspace Admin roles can edit and delete grids.,
"In a service mesh, which roles can create meshes?",Only the Admin role can create grids.,
Can roles be customized in service mesh?,"Yes, starting from v0.6.0 of DCE 5.0, the global management module supports configuring custom roles for the service mesh and granting different permissions.",
What user roles does the service mesh support?,"The service grid supports the following user roles: Admin, Workspace Admin, Workspace Editor, and Workspace Viewer.",
What is pinia store?,"pinia store is a state management library that can be used to share state between front-end child applications and parent applications. When connected to the DCE 5.0 parent application Anakin, information such as child product registration information will be included in the pinia store.",
How to connect the front-end application to the DCE 5.0 parent application Anakin?,"Front-end applications can use qiankun to connect to the DCE 5.0 parent application Anakin. For details, see the examples above and qiankun official documentation.",
Where is the navigation bar category of global management configured?,The navigation bar category of global management is configured in ConfigMap.,
How to register container management into navigation bar menu?,"Container management can be registered to the navigation bar menu through GProductNavigator CR, see the example above for details.",
What is container management?,"Container management is a functional item of DCE global management, which is used to manage the life cycle of containerized applications, including creation, deployment, scaling, upgrade and other operations.",
How to perform `helm upgrade` to upgrade the global management version?,"You can execute the following command to upgrade, where `ghippo` is the name of the Helm chart, `ghippo-system` is the namespace of the Helm chart, and `0.9.0` is the global management version number. \n```shell\nhelm upgrade ghippo ghippo/ghippo \\nn ghippo-system \\nf ./bak.yaml \\n--set global.imageRegistry=$imageRegistry \\n--version 0.9.0\n ```",
How to update ghippo crds?,You can execute the following command to update. \n```shell\nkubectl apply -f ./crds\n```,
Which command should be executed to back up the `--set` parameter of the old version before upgrading the global management version?,You should execute the following command to back up the `--set` parameter of the old version. \n```shell\nhelm get values ghippo -n ghippo-system -o yaml > bak.yaml\n```,
How do you choose which global management version you want to install?,"You can execute the following command, where `0.9.0` is the global management version number. \n```shell\nhelm search repo ghippo/ghippo --versions\n```",
How to update the globally managed helm repository?,"You can execute the following command to update, where `ghippo` is the name of the Helm chart and `ghippo-system` is the namespace of the Helm chart. \n```shell\nhelm repo update ghippo\n```",
How to add a globally managed helm repository?,You can execute the following command to add it. \n```shell\nhelm repo add ghippo http://{harbor url}/chartrepo/{project}\n```,
How to check whether the global management helm repository exists?,You can execute the following command and check whether it exists by checking the returned results. \n```shell\nhelm repo list | grep ghippo\n```,
What parameters need to be backed up during upgrade?,It is recommended to back up the `--set` parameter of the old version and execute the following command to back up: helm get values ghippo -n ghippo-system -o yaml > bak.yaml.,
What are the ways to load images?,"There are two ways, one is to synchronize the image to the mirror warehouse through chart-syncer, and the other is to load it directly using Docker or containerd.",
Where can I download the global management module?,The global management module can be downloaded from the Download Center (../../download/dce5.md).,
What should you pay attention to when using OIDC?,"The following matters need to be noted: 1) Users who log in for the first time will not be given any default permissions and require administrator authorization; 2) Users will be synchronized to the `User and Access Control` of DCE 5.0 after completing the first login - > `User List`; 3) For practical operation tutorials, please refer to the special operation demonstration video or refer to relevant documentation.",
How to carry out user identity authentication interaction process?,"The user first initiates a single sign-on request to DCE 5.0. DCE 5.0 constructs an OIDC authorization request based on the information carried in the login link and sends it to the browser. After receiving the request, the browser forwards it to the enterprise IdP, where the user enters the username and password and performs verification. The enterprise IdP builds an ID Token and sends an OIDC authorization response to the browser. After the browser responds, it forwards the OIDC authorization response to DCE 5.0. DCE 5.0 takes out the ID Token from the OIDC authorization response, maps it to a specific user list according to the configured identity conversion rules, and issues the Token. Finally complete the single sign-on and access DCE 5.0.",
How to use OIDC protocol based identity provider functionality in DCE 5.0?,"1) Log in to the web console using a user with the `admin` role; 2) Navigate to `Users and Access Control` under `Global Management`, select `Identity Provider`-> `OIDC`, and click Create Identity Provider ;3) Fill in the form fields, establish a trust relationship with the identity provider, and click Save.",
What is the role of OIDC?,The role of OIDC is to allow enterprises or organizations to use their own account systems and build trust relationships with identity providers based on the OIDC protocol without the need to create usernames and passwords for each organizational member.,
What is OIDC?,OIDC is an identity authentication standard protocol based on the OAuth2 protocol. It is an identity layer used to establish a trust relationship between identity providers and users.,
"If there is no operation after logging in, how long will it take for the user to automatically log out?","If there is no operation after logging in, the user will automatically log out after 2 hours.",
"If the login is successful, which state will the user enter?","If the login is successful, the user will enter the logged in state.",
What information does a user need to enter to log in to the system?,Users need to enter their username and password to log in to the system.,
How does DCE 5.0 assign accounts to users?,DCE 5.0 assigns an account with certain permissions to the user through the administrator creating a new user in `User and Access Control`.,
Why do users need an account that uniquely identifies the user?,Users need an account that uniquely identifies the user to identify themselves in the system and bind their own data.,
What if a user forgets their password and cannot verify their identity via email?,"If the user does not set an email address or sets an incorrect email address, the user with the `admin` role needs to find the user by user name in `Global Management->User and Access Control`, and set a new email address for the user in the user details. login password. If the mail server is not connected, please check whether the address, username and password are correct.",
How to test whether the mail server was successfully set up?,"After the configuration is completed, click `Save`, then click `Test Mail Server`. If a prompt for successfully sending the email appears in the upper right corner of the screen, it means that the email server has been set up successfully.",
How to configure the mail server in DCE 5.0?,"The configuration steps are as follows:\n1. Log in to the Web console using a user with the `admin` role. \n2. Click `Global Management` at the bottom of the left navigation bar. \n3. Click `Platform Settings` and select `Mail Server Settings`. \n4. Fill in the SMTP server address, SMTP server port, user name, password and sender email address, and then click Save and Test Mail Server.",
When does DCE 5.0 use email to authenticate users?,DCE 5.0 uses email to authenticate users when they forget their password.,
How to manage MinIO instances in DCE?,"Users can view all MinIO instances in the MinIO instance list and perform related operations on them, such as: view overview, view monitoring, view configuration parameters, modify configuration parameters, etc. It also supports the creation of new MinIO instances, and the required MinIO instance can be located through the search function. If a MinIO instance is no longer needed, you can also delete the resource to release it.",
What operations can users perform on Redis and Kafka instances in DCE?,"Users can view overviews, view monitoring, view configuration parameters, and modify configuration parameters for Redis and Kafka instances. In addition, you can also search for instance names to locate them and delete unnecessary instances.",
How can users search for instance names in DCE? What operations are supported in the instance list?,"Users can search by entering the instance name or filter the instance list. The instance list supports viewing the list, creating instances, updating instance configurations, and deleting instances.",
What operating permissions does the Workspace Viewer user have in the Elasticsearch module?,"View the list, instance name search, and various operation permissions in Elasticsearch instance details.",
What operating permissions does the Workspace Editor user have in the RabbitMQ module?,"View lists, search by instance name, update instance configuration, and RabbitM",
What operating permissions does the Workspace Admin user have in the MySQL module?,"View lists, search for instance names, create instances, update instance configurations, delete instances, and MyS",
What three user roles does the middleware data service support?,"Workspace Admin, Workspace Editor, Workspace Viewer.",
What six types of middleware does middleware data service support?,MyS,
How to confirm after the update is successful?,"After the update is successful, a prompt message will be displayed, and the latest information will be displayed on the multi-cloud service list page.",
How to perform update operations using forms? What content cannot be modified?,"In the multi-cloud service list, click the `···` button of the service that needs to be updated, select the `Update` option in the pop-up menu, modify the content that needs to be changed in the form, and then click OK. The access type, name, and multi-cloud namespace cannot be modified.",
How to perform update operations by editing YAML?,"In the multi-cloud service list, click the `···` button of the service that needs to be updated, select the `Edit YAML` option, modify the YAML information and click OK to perform the update operation.",
How to enter the multi-cloud service list for update operation?,"In a certain multi-cloud instance, click `Resource Management` -> `Multi-cloud Service` in the left navigation bar to enter the multi-cloud service list for update operations.",
What two update methods are provided by multi-cloud services?,The multi-cloud service provides two methods of editing YAML and updating forms.,
What problems does a multi-cloud orchestration platform solve?,"The multi-cloud orchestration platform solves the centralized management problem of multi-cloud and hybrid cloud, and provides cross-cloud application deployment, release and operation and maintenance capabilities; supports elastic expansion and contraction of applications based on cluster resources to achieve global load balancing; provides fault recovery capabilities, Completely solve the problem of multi-cloud application disaster recovery.",
What capabilities does a multi-cloud orchestration platform provide?,"The multi-cloud orchestration platform provides cross-cloud application deployment, release and operation and maintenance capabilities; supports elastic expansion and contraction of applications based on cluster resources to achieve global load balancing; provides fault recovery capabilities and completely solves the problem of multi-cloud application disaster recovery.",
How to implement load balancing for applications deployed in multi-cloud environments?,The multi-cloud orchestration platform supports elastic scaling of applications based on cluster resources to achieve global load balancing.,
Why do businesses need a multi-cloud environment?,"As technology develops and enterprise needs change, a single public cloud can no longer meet enterprise needs for flexibility, reliability, and security. The multi-cloud environment can meet the needs of enterprises to choose different cloud service providers in different scenarios, thereby building a multi-cloud environment exclusive to the enterprise.",
What is multicloud orchestration?,"Multi-cloud orchestration is an application-centric, out-of-the-box multi-cloud application orchestration platform that provides cross-cloud application deployment, release and operation and maintenance capabilities; supports elastic expansion and contraction of applications based on cluster resources to achieve global load balancing; provides The ability to recover from faults completely solves the problem of multi-cloud application disaster recovery.",
What identities are there in the workspace and their permissions?,"There are three identities in the workspace: WorkspaceAdmin, WorkspaceEdit, and WorkspaceReadOnly. Among them, the permissions of WorkspaceAdmin and WorkspaceEdit are almost equal in multi-cloud. When the corresponding multi-cloud instance management capabilities are added later, the two will be distinguished; while WorkspaceReadOnly is still restricted by read-only permissions in multi-cloud orchestration.",
Can multiple workspaces directly bind the same resource object?,"no. In order to ensure the stability of matching resources and permissions, multiple workspaces are not supported to directly bind the same resource object. If you need to allow other workspaces to use the resource object, you can handle it in the shared resource module of the workspace.",
How are multi-cloud instances or multi-cloud namespaces tied to workspaces?,The super administrator with the multi-cloud orchestration module can bind the corresponding multi-cloud instance or multi-cloud namespace to the corresponding workspace in the form of resources. The management entrance of the workspace can be seen in the multi-cloud instance list.,
What is the main function of a workspace?,"Workspaces can be understood as projects under departments, and administrators map hierarchical relationships in the enterprise through hierarchies and workspaces. It is used for resource management and is combined with the multi-cloud orchestration module to implement multi-cloud orchestration permission transfer.",
What is a workspace?,Workspace is a division design of DCE 5.0 that supports global resource management; it is used to manage resources and consists of two parts: hierarchy and workspace.,
How to check after successful binding?,"Click `Global Management` to enter the workspace and hierarchy page, select the corresponding resource group, and you can see the bound multi-cloud instances/namespaces.",
How to bind multi-cloud namespace and workspace?,"Enter the multi-cloud orchestration module, click the workspace in the upper right corner of the page, select the `Multi-cloud Namespace` tab, select a multi-cloud namespace of a multi-cloud instance in the tab, click `ⵗ` on the right, and select `Bind to Work Space`, select the workspace you want to bind, and click OK.",
How to bind multi-cloud instances and workspaces?,"Enter the multi-cloud orchestration module, click `Workspace` in the upper right corner of the page, select the target instance, click `ⵗ`, select `Bind to workspace`, select the workspace you want to bind, and then click `OK`.",
How many workspaces can a multi-cloud instance/namespace be bound to?,A multi-cloud instance/namespace can only be bound to one workspace.,
What permissions are required to bind multi-cloud instances and multi-cloud namespaces?,The current operating user should have Admin or Workspace Admin permissions.,
What does the YAML editor that comes with the multi-cloud orchestration module automatically detect?,The YAML editor that comes with the multi-cloud orchestration module automatically detects YAML syntax and displays relevant prompts when there are errors.,
"In the YAML example of differentiation strategy, how to add an environment tag to Deployment?","You need to select the target cluster as `demo-dev` or `demo-stage`, then add the label path and value in `overriders.plaintext`, and set the operator to `add`. For example: \n```yaml\n- path: ""/metadata/labels/env""\noperator: add\nvalue: demo-dev\n````",
How do I enter the YAML statement for a deployment policy?,"After creating the YAML statement of the Deployment, click `Next`, and then enter the YAML statement of the deployment policy.",
How to create a Deployment YAML statement?,"In the left navigation bar, click Multi-Cloud Workloads, click the YAML Create button in the upper right corner and enter the code.",
How can you create multi-cloud workloads?,"In addition to creating workloads through images, you can also create them by entering YAML statements.",
Is it possible to update load names and multi-cloud namespaces?,Can't.,
How to update multi-cloud configuration items through a form?,"On the multi-cloud configuration item editing page, click the `Update` button, modify the required content in the form and click OK to update.",
How to update multi-cloud configuration items by editing YAML?,"On the multi-cloud configuration item editing page, click `···` to edit YAML. After modifying the required information, click OK to update.",
How to enter the multi-cloud configuration item editing page?,"After entering a multi-cloud instance, click `Resource Management` -> `Multi-cloud Configuration Items` in the left navigation bar to enter the multi-cloud configuration item editing page.",
What are the two update methods for multi-cloud configuration items?,There are two ways to edit YAML and form update.,
How to enter the multi-cloud stateless load page?,You can click `Multi-cloud workloads' in the left navigation bar to enter the multi-cloud stateless load page.,
How to update a stateless load?,"You can update a stateless load through the following steps:\n1. In the left navigation bar, click `Multi-cloud workload` to enter the multi-cloud stateless load page, click `···` to edit YAML or update;\n2. Click `Edit YAML` can update the stateless load by modifying the YAML information;\n3. Click `Update` to update the stateless load through the form. The load name and multi-cloud namespace cannot be updated;\n4. Click OK after completing the content that needs to be modified. update completed.",
How to update multi-cloud routing using form method?,"In the multi-cloud route list, select the multi-cloud route to be modified, click `Update` in the right operation bar, modify the required content in the pop-up form and click OK.",
What information cannot be modified when the form updates multi-cloud routing?,"When the form updates multi-cloud routing, the name, multi-cloud namespace, and Ingress Class cluster cannot be modified.",
How to update multi-cloud routes by editing YAML?,"In the multi-cloud route list, click on the `···` of the multi-cloud route to be modified, select `Edit YAML` in the pop-up menu, modify the YAML information and click OK to update.",
How to enter the multi-cloud routing list?,"In a certain multi-cloud instance, click `Resource Management` -> `Multi-cloud Routing` in the left navigation bar to enter the multi-cloud routing list.",
What two ways can multi-cloud routing be updated?,Multi-cloud routes can be updated by editing YAML and updating forms.,
How to update the key through form?,"Click `Update` on the edit page, complete the content that needs to be modified in the pop-up window, and click OK.",
What information cannot be changed when updating the key?,"When updating, the name, multi-cloud namespace, and key type cannot be changed.",
How to update the key through YAML in the edit page?,"On the edit page, click `···` and select `Edit YAML`. Modify the YAML information and click OK to update the key.",
How to enter the edit page of the multi-cloud key?,"After entering a certain multi-cloud instance, in the left navigation bar, click `Resource Management` -> `Multi-cloud Key`, click `···` to enter the editing page.",
What two ways can a multi-cloud key be updated?,Edit YAML and form updates.,
"If additional resources need to be created, how can this be done?",Other resources can be created temporarily using `YAML`.,
What types of workloads can multicloud orchestration software support?,"Currently, only the creation of stateless workloads (Deployment) is supported.",
What workloads can multi-cloud orchestration software manage?,Can manage workloads distributed across clusters and managed by region and availability zone.,
"For instance resources, what new data collection APIs have been added?",Added instance's CPU and memory resource data collection API and instance's event query API.,
"For multi-cloud resource creation, which prompt has been added?",This update adds new prompts for multi-cloud resource creation.,
What permission checksum interfaces are enabled for the new multi-cloud orchestration?,"The newly added multi-cloud orchestration enables permission verification and interfaces, including the list instance interface, displaying data based on permissions, querying the labels of all member clusters, and converting single-machine cluster applications into multi-cluster applications with one click.",
What issues were fixed?,"This update fixes a number of issues, including the estimator not adapting to offline installation and the abnormal stateless load display on the instance details page.",
What are the new features in this update?,"This update adds a number of new features, including prometheus metrics, opentelemetry link trace, displaying the corresponding cluster list after creating a workload and specifying the region, displaying the corresponding cluster list after creating the workload and specifying the label, and productizing failover failover.",
What is optimized in the 2022-10-21 version?,This version optimizes the protobuf data structure of karmada PropagationPolicy and OverridePolicy.,
Which problem is fixed in the 2022-11-25 version?,This version fixes the problem that the estimator does not adapt to offline installation and the stateless load display abnormality on the instance details page.,
What are the new features in the 2022-12-25 version?,"This version adds new functions such as cronjob's addition, deletion, modification, and other related interfaces, job's addition, deletion, modification, and other related interfaces, one-click migration of single-cluster applications, automatic upgrade of dependent resources for multi-cluster applications, etc.",
What issues are fixed in the 2023-03-29 version?,"This version fixes the problem that kind is not automatically filled in when the CR resource type is selected in the deployment strategy, and the problem of adjusting the prompt information of the instance deletion pop-up window whether to delete the karmada instance simultaneously, etc.",
What new features are added in the 2023-04-27 version?,"This version adds support for cluster scheduling groups, support for mirror selector selection, new scheduled rescheduling in advanced settings, and cluster health status threshold settings.",
What impact will removing a cluster have on multi-cloud workloads?,"After removing the working cluster, the cluster workload will no longer be managed by multi-cloud orchestration, and all multi-cloud workloads currently distributed to this cluster will be automatically loaded onto other managed clusters.",
How to remove a cluster?,"In the working cluster list, click the `...` button on the right and select `Remove` in the pop-up menu. Enter the name of the cluster in the pop-up window, confirm it is correct and click `Confirm to remove`.",
How are Secrets created?,"Currently, two creation methods are provided: wizard creation and YAML creation.",
What is the difference between Secret and ConfigMap?,"Secret is similar to ConfigMap, but is specifically used to save confidential data, while ConfigMap is used to save non-sensitive configuration information.",
What is Secret?,"A Secret is an object that contains a small amount of sensitive information such as a password, token, or key, and is specifically designed to hold confidential data.",
How to confirm after successful installation?,"After successful installation, you can see in the `Helm Application` list that the status of the insight-agent application has changed from `Not Ready` to `Deployed`, and the status of all components is `Running`. After waiting for a period of time, you can view the data of the cluster in the `Observability` module.",
What information needs to be filled in during installation?,"During installation, you need to select the version and fill in the address of the corresponding data storage component in the global management cluster. The address of the component for data reporting has been filled in by default. If you need to modify it, please refer to the relevant documents.",
What are the prerequisites for installing insight-agent?,"Before installation, you need to ensure that the cluster has successfully accessed the container management module. For specific access methods, please refer to relevant documents.",
Where can I install insight-agent?,"Insight-agent can be installed in the `Container Management` module. The specific steps are: enter the `Container Management` module, click the name of the cluster where you want to install insight-agent in the `Container List`, select `Helm Application -> Helm Template` on the left navigation bar, and query `insight-agent` in the search box , click the card to enter the details, view the Readme of insight-agent, and click `Install` to install.",
What is an insight-agent?,"`insight-agent` is a plug-in for cluster observation data collection, supporting unified observation of indicators, links, and log data.",
Does Insight’s `node-exporter` need to be installed?,"Installation is recommended because Insight's `node exporter` will enable some features to collect special indicators. However, it should be noted that currently it is not supported to modify the port in the installation command. You need to manually modify the related ports of insight node-exporter daemonset and svc after `helm install insight-agent`.",
"When the open source `node-exporter` enables hostnetwork by default, what happens when installing `insight-agent`?","If the existing monitoring system in the cluster has installed `node-exporter`, it will not run properly due to node-exporter port conflict when installing `insight-agent`.",
How to solve the conflict between installing insight-agent in DCE 5.0 and the prometheus operator in dx-insight?,"The solution is: enable the parameters of the prometheus operator, retain the prometheus operator in dx-insight, and be compatible with the prometheus operator of insight-agent in 5.0. The specific steps are as follows:\n1. Log in to the console. \n2. Enable the `--deny-namespaces` parameter in the two prometheus operators respectively. \n3. Execute the following command (the following command is for reference only, the actual prometheus operator name and namespace in the command need to be replaced).",
Why does installing insight-agent in DCE 5.0 conflict with the prometheus operator in dx-insight?,"Because most DCE 4.0 clusters have installed dx-insight as the monitoring system, and there is already a prometheus operator in dx-insight, installing insight-agent at this time will conflict with the existing prometheus operator in the cluster.",
How to mount JMX Exporter through init container?,"You need to make the JMX exporter into a Docker image, add the init container to the Java application deployment Yaml, download jmx_exporter in the init container and copy it to the target path, mount it in the Java container and set the environment variables.",
How to build JMX Exporter JAR files into business images?,"You can find the latest jar package download address on the Github page of jmx_exporter, and refer to the Dockerfile to package it in the business application image.",
How should the jar package files and configuration files of JMX Exporter be packaged?,It is recommended to package the JMX Exporter jar package and configuration files into the business container image.,
Why is the first startup method not recommended?,"The first method is not officially recommended. On the one hand, the configuration is complicated, and on the other hand, it requires a separate process, and the monitoring of this process itself has become a new problem.",
What two usages does JMX-Exporter provide?,JMX-Exporter provides two usages: starting an independent process and starting within a JVM process.,
"If a Java application has exposed JVM indicators, which document can you refer to for docking?","If the Java application has exposed JVM indicators, you can refer to the document Observability of Connecting Java Applications with Existing JVM Indicators for docking.",
What documentation can I refer to if my Java application doesn't start exposing JVM metrics?,You can refer to the following two documents: Exposing JVM monitoring indicators using JMX Exporter and Exposing JVM monitoring indicators using OpenTelemetry Java Agent.,
What is the main content of this document?,"This document mainly introduces how to monitor the JVM of customer Java applications, including how applications that have exposed JVM indicators and applications that have not exposed JVM indicators can interface with Observability Insight.",
Are the Kubectl commands tested in this table?,"The Kubectl command has not been tested in this table, only some of the indicator collection and log query functions have been tested.",
Why is there a remarks column in the table? What information is included in the note?,"The remarks column is to help readers better understand the content of each test, including the specific functions of the test, the scenarios in which it is used, and other information. For example, the note for the test ""Log Fuzzy Query-workloadSearch"" is ""Supports querying logs by workload name.""",
What Kubernetes versions are listed in this table?,"This table lists Kubernetes 1.26, Kubernetes 1.23.0 ~ 1.23.13, Kubernetes 1.24.0 ~ 1.24.7, Kubernetes 1.25.0 ~ 1.25.3, Kubernetes 1.22, Kubernetes 1.21, Kubernetes 1.20, Kubernetes 1.19 and Kubernetes 1 .18 and other versions.",
Which version of Kubernetes in the table passed all tests?,"In this table, Kubernetes version 1.26 passes all tests.",
What is the content of this form? What features does it test?,"This table is the result of the Kubernetes cluster compatibility test. It tested some indicator collection and log query functions, including collecting and querying indicators of web applications, adding custom indicator collection, querying real-time indicators, querying instantaneous indicators, and querying indicators over a period of time. Batch query cluster CPU, memory usage, total node CPU, etc.",
What functions can the billing and metering module help enterprises achieve?,"The billing and metering module can help enterprises determine reasonable charging standards, refine billing, and generate detailed bills.",
What is the role of billing metering?,Billing measurement mainly realizes the measurement and billing of the value of enterprise IT services through the collection and analysis of enterprise IT resources.,
What report types does the report management module provide?,"The report management module provides four types of reports: cluster reports, node reports, audit reports and alarm reports.",
What is the core work of report management?,"The core work of report management is to provide decision-making support for enterprise managers through the collection, integration, analysis and visual display of enterprise IT resources, thereby achieving effective control of enterprise IT assets.",
What two operations management capabilities does DCE 5.0 provide?,DCE 5.0 provides two operation management functions: report management and billing measurement.,
What is the implementation method of unified login/unified AuthN authentication?,"Unified login/unified AuthN authentication is implemented by using the global management (Keycloak) login page, and API authn token verification uses Istio Gateway. After GProduct is connected to global management, there is no need to pay attention to how to implement login and authentication.",
How to connect routing and AuthN?,"The main purpose of interconnecting routing and AuthN is to unify the routing entrance through the globally managed Istio Gateway. At the same time, the global management (Keycloak) login page is used to achieve unified login and authentication. The API authn token verification uses the Istio Gateway.",
What content does the docking navigation bar contain?,The docking navigation bar mainly places the entrance in the left navigation bar.,
What modules does GProduct need to connect to?,"GProduct needs to connect to the navigation bar, routing and AuthN in global management.",
What is GProduct?,GProduct is the collective name for all other modules in DCE 5.0 except global management.,
How can I switch platform language?,"The platform provides three ways to switch languages: Chinese, English-English, and automatically detects your browser language preference.",
Where can I click to open the personal center?,The personal center can be opened in the username position in the upper right corner.,
Will switching languages affect other users?,Each user's multi-language services are independent of each other and will not affect other users after switching.,
What two languages does the platform currently support?,Currently supports two languages: Chinese and English.,
How to switch the language of the platform?,1. Log in to the web console using your username/password. \n2. Click `Global Management` at the bottom of the left navigation bar. \n3. Click on the username in the upper right corner and select `Personal Center`. \n4. Click the `Language Settings` tab. \n5. Switch language options.,
How to synchronize users or user groups in the enterprise user management system to DCE 5.0?,Users or user groups in the enterprise user management system can be synchronized to DCE 5.0 at one time through manual synchronization or automatic synchronization every ten minutes.,
What fields need to be filled in when setting up an LDAP identity provider?,"The following fields need to be filled in when establishing an LDAP identity provider: server, user name, password, base DN, user object filter, automatic synchronization, whether to enable TLS, full name mapping, and mailbox mapping.",
How to establish trust relationship with identity provider and user mapping relationship in global management?,"In Global Management, you need to navigate to `Users and Access Control` under `Global Management` and select `Create Identity Provider`. Fill in the relevant fields in the `LDAP` tab and click Save.",
What is LDAP?,"LDAP, the full name of Lightweight Directory Access Protocol, is an open and neutral industry standard application protocol that provides access control and maintains directory information of distributed information through IP protocol.",
How can resource quotas and limits be used to help cluster administrators better manage and allocate resources?,Cluster administrators can use resource quotas and limits to constrain the amount of computing resources used by project teams to ensure that project teams do not abuse resources.,
What resource limits does `LimitRange` provide in DCE 5.0?,"The resource limits provided by `LimitRange` in DCE 5.0 include: the minimum and maximum amount of resources that can be used by each Pod or container in the namespace, the minimum and maximum storage requests that can be used by each PVC in the namespace, resource requests in the namespace, and limit, set the default request/limit for the computing resources in the namespace and automatically inject it into the container at runtime.",
What is LimitRange?,LimitRange is a security policy that limits the amount of resource allocation (limit and request) that can be specified for each applicable object class in a namespace.,
What do resource limits do?,Resource quotas can be used to limit the maximum number of objects that can be created in a namespace and the total amount of computing resources that can be consumed by resources in the namespace.,
What are resource quotas?,Resource quota is through Resource,
What are the reference materials?,You can refer to the following Demo and official documents:\n- [otel-grpc-examples](https://github.com/openinsight-proj/otel-grpc-examples/tree/no-metadata-grpcgateway-v1.11.1) \n- [opentelemetry-demo/productcatalogservice](https://github.com/open-telemetry/opentelemetry-demo/tree/main/src/productcatalogservice)\n- [opentelemetry-collector-contrib/demo](https: //github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/examples/demo)\n- [OpenTelemetry official documentation](https://opentelemetry.io/docs/),
How to log errors and exceptions?,"You can use the `RecordError` method of the Span object to convert the error into a span event and record it, and use the `SetStatus` method where it needs to be marked as an error. The sample code is as follows:\n```golang\nspan.RecordError(err)\nspan.SetStatus(codes.Error, ""internal error"")\n```",
How to add custom properties and events to span?,"You need to import the tracking and attribute libraries, and then get the current Span object using the `SetAttributes` and `AddEvent` methods. The sample code is as follows:\n```golang\nspan := trace.SpanFromContext(c.Request.Context())\nspan.SetAttributes(attribute.String(""controller"", ""books""))\nspan.AddEvent(msg )\n```",
How to customize Span?,"You can use the `Tracer.Start` method to create a new Span object and call the `End()` method in the function that needs to be logged. The sample code is as follows:\n```golang\n_, span := otel.Tracer(""GetServiceDetail"").Start(ctx,\n""spanMetricDao.GetServiceDetail"",\ntrace.WithSpanKind(trace.SpanKindInternal))\ndefer span .End()\n```",
How can I add a wrapper handler function for http.Handler without using request routing?,"Everywhere you pass http.Handler to ServeMux, you wrap a handler function. The sample code is as follows:\n```golang\nmux.Handle(""/path"", otelhttp.NewHandler(http.HandlerFunc(f), ""description of path""))\n```",
"If Grpc Client is used in the program to call third-party services, how to add an interceptor?","It is necessary to add an interceptor to Grpc Client. The sample code is as follows:\n```golang\nconn, err := grpc.Dial(addr, grpc.WithTransportCredentials(insecure.NewCredentials()),\ngrpc.WithUnaryInterceptor(otelgrpc.UnaryClientInterceptor( )),\ngrpc.WithStreamInterceptor(otelgrpc.StreamClientInterceptor()),\n)\n````",
How to detect any gRPC servers you have?,"Add the interceptor to the server's instantiation, the sample code is as follows:\n```golang\nimport (\ngrpcotel ""go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc""\n) \nfunc main() {\n[...]\ns := grpc.NewServer(\ngrpc.UnaryInterceptor(grpcotel.UnaryServerInterceptor()),\ngrpc.StreamInterceptor(grpcotel.StreamServerInterceptor()),\n)\n }\n```",
How to run it in production environment?,You can refer to the related introduction of `Only injecting environment variable annotations' in Non-intrusive application enhancement through Operator to add annotations to the deployment yaml or manually add environment variables to the deployment yaml.,
How to customize Span to record more internally called functions?,"You can use the following code to customize Span to record more internally called functions:\n```golang\n_, span := otel.Tracer(""GetServiceDetail"").Start(ctx,\n""spanMetricDao.GetServiceDetail"",\ ntrace.WithSpanKind(trace.SpanKindInternal))\ndefer span.End()\n````",
How to add custom properties and events?,You need to follow the following steps: 1. Import tracking and property libraries; 2. Get the current Span from the context; 3. Set properties in the current Span; 4. Add Events to the current Span.,
How to log errors and exceptions?,"You can use the following code to record errors and exceptions:\n```golang\n// Get the current span\nspan:= trace.SpanFromContext(ctx)\n// RecordError will automatically convert an error into span even\nspan.RecordError (err)\n// Mark this span error\nspan.SetStatus(codes.Error, ""internal error"")\n````",
How to add OTel Gin middleware to the application?,Gin can be configured to use the middleware by adding the following line to `main.go`:\n```golang\nimport (\n....\n"go.opentelemetry.io/contrib/instrumentation/github. com/gin-gonic/gin/otelgin"\n)\nfunc main() {\n...\nr := gin.Default()\nr.Use(otelgin.Middleware("my-app" ))\n...\n}\n```,
How to initialize tracker in main.go?,"Modify the main function to initialize the tracker in main.go. In addition, when your service is shut down, you should call `TracerProvider.Shutdown()` to ensure that all spans are exported. The service makes this call a deferred function in the main function:\n```golang\nfunc main() {\n// start otel tracing\nif shutdown := retryInitTracer(); shutdown != nil {\ndefer shutdown( )\n}\n......\n}\n```",
How to create an initialization function using OTel SDK?,"The following code can be used:\n```golang\nvar tracerExp *otlptrace.Exporter\nfunc retryInitTracer() func() {\nvar shutdown func()\ngo func() {\nfor {\n// otel will reconnected and re-send spans when otel col recover. so, we don""t need to re-init tracer exporter.\nif tracerExp == nil {\nshutdown = initTracer()\n} else {\nbreak\n}\ntime.Sleep (time.Minute * 5)\n}\n}()\nreturn shutdown\n}\nfunc initTracer() func() {\n// temporarily set timeout to 10s\nctx, cancel := context.WithTimeout(context .Background(), 10*time.Second)\ndefer cancel()\nserviceName, ok := os.LookupEnv(""OTEL_SERVICE_NAME"")\nif !ok {\nserviceName = ""server_name""\nos.Setenv(""OTEL_SERVICE_NAME"", serviceName)\n}\notelAgentAddr, ok := os.LookupEnv(""OTEL_EXPORTER_OTLP_ENDPOINT"")\nif !ok {\notelAgentAddr = ""http://localhost:4317""\nos.Setenv(""OTEL_EXPORTER_OTLP_ENDPOINT"", otelAgentAddr)\n} \nzap.S().Infof(""OTLP Trace connect to: %s with service name: %s"", otelAgentAddr, serviceName)\ntraceExporter, err := otlptracegrpc.New(ctx, otlptracegrpc.WithInsecure(), otlptracegrpc.WithDialOption (grpc.WithBlock()))\nif err != nil {\nhandleErr(err, ""OTLP Trace gRPC Creation"")\nreturn nil\n}\ntracerProvider := sdktrace.NewTracerProvider(\nsdktrace.WithBatcher(traceExporter),\ nsdktrace.withsampler (sdktrace.alwayssample ()), \ nsdktrace.withResource (Resource.newwithattributes (SEMCONV.SChemaurll)) \ notel.se TTRACERPROVIDER (TracerProvider) \ notel.setTextMapPropagator (Propagation.newCompositexTEXTMAPPROPAGAGATOR (Propagation.TraceContext {}, PROPAGAT Ion. Baggage{}))\ntracerExp = traceExporter\nreturn func() {\n// Shutdown will flush any remaining spans and shut down the exporter.\nhandleErr(tracerProvider.Shutdown(ctx), ""failed to shutdown TracerProvider"")\n }\n}\nfunc handleErr(err error, message string) {\nif err != nil {\nzap.S().Errorf(""%s: %v"", message, err)\n}\n}\ n```",
How to install dependencies in Go application?,You need to run the following command:\n```golang\ngo get go.opentelemetry.io/otel@v1.8.0 \\ngo.opentelemetry.io/otel/trace@v1.8.0 \\ngo.opentelemetry.io/otel/ sdk@v1.8.0 \\ngo.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin@v0.33.0 \\ngo.opentelemetry.io/otel/exporters/otlp/otlptrace@v1. 7.0 \\ngo.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc@v1.4.1\n````,
What telemetry data can OpenTelemetry be used to generate and collect?,"OpenTelemetry can be used to generate and collect links, metrics and logs.",
What is OpenTelemetry?,"OpenTelemetry is an open source observability framework that helps Go applications generate and collect telemetry data: links, metrics, and logs.",
Is it currently possible to provide persistence services for custom dashboards?,This is not currently possible. You need to back up your customized dashboard before upgrading or restarting Insight.,
What parameters need to be filled in to import a dashboard in the UI?,"You need to fill in the name, target path, and data source.",
What is the full name of CRD?,"Custom Resource Definition, custom resource definition.",
How can I import a customized dashboard?,Customized dashboards can be imported through CRD and UI.,
For which indicators can users set alarm rules?,"Users can alert on CPU usage, memory usage, disk usage, disk reads per second, disk writes per second, cluster disk read throughput, cluster disk write throughput, network sending rate and network receiving rate Rule setting.",
What alarm notification methods does the DCE 5.0 alarm center support?,"DCE 5.0 Alarm Center supports alarm notification methods including email, DingTalk, Enterprise WeChat, Webhook and SMS notification.",
What are the two types of alert rules?,Alert rules are divided into active and expired rules.,
What global alerting policies are built into DCE 5.0?,"DCE 5.0 has built-in global alarm policies, including CPU usage, memory usage, disk usage, disk reads per second, disk writes per second, cluster disk read throughput, cluster disk write throughput, network Send rate and network receive rate.",
What problems can the DCE 5.0 alarm center help users solve?,"The DCE 5.0 alarm center can help users discover and solve problems in the cluster in a timely manner, improve business stability and availability, and facilitate cluster inspection and troubleshooting.",
How to write OpenShift system monitoring data into Prometheus?,"System monitoring data can be written to Prometheus through OpenShift's own mechanism. For specific implementation methods, you can refer to the above code and create the corresponding ConfigMap.",
What parameters need to be added when using helm install to install Insight Agent?,"When using helm install to install Insight Agent, in addition to the basic installation configuration, you also need to add the following parameters: \n--set fluent-bit.ocp.enabled=true \\n--set fluent-bit.serviceAccount. create=false \\n--set fluent-bit.securityContext.runAsUser=0 \\n--set fluent-bit.securityContext.seLinuxOptions.type=spc_t \\n--set fluent-bit.securityContext.readOnlyRootFilesystem=false \\n--set fluent-bit.securityContext.allowPrivilegeEscalation=false \\n--set compatibility.openshift.prometheus.enabled=true \\n--set kube-prometheus-stack.prometheus.enabled=false \\n --set kube-prometheus-stack.kubeApiServer.enabled=false \\n--set kube-prometheus-stack.kubelet.enabled=false \\n--set kube-prometheus-stack.kubeControllerManager.enabled=false \\ n--set kube-prometheus-stack.coreDns.enabled=false \ \n--set kube-prometheus-stack.kubeDns.enabled=false \ \n--set kube-prometheus-stack.kubeEtcd.enabled=false \ \n--set kube-prometheus-stack.kubeEtcd.enabled=false \ \n--set kube-prometheus-stack.kubeScheduler.enabled=false \ \n--set kube-prometheus-stack.kubeStateMetrics.enabled=false \ \n--set kube-prometheus-stack.nodeExporter.enabled=false \ \n--set kube-prometheus-stack.prometheusOperator.kubeletService.namespace=""insight-system"" \ \n--set kube-prometheus- stack.prometheusOperator.prometheusInstanceNamespaces=""insight-system"" \ \n--set kube-prometheus-stack.prometheusOperator.denyNamespaces[0]=""openshift-monitoring"" \ \n--set kube-prometheus-stack.prometheusOperator.denyNamespaces [1]=""openshift-user-workload-monitoring"" \ \n--set kube-prometheus-stack.prometheusOperator.denyNamespaces[2]=""openshift-customer-monitoring"" \ \n--set kube-prometheus-stack .prometheusOperator.denyNamespaces[3]=""openshift-route-monitor-operator""",
Why do I need to install Insight Agent on an OpenShift system?,"Although the OpenShift system comes with a monitoring system, due to some rules of data collection agreement, we still need to install Insight Agent.",
What is authorization?,"Authorization refers to granting users the permissions they need to complete specific tasks. Authorization takes effect through the permissions of system roles or custom roles. After users obtain specific permissions, they can operate resources or services.",
What is a role?,"Roles are the bridge connecting users and permissions. One role corresponds to a set of permissions, and different roles have different permissions. There are two types of roles in global management: predefined roles and custom roles. Predefined roles are created by the system and cannot be modified. Each sub-module has an Admin role; custom roles are created, updated and deleted by the administrator themselves, and the permissions in the custom roles are maintained by the administrator themselves.",
What are user groups?,"A user group is a collection of one or more users. IAM can implement user authorization through user groups. Usually, you first create an IAM user and join a certain user group. The user will inherit the permissions of this user group. When a user joins multiple user groups, the user will have the permissions of multiple user groups at the same time.",
What is RBAC?,"RBAC is the abbreviation of Role-Based Access Control. It assigns the concept of roles to users and associates users with permissions through roles to achieve flexible configuration. The three elements of the RBAC model are: users, roles, and permissions. Use the RBAC mechanism to authorize IAM users to access platform resources.",
What is IAM?,"IAM is the abbreviation of Identity and Access Management, which is the user and access control module. IAM administrators have the highest authority in this module and can authorize users to access platform resources.",
How to reload nginx so that its configuration takes effect?,You can use the following command:\n```\nnginx -s reload\n```,
How to configure SSL certificate for nginx?,You can refer to the following steps:\n1. Enter the nginx configuration file storage directory. \n2. Create a cert folder to store the SSL certificate. \n3. Copy the SM2 and RSA (optional) certificates to the `/usr/local/nginx/conf/cert` directory. \n4. Edit the nginx.conf configuration and configure the SSL certificate. \n5. Reload nginx to make its configuration take effect.,
How to configure the national encryption gateway for DCE 5.0?,You can refer to the following steps:\n1. Prepare a Linux host with Docker installed and make sure it can access the Internet. \n2. Compile and install Tengine & Tongsuo. \n3. Generate SSL certificates (SM2 and RSA certificates). \n4. Configure the SSL certificate for nginx. \n5. Reload nginx to make its configuration take effect.,
What is Tongsuo?,"Tongsuo/Tongsuo (formerly BabaSSL) is an open source basic cryptographic library that provides modern cryptography algorithms and secure communication protocols. It provides underlying cryptography basic capabilities for many business scenarios such as storage, networking, key management, and privacy computing.",
What is Tengine?,"Tengine is a web server project initiated by Taobao. Based on Nginx, it adds many advanced functions and features to meet the needs of large-traffic websites.",
"If I delete an existing group, what happens to the internal members?","Deleting a user group will not delete the users in the group, but users in the group will no longer be able to inherit the group's permissions.",
How to add new members to an existing user group?,"Administrators can go to ""Users and Access Control"" -> ""User Groups"", click ""..."" -> ""Add Members"" to the right of an existing user group, select the members to be added to the group and submit That’s it.",
How to authorize an existing user group?,"Administrators can go to ""User and Access Control"" -> ""User Groups"", click ""..."" -> ""Authorization"" to the right of an existing user group, then check the required role permissions and submit. .",
How to create a new user group?,"Administrators can go to ""Users and Access Control"" -> ""User Groups"", enter the ""Create New User Group"" page, fill in the relevant information and submit.",
What are the applicable scenarios for user groups?,"When user permissions change, they only need to be moved to the corresponding user group and will not affect other users. When the permissions of a user group change, you only need to modify the role permissions of the user group, and it will apply to all users in the group.",
What are user groups?,user group is a collection of users. Users can inherit the role permissions of the user group by joining the user group. Authorizing users in batches through user groups can better manage users and their permissions.,
What is the process for using IAM?,"Log in to the DCE platform using the DCE platform main account or a user account with administrator rights. Then proceed in this order: create user, authorize user, create user group, create custom role, and create identity provider. Please refer to the relevant documentation for specific details.",
What are the advantages of IAM?,"IAM uses a clear and tidy page to open up the authorization relationship between users, user groups, and roles, and realizes the authorization of users (user groups) with the shortest link. In addition, IAM predefines an administrator role for each submodule, eliminating the need for user maintenance. Ultimately, IAM can establish a trust relationship between the enterprise and the DCE platform, and enable employees to directly log in to the DCE platform using existing enterprise accounts through joint authentication.",
What is IAM?,"IAM (Identity and Access Management) is an important module of DCE global management. It can create, manage and destroy users (user groups), and use system roles and custom roles to control other users' permissions to use the DCE platform.",
"After using an identity provider for identity authentication, do users need to remember two sets of platform accounts?","After using the identity provider for identity authentication, users only need to log in to the enterprise management system to access DCE 5.0, without having to remember two sets of platform accounts.",
What is OAuth 2.0?,OAuth 2.0 is an open authorization protocol and authorization framework that enables third-party applications to gain access on their own behalf.,
What is OIDC?,"OIDC is the abbreviation of OpenID Connect, an identity authentication standard protocol based on the OAuth 2.0 protocol.",
What is LDAP?,"LDAP refers to Lightweight Directory Access Protocol, which is often used for single sign-on, that is, users can log in using one account and password in multiple services.",
What protocols does Global Management support for identity authentication?,Global management supports single sign-on for identity authentication based on LDAP and OIDC protocols.,
What is a service provider?,The service provider establishes a trust relationship with the identity provider and uses the user information provided by the IDP to provide specific services to the user.,
What is an identity provider?,"An identity provider is a service responsible for collecting and storing user identity information, usernames, passwords, etc., and authenticating users when they log in.",
How do I manage platform settings?,"Only platform administrators have management rights for platform settings and can be managed through `Global Management` -> `Platform Settings`. You can manage account security information, platform logo, license authorization, mail server, etc. For specific operations, refer to [Platform Settings](../user-guide/platform-setting/about.md).",
How to perform audit log query?,"On the audit log page, you can perform combined queries through multiple dimensions such as event source, resource type, operation status, etc., and support audit log export. For specific operations, refer to [Audit Log](../user-guide/audit-log.md).",
What resources is the workspace used to manage?,"[Workspace](../user-guide/workspace/workspace.md) is used to manage resources, including hierarchy and workspace. Workspaces can be understood as projects under departments, and administrators map hierarchical relationships in the enterprise through hierarchies and workspaces. Supports adding resources to workspace - resource group, supports 6 resource types.",
How to grant different permissions to a single user or a group of users?,"First, you need to create the corresponding [role](../user-guide/access-control/role.md) in the `Global Management` -> `User and Access Control` -> `Permission Management` page, and then in [ Grant it to the corresponding user or user group in User Management] (../user-guide/access-control/user.md) or [User Group Management] (../user-guide/access-control/group.md) Just the corresponding role.",
How to create a user in the DCE platform?,"The platform administrator Super Admin or the user and access control administrator IAM Admin is created on the `Global Management` -> `User and Access Control` -> `User` page, or connected through LDAP. For specific steps, please refer to [User Management](../user-guide/access-control/user.md).",
What operations can the Folder Admin role perform on the workspace under it - Shared Resources?,"For the workspace under it - shared resources, the Folder Admin role can perform operations such as adding new shares, removing shares, and resource quotas.",
What operations can the Folder Admin role perform on the workspace-resource group under it?,"For the workspace-resource group under it, the Folder Admin role can view, bind and unbind resources.",
What objects can the Folder Viewer role perform authorized operations on?,"In a folder, the Folder Viewer role cannot perform authorization operations.",
"In a folder, what objects can the Folder Editor role perform authorized operations on?","In a folder, the Folder Editor role can perform authorized operations on subfolders.",
"When the folder corresponds to the department/supplier/project level in the enterprise, if the user has administrative rights (Admin) in the first-level department, which departments or projects below it also have administrative rights?","When the folder corresponds to the department/supplier/project level in the enterprise, if the user has administrative rights (Admin) in the first-level department, the second-level, third-level, and fourth-level departments or projects below it also have administrative rights. .",
What type of resources does the permission mapping capability of folders not apply to?,The permission mapping capability of folders does not apply to shared resources.,
"If a user has the Folder Admin role in a folder, what role will be mapped to the workspace under it?","If the user has the Folder Admin role in the folder, the workspace mapped to it will be Workspace Admin.",
What objects can the folder permission mapping capability map user/user group permissions to?,"Folder permission mapping capability can map user/user group permissions to subfolders, workspaces and resources below them.",
What role permissions can be granted to the root folder?,"The root folder can be granted the Folder Admin role, which has management rights for all departments, projects, and their resources.",
How to grant different roles to users/groups in the same folder?,Different roles can be granted by granting permissions to users/groups in the same folder.,
What three roles does a folder provide?,"Folders provide three roles: Folder Admin, Folder Editor, and Folder Viewer.",
What resources can a folder contain?,"A folder can contain workspaces, subfolders, or a combination of both.",
How many levels of hierarchical structure does a folder support?,DCE supports up to five levels of folder hierarchy.,
Under what circumstances cannot an already created folder be deleted?,"When there are resources in the resource group and shared resources under the folder, the folder cannot be deleted, and all resources need to be unbundled before deleting; when the microservice engine module has access to the registration center resources under the folder, This folder cannot be deleted. You need to remove all access registration centers before deleting the folder.",
How to edit or delete a folder that has been created?,Select a folder or folder and click `...` on the right to edit or delete it.,
How to view the newly created folder after successful creation?,"After successful creation, the folder name will be displayed in the tree structure on the left, with different icons representing the workspace and folder.",
What are the specific steps to create a folder?,"1. Log in to the Web console as a user with the admin/folder admin role, and click `Global Management` at the bottom of the left navigation bar. \n2. Click `Workspace and Hierarchy` in the left navigation bar, and click the `Create Folder` button in the upper right corner. \n3. After filling in the folder name, upper-level folder and other information, click `OK` to complete the creation of the folder.",
What permissions does the Kpanda Owner role have?,"The Kpanda Owner role is a system role and belongs to the container management module in global management mode. This role has all permissions under the service, such as creating/accessing a cluster, deploying applications, and granting cluster/namespace-related permissions to users/user groups.",
What is the IAM Owner role?,"The IAM Owner role is a system role and belongs to the user and access control module in global management mode. This role has all permissions under the service, such as managing users/user groups and authorization.",
What permissions does the Admin role have?,"The Admin role is a system role that has the authority of a platform administrator and can manage all platform resources, representing the highest authority of the platform.",
What are the predefined roles in global management mode?,"In global management mode, four roles are predefined in user and access control: Admin, IAM Owner, Audit Owner and Kpanda Owner.",
What is global management mode?,The global management mode is a permission management mode that is suitable for scenarios where administrators need to be set up for sub-modules and the permissions of users/user groups can be further managed through the administrator. The roles in user and access control represent the highest permissions of each sub-module.,
In which modules do ordinary users have operating permissions?,"Ordinary users have operating permissions in the application workbench, microservice engine, service grid, observability, data services and other modules.",
What is the authorization and resource planning process for ordinary users?,The authorization and resource planning process for ordinary users includes the following steps:\n1. Create a user\n2. Prepare the Kubernetes namespace\n3. Prepare the workspace\n4. Bind the workspace to the namespace\n5. Authorize the user Workspace Editor,
What is an ordinary user?,"Ordinary users are able to use most of the DCE product modules and functions (except management functions), have certain operating rights over resources within the scope of authority, and can independently use resources to deploy applications.",
Can the columns be customized according to needs?,"Yes, columns can be customized according to needs, and the gear icon setting function is supported.",
Is custom time range supported?,"Yes, custom time ranges are supported.",
How to export or download data?,Corresponding data can be obtained through the export/download data function.,
For which entities can billing statistics be performed?,"Currently, billing statistics are supported for clusters, nodes, Pods, namespaces and workspaces.",
How do I enable or disable a certain kind of billing?,"Administrators can enable or disable certain accounting. After enabling certain accounting, the accounting will start collecting data and displaying the accounting information within 20 minutes. If certain accounting is disabled, data collection will stop within 20 minutes, and previously collected data can still be displayed normally.",
What is the time interval for billing statistics?,Billing will start collecting data and display billing information within 20 minutes.,
What two currencies are supported?,"Currently, two currencies, RMB and USD, are supported.",
How to create a namespace under the workspace and deploy the application?,"Users can go to the Application Workbench to create namespaces and deploy applications under these workspaces. For specific steps, please refer to [Ambassador official documentation](../../../amamba/user-guide/namespace/namespace.md).",
How do I assign a cluster to a workspace?,"After selecting a workspace name under global management, click the `Add Shared Resource` button in the shared resources, select the cluster and fill in the resource limit and click OK.",
How to create a workspace?,"Log in to the Web console as a user with the admin/folder admin role, click `Global Management` at the bottom of the left navigation bar, select `Workspace and Hierarchy`, click the `Create Workspace` button in the upper right corner, fill in the workspace name and ownership After adding the folder and other information, click OK to complete the creation.",
Why use workspaces?,"Workspaces can provide higher-dimensional resource quota capabilities, enabling workspaces (tenants) to self-service create Kubernetes namespaces under resource quotas, and can also meet multi-tenant usage scenarios.",
What is a workspace?,"Workspaces are designed to meet multi-tenant usage scenarios. They form an isolated resource environment based on multiple resources such as clusters, cluster namespaces, grids, grid namespaces, multi-clouds, and multi-cloud namespaces. Workspaces can be mapped to projects and tenants. , enterprises, suppliers and other concepts.",
How to create a folder role?,"(1) Click `Global Management` -> `Users and Access Control` -> `Role` from the left navigation bar, and click `Create Custom Role`. (2) Enter the name and description, select `Folder Role`, check the role permissions and click `OK`. (3) Return to the role list, search for the custom role you just created, and click `⋮` on the right to perform operations such as copying, editing, and deleting.",
What is the folder role?,Folder roles refer to roles that can control the related features of a certain module of DCE 5.0 based on folders and subfolders.,
How to create a workspace role?,"(1) Click `Global Management` -> `Users and Access Control` -> `Role` from the left navigation bar, and click `Create Custom Role`. (2) Enter the name and description, select `Workspace Role`, check the role permissions and click `OK`. (3) Return to the role list, search for the custom role you just created, and click `⋮` on the right to perform operations such as copying, editing, and deleting.",
What are workspace roles?,Workspace roles refer to roles that can control the related features of a module according to the workspace.,
How to create a platform role?,"(1) Click `Global Management` -> `Users and Access Control` -> `Role` from the left navigation bar, and click `Create Custom Role`. (2) Enter the name and description, select `Platform Role`, check the role permissions and click `OK`. (3) Return to the role list, search for the custom role you just created, and click `⋮` on the right to perform operations such as copying, editing, and deleting.",
What does the platform role mean?,Platform roles refer to roles that can control features related to a certain module of DCE 5.0.,
What three scopes of custom roles does DCE 5.0 support?,"DCE 5.0 supports the creation of platform roles, workspace roles, and folder roles.",
What are the access keys in Personal Center used for?,"The access key in the personal center is used to create an independent API key to ensure system security, and API key expiration settings can be set.",
What items can be set in the security policy in platform settings?,"The security policy in the platform settings can set custom password rules and password policies, session timeout policies, account lock policies, and login and logout policies.",
In what ways can the audit logs be searched?,"The audit log can be searched by status, operator, resource type, and resource name. It also supports displaying the audit log of the last day or a custom time period.",
What operations are supported in the workspace?,"The workspace supports operations such as adding users/user groups and authorizing them, adding resources to the workspace, sharing resources, moving the workspace, and resource quotas.",
What two identity provider protocols does DCE global management support?,"DCE global management supports two identity provider protocols, LDAP and OIDC. The LDAP protocol supports manual/automatic synchronization of external users and manual synchronization of external user groups, and the OIDC protocol supports manual synchronization of external users.",
What are the types of differentiated configurations of DCE multi-cloud modules?,"The differentiated configuration of the DCE multi-cloud module includes five types: image source address, resource limit, running command, label and annotation.",
What is the DCE multi-cloud module?,"The DCE multi-cloud module is part of DaoCloud's enterprise-class container service, which can deploy, manage and monitor container applications across multiple public and private clouds.",
What are the six types of differentiation strategies?,"Differentiation strategies include: differentiated configuration of images, differentiated configuration of running parameters, differentiated configuration of running commands, customized differentiated configuration, differentiated configuration of labels and differentiated configuration of annotations.",
How to create a differentiation strategy through forms?,"You can create a differentiation strategy through the following steps:\n1. Enter the namespace level menu and click the `Create Differentiation Policy` button. \n2. Enter the form creation page. To create a complete differentiation strategy, you need to configure three parts: basic configuration, resource configuration, and differentiation strategy. \n3. After completing the filling, click OK to create it successfully. It supports YAML update, form update, and deletion operations for a differentiation strategy.",
How to create a differentiation strategy using YAML?,"You can use YAML to create a differentiation policy through the following steps:\n1. After entering a multi-cloud instance, in the left navigation bar, click `Policy Management` -> `Differentiation Policy`, and click the `YAML Create` button. \n2. In the `YAML creation` page, enter the correct YAML statement and click `OK`. \n3. Return to the deployment policy list, the newly created one is first by default. Click `⋮` on the right side of the list to edit YAML and perform deletion operations.",
What are the two levels of differentiation strategy?,Differentiation strategies are divided into namespace level and cluster level.,
What is the role of differentiation strategy?,"The purpose of the differentiation strategy is to deliver different configurations to different clusters, which can be different. For example, if the mirror warehouse addresses corresponding to different clusters are different, then you need to set different mirror addresses for the workloads in different clusters. It also supports setting environment variables, etc.",
What Kubernetes Resource types are currently supported for multi-cloudization?,"Currently, the open operation portal only supports multi-cloud deployment of Deployment. However, if the Deployment in the sub-cluster is associated with the corresponding ConfigMap and Secret, the resource will automatically be multi-cloudized. The purpose of this is to ensure that when the multi-cloud workload is distributed to other clusters, the resources that the workload depends on also exist simultaneously. Otherwise, the workload may start abnormally.",
"After converting a multi-cloud workload, will the subcluster workload be deleted after the atomic cluster is kicked out of the deployment policy?","Yes, once managed into a multi-cloud orchestration, Atomic Clusters become a standard multi-cloud workload. When the deployment strategy changes and subclusters are no longer propagated, they will be deleted according to Karmada's design principles to ensure consistency.",
What happens when you go multi-cloud?,"When going multi-cloud, the ConfigMap and Secret associated with the workload will be automatically converted into multi-cloud resources simultaneously, and corresponding deployment strategies will be automatically created and atomic clusters will be managed.",
How to convert sub-cluster workloads to multi-cloud workloads with one click?,"In multi-cloud orchestration, the workload of a sub-cluster can be converted into a multi-cloud workload with one click through a simple selection operation. The specific steps are: select the corresponding sub-cluster -> select the application -> click Confirm to complete the multi-cloudization of the workload.",
What is multicloud orchestration?,"Multi-cloud orchestration is an integrated multi-cloud resource management solution that can manage resources on different cloud platforms and provide unified orchestration, scheduling and management capabilities.",
How to create a namespace level deployment policy in a form?,"To create a namespace-level deployment strategy in the form, you need to fill in information such as basic configuration, configuration resources, and deployment strategy, including name, multi-cloud namespace, label annotation, multi-cloud resources, target deployment cluster, scheduling type, taint tolerance, and propagation Constraints etc.",
Can you give an example of YAML for a deployment strategy? What elements are required?,"Can. A deployment policy YAML example needs to contain the `kind`, `apiVersion`, `metadata`, and `spec` elements.",
How to create a namespace-level deployment policy via YAML in a multi-cloud instance?,"After entering a multi-cloud instance, in the left navigation bar, click `Policy Management` -> `Deployment Policy`->`Namespace Level`, click the `YAML Create` button in the upper right corner, and enter the correct YAML statement. Click `OK`.",
How does multi-cloud orchestration support creating and editing deployment policy information?,Multi-cloud orchestration supports creating and editing deployment policy information in YAML and forms.,
What is a deployment strategy for multi-cloud orchestration?,"The deployment strategy of multi-cloud orchestration defines how to distribute resources in multi-cloud and multi-cluster, which is divided into namespace level and cluster level.",
 What information can be viewed in the multi-cloud instance list?,"In the multi-cloud instance list, you can view basic information such as the name, alias, creation time, and status of each instance.",
What are overall CPU usage and overall memory usage calculated against?,The overall CPU usage and overall memory usage are calculated based on the combined calculation of all working clusters connected to all multi-cloud orchestration.,
What instance information is displayed on the multi-cloud instance overview page?,"The multi-cloud instance overview page displays the basic information of the current instance, multi-cloud workload status, resource information, working clusters and other information. This includes the instance's name, alias, creation time, Karmada version, status, and overall CPU and memory usage.",
How can I search for instances in a multi-cloud instance list?,Instance searches are currently possible via fuzzy matching of cluster names.,
How to create a working cluster in single data center single management cluster mode?,Create one or more working clusters based on the management cluster on the container management module under the platform as needed.,
"In a multi-data center scenario, which deployment mode is recommended?","In a multi-data center scenario, it is recommended to use the classic mode. You can deploy a management cluster in different data centers or regions, and connect all management clusters to the global service cluster for management, thereby achieving unified management of clusters in different regions. life cycle.",
"In a single data center scenario, which deployment mode is recommended?","In a single data center scenario, it is recommended to use the simple mode, that is, only one cluster is needed to run platform-related components (global service cluster + management cluster two-in-one), and multiple clusters are deployed to run business loads (work clusters).",
What cluster roles does DCE 5.0 provide?,"DCE 5.0 provides four cluster roles: global service cluster, management cluster, worker cluster and external cluster.",
How to contact Daocloud for authorization?,"After successfully installing the DCE 5.0 commercial version, please contact Daocloud customer service for authorization: email info@daocloud.io or call 4000026898.",
What are the offline steps to install DCE 5.0 commercial version?,"The first step is to download the offline package, the second step is to configure the cluster configuration file, and the third step is to start the installation. During the specific execution process, you also need to pay attention to some command line parameters and authorization after successful installation.",
What preparations are needed to install DCE 5.0 commercial version?,"Before installation, make sure you have read and understood documents such as deployment requirements, deployment architecture, and preparations. At the same time, you need to pay attention to the known issues and new features of the installed version.",
How to download the offline package of DCE 5.0 commercial version?,You can download the latest version of the DCE 5.0 commercial version offline package at [Download Center](https://docs.daocloud.io/download/dce5/).,
What offline packages are required for DCE 5.0 commercial version?,"DCE 5.0 commercial version requires offline image package, addon offline package, ISO offline package and osPackage offline package.",
How to uninstall DCE 5.0 Community Edition?,"Execute the above commands in sequence to uninstall DCE 5.0 Community Edition. It should be noted that no backup will be performed during the uninstallation process, so please proceed with caution.",
What are the functions of Grafana's system configuration?,"Grafana's system configuration displays the default storage time of indicators, logs, and links, as well as the default Apdex threshold, and supports custom modification of the storage time of indicators, logs, and link data. At the same time, it provides unified monitoring of observable components and detects the health status of system components in real time.",
What types of metric data can Grafana collect?,Grafana supports customizing the namespace range discovered by Pods by using ServiceMonitor and selecting the listening Service for indicator data collection by using matchLabel. It also supports viewing the container group status of the collection components.,
Can Grafana collect and persist logs in a unified manner?,"Yes, Grafana can uniformly collect nodes, containers, intra-container and k8s events, etc., and supports annotation output to middleware such as Elasticsearch for persistent storage.",
What functions does Grafana’s Alert Center have?,Grafana's alarm center includes active alarms and historical alarms. Administrators can create global alarm rules or use predefined indicators or write Prom,
What types of data queries does Grafana support?,Grafana supports indicator query and log query. Indicator query can be done through subscribed basic indicators or native Prom,
Can Grafana view data from multiple clusters?,"Yes, the data sources used by Grafana support viewing data across multiple clusters.",
What enhanced observability features does the DCE 5.0 commercial version provide compared to the community version?,"Compared with the community version, the DCE 5.0 commercial version provides enhanced observable functions such as service monitoring, topology map, and link query. At the same time, in terms of resource monitoring, the commercial version also provides cluster monitoring functions.",
What observability capabilities does DCE 5.0 Community Edition provide?,"DCE 5.0 Community Edition provides observable functions such as resource monitoring, dashboards, data query, alarm center, log collection and query, indicator collection and system configuration.",
How to connect to the global service cluster through NodePort?,The corresponding port information of the corresponding service needs to be set according to different situations.,
"In addition to the interface provided by Insight Server, what other methods can be used to connect to the global service cluster?",Can be connected through LoadBalancer connection and NodePort connection.,
How to obtain the address of each service when LoadBalancer is disabled in the management cluster?,An additional externally accessible node IP needs to be passed to call the interface to obtain each service address.,
How to obtain the address of each service when the management cluster is exposed using the default LoadBalancer method?,Log in to the console of the global management cluster and execute the command "curl --location --request POST "http://""${INSIGHT_SERVER_IP}""/apis/insight.io/v1alpha1/agentinstallparam"" to obtain each service address.,
In what ways can we obtain observation data for multiple subclusters?,"Observation data of multiple sub-clusters can be obtained through log service, indicator service and link service.",
"After installing insight-agent in the global service cluster, what is the recommended way to access the cluster?",It is recommended to access the cluster through the domain name.,
What is a global service cluster?,The global service cluster is the center for unified storage and query of multi-cluster observation data. Sub-clusters are required to report the collected observation data to the global management cluster for unified storage.,
What is the value of the OTEL_RESOURCE_ATTRIBUTES environment variable?,"k8s.container.name=voting-svc,k8s.deployment.name=voting,k8s.deployment.uid=79e015e2-4643-44c0-993c-e486aebaba10,k8s.namespace.name=default,k8s.node.name=$( OTEL_RESOURCE_ATTRIBUTES_NODE_NAME),k8s.pod.name=$(OTEL_RESOURCE_ATTRIBUTES_POD_NAME),k8s.pod.uid=$(OTEL_RESOURCE_ATTRIBUTES_POD_UID),k8s.replicaset.name=voting-84b696c897,k8s.replicaset.uid=63f561 67-6632-415d-8b01-43a3db9891ff .",
What permissions are configured in the container's security context?,"SYS_PTRACE, and set runAsUser to 0 and privileged to true (that is, privileged mode).",
What volumes are mounted in the container?,launcherdir and kube-api-access-gwj5v.,
Which image is used for the container?,keyval/otel-go-agent:v0.6.0.,
What is the value of the OTEL_EXPORTER_OTLP_ENDPOINT environment variable?,http://insight-agent-opentelemetry-collector.insight-system.svc.cluster.local:4317.,
What environment variables are defined in this code?,"PROM_PORT,OTEL_TRACES_EXPORTER,OTEL_EXPORTER_OTLP_ENDPOINT,OTEL_EXPORTER_OTLP_TIMEOUT,SPLUNK_TRACE_RESPONSE_HEADER_ENABLED,OTEL_SERVICE_NAME,OTEL_RESOURCE_ATTRIBUTES_POD_NAME,OTEL_RESOURCE_ATTRIBUTES_POD_UID,OTEL_RESOURCE_ATTRIBUTES_ NODE_NAME, OTEL_PROPAGATORS, OTEL_TRACES_SAMPLER and OTEL_RESOURCE_ATTRIBUTES.",
How to install Instrumentation CR?,"Just install it under the Insight-system namespace. The sample code is as follows:\n```bash\nkubectl apply -f - <<EOF\napiVersion: opentelemetry.io/v1alpha1\nkind: Instrumentation\nmetadata:\nname: insight-opentelemetry-autoinstrumentation\nnamespace: insight-system\nspec :\n# https://github.com/open-telemetry/opentelemetry-operator/blob/main/docs/api.md#instrumentationspecresource\nresource:\naddK8sUIDAttributes: true\nenv:\n- name: OTEL_EXPORTER_OTLP_ENDPOINT\nvalue: http://insight-agent-opentelemetry-collector.insight-system.svc.cluster.local:4317\nsampler:\n# Enum: always_on, always_off, traceidratio, parentbased_always_on, parentbased_always_off, parentbased_traceidratio, jaeger_remote, xray\ntype: always_on \njava:\nimage: ghcr.m.daocloud.io/open-telemetry/opentelemetry-operator/autoinstrumentation-java:1.17.0\nenv:\n- name: OTEL_JAVAAGENT_DEBUG\nvalue: ""false""\n- name: OTEL_INSTRUMENTATION_JDBC_ENABLED \nvalue: ""true""\n- name: SPLUNK_PROFILER_ENABLED\nvalue: ""false""\nnodejs:\nimage: ghcr.m.daocloud.io/open-telemetry/opentelemetry-operator/autoinstrumentation-nodejs:0.31.0\npython: \nimage: ghcr.m.daocloud.io/open-telemetry/opentelemetry-operator/autoinstrumentation-python:0.34b0\ndotnet:\nimage: ghcr.m.daocloud.io/open-telemetry/opentelemetry-operator/autoinstrumentation-dotnet :0.3.1-beta.1\nEOF\n````",
What are the prerequisites for using the "automatic probe enhancement" method?,"To use the ""automatic probe enhancement"" method, you need to ensure that the Insight Agent is ready, the trace function is enabled for the Insight-agent, and the address and port of the trace data are filled in correctly, as well as deployment/opentelemetry-operator-controller-manager and deployment/insight- The Pod corresponding to agent-opentelemetry-collector is ready.",
What is the "automatic probe enhancement" method?,"""Automatic probe enhancement"" is an automatic enhancement method based on eBPF, which can automatically add OpenTelemetry-related environment variables and Golang ebpf probe containers to applications.",
What are the verified operating systems mentioned in this document?,"The verified operating system mentioned in this document is UnionTech OS Server 20 1050d, and the recommended kernel is 4.19.0-server-amd64.",
What is the main content of this document?,This document describes how to deploy DCE 5.0 commercial version on Other Linux operating systems.,
What dependent components need to be installed on the Tinder machine? What are the version requirements?,"The Tinder machine needs to install seven dependent components: podman, helm, skopeo, kind, kubectl, yq and minio client. The corresponding version requirements for these components are v4.4.1, ≥ 3.11.1, ≥ 1.11.1, v0.17.0, ≥ 1.25.6, ≥ 4.31.1 and mc.RELEASE.2023-02-16T19-20-11Z. If these dependent components do not exist, you need to install them through a script.",
Which check items in the pre-check have requirements on the system resources of the machine?,"The check items that require the machine's system resources in the pre-check include: user permissions, swap, time synchronization, and nameserver. Among them, user permissions must be deployed using the root user, swap must be closed to avoid io surges, time synchronization requires that all cluster nodes must be time synchronized, and nameserver requires that /etc/resolv.conf has at least one nameserver.",
4 What server roles need to be prepared for node mode? What are the required configurations for each?,"The 4-node mode requires one fire node and three master nodes. The Tinder node needs to run as the image warehouse and chart museum required to execute the installation and deployment program and run it. It is required to have a 2-core CPU, 4G memory, and a 200G system hard disk. The master node requires an 8-core CPU, 16G memory, 100G system hard disk and 200G unpartitioned hard disk.",
What server roles are required for all-in-one mode? What are the required configurations for each?,"The all-in-one mode requires preparing a server, which needs to run as three roles: a mirror warehouse, a chart museum, and the global cluster itself. The server requires a 16-core CPU, 32G memory, 200G system hard disk and 400G unpartitioned hard disk.",
What machine preparation is required for DCE 5.0 deployment?,"The deployment of DCE 5.0 requires the preparation of three machines: all-in-one mode, 4-node mode, and 7-node mode, which require different numbers and configurations of server roles respectively.",
What are the known issues with DCE 5.0?,"Unpartitioned SSD disks are not supported in the default installation mode\n- Pure offline environment, no app store by default\n- There are known issues in the metallb community. There is a dadfailed IPV6 loopback address on the main network card, and metallb cannot work\n- If the machine is too stuck during insight-api-server startup, the database initialization (migrate) cannot be completed during the Liveness health check cycle\n- The iso path in the clusterConfig configuration file must be an absolute path, and relative paths are not supported\n- The default k8s version and offline package of kubean are still limited to k8s 1.24 version, and have not been updated to 1.25 (PG is not supported yet)\n- external-Registry If it is Harbor, the Project will not be automatically created for the time being, and needs to be created manually in advance\n- When Docker is running, the built-in repository cannot be pulled\n- After disabling IPV6, podman cannot start the kind cluster of the Tinder node",
What issues are fixed in DCE 5.0?,HwameiStor's operator randomly errors in cleaning up jobs in the cluster\n- mysql enhances semi-synchronization to solve the problem of data inconsistency in frequent master-slave switching\n- Mysql disk is overwritten by clusterPedia\n- Various sub-modules Problems with mirroring and helm\n- Problems with svc on public cloud,
What features have been optimized in DCE 5.0?,"- MinIO supports the use of VIP\n- coredns automatically injects VIP analysis into the warehouse\n- Optimizes the offline package production process and accelerates packaging of Docker images\n- Infrastructure support 1.25: Upgrade redis-operator, eck-operator, hwameiStor and other components\n- Upgrade to keycloakx\n- Istio version upgraded to v1.16.1",
What new features are added in DCE 5.0?,- ARM64 support: Build ARM64 offline package\n- Kylin v10 sp2 offline package\n- Infrastructure support 1.25\n- Support cluster deployment in private key mode\n- Workload elastic scaling based on custom indicators,
What are the known issues mentioned in the installer Release Notes?,"Known issues include possible installation failures on non-minimized REHL8 installations, potential risks during helm concurrent installation, and other issues. See Release Notes for details.",
What new features are added in v0.4.0 in the installer Release Notes?,"v0.4.0 adds new features such as clusterConfig syntax upgrade from v1alpha1 to v1alpha2, support for cluster deployment in private key mode, and new installation of contour and cert-manager.",
What new features are added in v0.5.0 in the installer Release Notes?,"v0.5.0 adds new features such as offline package separation osPackage, support for addon offline, and support for REHL 8.4 and REHL 7.9 operating systems.",
What is optimized in v0.6.1 in the installer Release Notes?,v0.6.1 optimizes operations such as upgrading Kpanda to v0.16.1 and Skoala to v0.19.4.,
What new features have been added to v0.7.0 in the installer Release Notes?,"v0.7.0 adds support for Other Linux to deploy DCE 5.0, support for the operating system OpenEuler 22.03, support for external OS Repos and kernel parameter tuning and other new features.",
What are the minimum hardware requirements for kind cluster nodes?,"The minimum hardware requirements for kind cluster nodes are as follows: CPU > 10 cores, memory > 12 GB, and disk space > 100 GB.",
For what purposes is a kind cluster suitable?,kind clusters are recommended for testing and development only.,
What is the recommended Kubernetes version?,It is recommended to use the highest official stable version of K8s. Currently v1.24 is recommended and v1.20 is the minimum supported.,
What are the node hardware requirements for a standard Kubernetes cluster?,"The node hardware requirements of a standard Kubernetes cluster are as follows: 3 Master + 2 Workers; Master node: 4 Core, 8 G; system disk 100G; Worker node: 4 Core, 8 G; system disk 100G.",
What types of Kubernetes clusters can DCE 5.0 be installed on?,DCE 5.0 can be installed on standard Kubernetes clusters and kind clusters.,
Do I need authorization after successful installation?,"Yes, you need to contact Daocloud for authorization after successful installation. You can contact Daocloud via email at info@daocloud.io or call 400 002 6898 for authorization.",
How to use DCE 5.0 after installation is complete?,"The command line will prompt that the installation is successful and display the URL, account and password (admin/changeme). Use the prompted URL to explore the new DCE 5.0.",
How to start installing DCE 5.0?,"Download and unzip the offline package on the Control plane node. After modifying the cluster configuration file and manifest file, execute the following command to start installing DCE 5.0:\n```bash\n./offline/dce5-installer install-app -m. /offline/sample/manifest.yaml -c ./offline/sample/clusterConfig.yaml --platform openshift -z\n````",
How to configure manifest file?,"This file can be obtained under the offline package offline/sample and modified as needed. If you want to use hwameiStor as StorageClass, please make sure there is no default StorageClass in the current cluster. If so, the default StorageClass needs to close hwameiStor, that is, change the value of enable to false.",
How to set the cluster configuration file clusterConfig.yaml?,This file can be obtained under the offline package offline/sample and modified as needed. The reference configuration is:\n```yaml\napiVersion: provision.daocloud.io/v1alpha3\nkind: ClusterConfig\nmetadata:\ncreationTimestamp: null\nspec:\nloadBalancer:\ntype: metallb # Recommended metallb\nistioGatewayVip: 10.5.14 .XXX/32\ninsightVip: 10.5.14.XXX/32\nfullPackagePath: /home/offline # Offline package directory\nimagesAndCharts:\ntype: external\nexternalImageRepo: http://10.5.14.XXX:XXXX # Private image warehouse Address\nexternalImageRepoUsername: admin\nexternalImageRepoPassword: Harbor12345\n````,
How to download the full mode offline package?,"The latest version can be downloaded from the Download Center. Select the corresponding version according to the required CPU architecture, such as amd64 or arm64. After downloading, unzip the offline package.",
What are the prerequisites for installing DCE 5.0?,The prerequisites for installing DCE 5.0 include: having an OCP environment with a version no lower than v1.22.x; preparing a private image warehouse and ensuring that the cluster can access it; sufficient resources. It is recommended that the cluster has at least 12 cores and 24 GB available. resource.,
What Kubernetes versions are supported by default in DCE 5.0?,"The Kubernetes versions supported by DCE 5.0 by default are v1.22.x, v1.23.x, v1.24.x, and v1.25.x.",
What operations need to be performed after successfully installing DCE 5.0 Community Edition?,"Genuine authorization is required. For details, please refer to [Apply for free community experience] (../../../dce/license0.md).",
How to access the DCE 5.0 console after installation is complete?,Record the URL of the prompt and use the default account and password (admin/changeme) to access it.,
What parameters need to be paid attention to during installation?,- `-p`: Specify the offline directory to decompress the offline package. \n- `-c`: Specify the cluster configuration file. There is no need to specify `-c` when using NodePort to expose the console. \n- `-z`: Minimize installation. \n- `-d`: Enable debug mode. \n- `--serial`: All installation tasks will be executed serially after being specified.,
How to download and install DCE 5.0 Community Edition?,"1. Download the dce5-installer binary file on the K8s cluster control plane node and set it as executable. \n2. Set the configuration file clusterConfig.yaml. \n3. Execute the installation command: `./dce5-installer install-app -c clusterConfig.yaml -z`. \n4. After successful installation, DCE 5.0 can be accessed through the prompted URL.",
What preparations are required for installation of DCE 5.0 Community Edition?,"- Prepare a K8s cluster and set up storage and CoreDNS\n- Install dependencies, including helm, skopeo, kubectl and yq\n- Set up the configuration file clusterConfig.yaml",
How to observe the startup status of Pod during the installation process?,You can use the command `docker exec -it fire-kind-cluster-control-plane kubectl get po -A -w`.,
What dependencies are required to install DCE 5.0 Community Edition?,"helm ≥ 3.11.1, skopeo ≥ 1.11.1, kubectl ≥ 1.25.6 and yq ≥ 4.31.1.",
How to install DCE 5.0 Community Edition?,"You can follow the steps in the document, including downloading the dce5-installer binary file, obtaining the IP of the host where kind is located, starting the installation of DCE 5.0, etc.",
"In a kind cluster, which port in the cluster is exposed?",Port 32088.,
How to install kind cluster?,"You can follow the steps in the document, including downloading the kind binary file, creating a configuration file, creating a cluster, etc.",
"If Podman is already on the machine but Docker is not available, do I need to install Docker?","need. Because after Podman starts kind, there will be problems with insufficient file handles and mismatched IPs.",
How to install Docker?,Different installation scripts can be executed according to the operating system. Please refer to the documentation for the script content.,
How to check system resources and networking status?,Just execute the following script:\n```shell\nset -e\nif [ $(free -g|grep Mem | awk "{print $2}") -lt 12 ]; then (echo "insufficient memory! (should >=12G)"; exit 1); fi\nif [ $(grep "processor" /proc/cpuinfo |sort |uniq |wc -l) -lt 8 ]; then (echo "insufficient CPU! (should >=8C )"; exit 1); fi\nif [ $(df -m / |tail -n 1 | awk "{print $4}") -lt 30720 ]; then (echo "insufficient free disk space of root partition!(should >=30G)"; exit 1); fi\nping daocloud.io -c 1 &> /dev/null || ( echo "no connection to internet! abort." && exit 1; )\necho "precheck pass.. "\n```,
What are the machine resources required to install DCE 5.0 Community Edition?,"CPU > 8 cores, RAM > 12 GB, Disk space > 100 GB.",
How to log in to the console after installation is complete?,"After the installation is complete, you will be prompted with the URL address of the console and the default account and password (admin/changeme). You can log in to the console through this address.",
How to perform a minimal installation?,You can perform a minimal installation by adding the `-z` parameter when executing the command.,
How to check the system family ID of the current environment?,"You can view the system family ID of the current environment by executing the following command:\n```\nexport USER=root\nexport PASS=xxxx\nexport ADDR=192.168.10.xxx\nexport ANSIBLE_HOST_KEY_CHECKING=False\nansible -m setup -a "" filter=ansible_os_family"" -e ""ansible_user=${USER} ansible_password=${PASS}"" -i ${ADDR}, all\n```",
What preparations are needed before installing DCE 5.0?,"Before installing DCE 5.0, you need to ensure that the machine meets the system environment requirements and refer to the official documentation for configuration; you need to prepare the installation package, cluster configuration file, etc.",
Where is the download address for the installation package of DCE 5.0?,"The installation package of DCE 5.0 can be downloaded from the official website, and the download address is: https://www.daocloud.io/mirror-dce.",
How to add differentiated configuration items to the image creation task?,"You can add corresponding differentiated configuration items in the left list area. After adding a differentiated configuration item, you need to specify the corresponding cluster. The selectable range of clusters is only the cluster selected at the beginning.",
How to perform differentiated configuration in the image creation task?,"On the differentiated configuration page, select the personalized container configuration, labels and annotations, and click OK.",
How to perform advanced configuration in the image creation task?,"In the advanced configuration page, you can configure task settings and labels and annotations.",
How to configure container information in the image creation task?,"On the container configuration page, configure the basic information of the container where the load is located, and optionally configure life cycle, health check and other information.",
How to choose the scheduling strategy in the image creation task?,"Scheduling strategies are divided into three methods:\n- Repeat: schedule the same number of instances in all selected clusters\n- Aggregation: schedule the smallest possible total number of instances in all selected clusters\n- Dynamic Weight: Based on the real-time maximum number of schedulable instances in all selected clusters, the total number of filled instances will be run in equal proportion.",
How to choose a deployment cluster in the image creation task?,"There are three ways to deploy a cluster:\n-Specify the cluster: Specify the cluster by selecting the cluster name\n-Specify the region: Specify the cluster by selecting the vendor, region, and availability zone\n-Specify the label: Specify the cluster by adding indicators",
How to configure the basic information of the load in the image creation task?,The following basic information can be configured in the image creation task:\n- Load name\n- Multi-cloud namespace\n- Deployment cluster\n- Number of instances\n- Scheduling policy\n- Description,
How to create an image creation task for multi-cloud workloads?,"In the left navigation bar, click `Multi-cloud Workloads` to enter the task page, and click the `Image Creation` button in the upper right corner. On the create task page, after configuring the basic information of the load, click Next.",
How do you control which clusters can deploy workloads or containers?,"Click the `…` button on the right side of the list and select `Modify cluster taint`. Enter the key value information of the stain in the pop-up box. The value can be empty. Select the stain effect from the drop-down list (currently NoSchedule and NoExecute are supported), and click OK. It supports adding multiple stains.",
What are the operations to connect to a new cluster?,"After clicking the `Connect to Cluster` button, the page for selecting nodes in container management will pop up. You can select a working node and click OK. If the nodes in the current list do not meet the requirements, it is also supported to connect new nodes in the container management module.",
How to access the working cluster?,"Click `Working Cluster Management' in the left navigation bar to enter the multi-cloud cluster management page. The list includes all working clusters in the current multi-cloud instance. If you want to access other working clusters, click the `Connect to Cluster' button in the upper right corner. . If you need to connect the network between working clusters, please create a grid instance on the service grid page and manage the following working clusters; for details, please [view the document](https://docs.daocloud.io/mspider/ user-guide/multicluster/cluster-interconnect/).",
What should I pay attention to when deleting a multi-cloud namespace?,"To delete a namespace, you need to remove all workloads under the namespace first. After deletion, the workloads and services in the namespace will be affected, so please operate with caution.",
What operations are possible in the list of multi-cloud namespaces?,You can edit the YAML and delete the namespace.,
How to create a multi-cloud namespace?,"You can follow the steps below:\n1. After entering a multi-cloud instance, in the left navigation bar, click `Resource Management` -> `Multi-cloud Namespace`, and click the `Create` button in the upper right corner. \n2. In the `Create Multi-Cloud Namespace` page, enter the name, add the label, and click `OK`.",
What does a multi-cloud namespace do?,Multi-cloud namespaces can manage workloads across clouds and clusters.,
How to verify whether multi-cloud nginx is running successfully?,Pods need to run successfully in both clusters and be accessible normally.,
What operations need to be performed after the conversion is successful?,"After the conversion is successful, you need to update the target deployment cluster and enable automatic propagation; update the deployment strategy of the service and select the deployment cluster.",
How to perform one-click transfer?,"Enter `Multi-cloud Workload - Stateless Load`, click Experience Now, select the target application, and its associated service will be automatically selected, and the associated configuration items and keys will also be synchronously converted.",
What are the prerequisites for one-click transfer?,The prerequisite is that the container management module is connected to a cluster whose cluster manufacturer is DaoCloud DCE and can access the UI interface of the cluster; the workload in the DCE4 cluster can run normally.,
What is covered in this section?,"This section introduces how to realize one-click conversion of DCE4 to DCE5 applications through the multi-cloud orchestration interface, taking nginx as an example for demonstration.",
How to optimize the configuration of multi-cloud workloads?,"The configuration of multi-cloud workloads is mainly related to its deployment policy (PP), and the corresponding cluster taint tolerance period needs to be modified in the deployment policy.",
How to configure failover for multi-cloud instances?,Enter the Advanced Settings->Failover section and fill in the parameter information provided in the document.,
What is the graceful eviction timeout?,"When a workload enters the graceful eviction queue, the maximum waiting time is the time configured by ClusterTaintEvictionRetryFrequency, and it will be deleted immediately after timeout.",
What is the eviction tolerance period?,"When a cluster is marked as unhealthy and exceeds the FailoverEvictionTimeout configured time, the cluster will be tainted and enter the eviction state (that is, increase the eviction taint).",
How to mark a cluster as unhealthy?,"When the cluster has not obtained health status information after the time configured by ClusterMonitorGracePeriod, it will be marked as unhealthy.",
How to optimize failover latency sensitivity?,Achieving the final effect of failover delay sensitivity requires a combination of configuration indicators in the cluster dimension and workload dimension. The cluster dimension includes the check duration to mark the cluster as unhealthy and the cluster eviction tolerance duration. The workload dimension includes the cluster taint tolerance duration.,
What parameter configuration options are available for the Failover feature?,"The Failover attribute has the following parameter configuration options: ClusterMonitorPeriod, ClusterMonitorGracePeriod, ClusterStartupGracePeriod, FailoverEvictionTimeout, ClusterTaintEvictionRetryFrequency.",
Who are the members of the platform technology team?,"The platform technical team includes a number of engineers responsible for R&D, operation and maintenance, and technical support. Details of the members can be found on the ""Technical Team"" page.",
Is there a declaration of open source software license information?,"Yes, the open source software statement lists each open source software used by the platform and declares the corresponding license information.",
What content does "About the Platform" display?,"""About the Platform"" displays product versions, open source software announcements, and technical teams. Among them, the open source software statement lists each open source software used by the platform and declares the corresponding license information. The technical team thanked the platform's technical team in the form of an animated video.",
How to view "About the Platform"?,"Log in to the Web console as a user with the `Admin` role, click `Global Management` at the bottom of the left navigation bar, select `Platform Settings`, and then select `About Platform` to view the product version, open source software statement and technical team.",
What is "About the Platform"?,"""About the platform"" is a sub-module in global management. It is used to display the currently updated versions of each sub-module of the platform, declare the various open source software used by the platform, and thank the platform's technical team in the form of animated videos.",
Gives an example of using an access key to make a GET request.,Refer to the following example:\n```\ncurl -X GET -H "Authorization:Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IkRKVjlBTHRBLXZ4Mmt,
How to access the OpenAPI interface documentation in DCE 5.0?,You can refer to [OpenAPI interface document](https://docs.daocloud.io/openapi/).,
What is `${token}`? How to get it?,"`${token}` is the access key, which can be obtained from the access key page of the personal center.",
Which request header needs to be added to the request to access DCE 5.0 openAPI?,You need to add the request header `Authorization:Bearer ${token}` to the request.,
How do I get the access key?,"In DCE 5.0, after logging in, enter the personal center and obtain the access key on the access key page.",
"Which role can view pipelines and run records, and copy, edit, and cancel run pipelines?","Workspace Admin,",
"Which role can create credentials, edit credentials information, and delete credentials?","Workspace Admin, Workspace Editor.",
"Which role can create namespaces, edit tags or resource quotas, and delete namespaces?",Workspace Admin.,
Which role can group or delete apps?,"Workspace Admin, Workspace Editor.",
Which role can update the copy count or edit the pipeline?,"Workspace Admin, Workspace Editor.",
Which role can edit YAML or update container images?,"Workspace Admin, Workspace Editor.",
Which role can create apps?,"Workspace Admin, Workspace Editor.",
"Which role can view the application list, application details, application logs, application monitoring, RabbitMQ details, service mesh and microservice engine?","Workspace Admin, Workspace Editor, Workspace Viewer.",
What three user roles does Application Workbench support?,"Workspace Admin, Workspace Editor, Workspace Viewer.",
Where is the filing information set?,Click the specified tab on the appearance customization page to set the filing information in the displayed module.,
How to perform advanced customization?,"Click the `Advanced Customization` button on the appearance customization page to edit CSS styles. If you want to restore the default settings, you can click the `One-click restore` button on the page. Please note that all custom settings will be discarded after a one-click restore.",
What is advanced customization?,"Advanced customization can modify the color, font spacing, font size, etc. of the entire container platform through CSS styles.",
How to customize the appearance?,"After logging in to the Web console as a user with the `admin` role, click `Global Management` -> `Platform Settings` at the bottom of the left navigation bar and select `Appearance Customization`. After modifying the corresponding icon, text, filing information, etc. in the specified tab, click `Save`.",
What is the role of appearance customization?,Appearance customization can change the logo on the login interface and top navigation bar to help users better identify the product.,
How to clean audit logs?,"Click `Manual Cleanup` in the `Clear Global Management Audit Logs` card -> `Clear Now` button, enter the number of days to clean up, click the `Clear Now` button, the system will automatically clean up the audit logs before that number of days. It is recommended to export and save before cleaning. \n- Click the Automatic Cleanup-Settings button in the `Clear Global Management Audit Logs` card, enter the maximum storage time, click `OK`, and the system will automatically clean up all logs outside of this time. It is recommended to export and save before cleaning.",
How to download and export audit logs?,"1. Log in to the web console as a user with the `Admin` or `Audit Owner` role. \n2. Click `Global Management` at the bottom of the left navigation bar. \n3. Navigate to the audit log under global management and select `Audit log`. \n4. Under `Audit Log`, select the time range, click `Export`, and download the audit log to the local computer.",
What features does the audit log have?,#NAME?,
Are audit logging enabled by default? How long can it be recorded?,"Yes, audit logging is enabled by default when installing and using the platform, and 365 days of user behavior can be viewed within the platform by default.",
What is the purpose of audit logs?,"The audit log helps you monitor and record the activities of each user. It provides the collection, storage and query functions of security-related records arranged in chronological order, and is used to continuously monitor and retain the user's usage behavior in the global management module, including But it is not limited to user creation, user login/logout, user authorization, and user operation behaviors related to Kubernetes.",
How to add service discovery rules?,"In the details page of a cluster, click the shortcut link to jump to `Container Management` -> `Custom Resources` to add service discovery rules.",
How to enter the details page of a cluster?,"When the cluster is connected to `insight-agent` and is running, click the cluster name to enter the details page.",
How to check the status of all cluster collection plug-ins?,Select `Collection Management` in the left navigation bar to view the status of all cluster collection plug-ins.,
What are the specific steps for collection management?,"Specific steps include: 1. Click on the upper left corner and select `Observability`. 2. Select `Collection Management` in the left navigation bar to view the status of all cluster collection plug-ins. 3. When the cluster is connected to `insight-agent` and is running, click `cluster name` to enter the details. 4. Click the shortcut link to jump to `Container Management` -> `Custom Resources` to add service discovery rules.",
What is collection management?,"`Collection management` is mainly the entrance to centralized management and display of the cluster installation collection plug-in `insight-agent`, helping users quickly check the health status of the cluster collection plug-in, and provides a quick entrance to configure collection rules.",
What specific metric names and descriptions are provided in Monitoring Metrics?,"Monitoring indicators provide specific indicator names and descriptions such as CPU usage, CPU requests, CPU limits, memory usage, memory requests, memory limits, disk read and write rates, and network sending and receiving rates.",
What indicator trends does the monitoring indicator card provide?,"The monitoring indicator card provides the change trend of CPU, memory, network and disk of the workload for a default 1 hour.",
What resource usage can be viewed in the resource consumption card?,"In the resource consumption card, you can view the CPU, memory, and network usage of the workload.",
What information is collected in the fault card?,The fault card counts the total number of alarms currently being generated by the workload.,
What's included in a container monitoring card?,"The container monitoring card includes three aspects: faults, resource consumption, and monitoring indicators.",
How do I view target workload details?,Click the target workload name to view details.,
How do I view data for different types of workloads?,Switch the top tab to view data for different types of workloads.,
How to enter container monitoring?,"In the observability product module, select `Resource Monitoring->Container Monitoring` through the left navigation bar to enter.",
What are the prerequisites for container monitoring?,"Insight Agent has been installed on the cluster, and all container groups are in the running state.",
What is container monitoring?,"Container monitoring is the monitoring of workloads in cluster management. The basic information and status of the workload can be viewed in the list. On the workload details page, you can view the number of ongoing alarms and changing trends in resource consumption such as CPU and memory.",
How to switch dashboards?,Click the title area to switch the dashboard.,
How to set relevant parameters to view cluster indicators?,"Taking `Insight Overview` as an example, select a cluster and view the resource usage in the selected cluster.",
How to select a dashboard in the navigation bar?,Select `Dashboard` in the left navigation bar.,
Observability Insight Which open source tool does it use to provide monitoring services?,Observability Insight uses the open source Grafana to provide monitoring services.,
What is Grafana?,Grafana is a cross-platform open source visual analysis tool.,
What does the global jaeger collector show? what's the effect?,The global jaeger collector shows how the jaeger collector of the global management cluster receives data from the otel collector in the global management cluster and sends link data to the Elasticsearch cluster. It can determine whether the `jaeger collector` of the global service cluster is running normally.,
What does the global opentelemetry collector show? what's the effect?,"The global opentelemetry collector shows how the `opentelemetry collector` of the `global service cluster` receives link data from the `otel collector` in the `working cluster` and sends aggregate link data. It can determine whether the `opentelemetry collector` of the global service cluster is running normally, and is responsible for sending the [audit log] (.. /../ghippo/user-guide/audit-log.md) and Kubernetes audit logs (not collected by default) to the `audit server` component of the global management module.",
What does the workload opentelemetry collector show? what's the effect?,The workload opentelemetry collector shows how the `opentelemetry collector` of different working clusters receives language probe/SDK link data and sends aggregate link data. You can select the cluster through the `Cluster` selection box in the upper left corner. It can determine whether the cluster's `opentelemetry collector` is running normally.,
What blocks does the Insight Tracing Debug dashboard consist of? What content are they responsible for monitoring?,"The Insight Tracing Debug dashboard consists of three blocks, which are responsible for monitoring the data of different clusters and transmission links of different components. They are: workload opentelemetry collector, global opentelemetry collector, global jaeger collector.",
What steps should I take if I find that I have no link data after applying link enhancements?,"You can enter the `Observability` of the DCE 5.0 platform, select `Dashboard` in the left navigation bar, switch to the `insight-system` -> `insight tracing debug` dashboard, and check the link data transmission through the generated timing chart Is there a problem.",
What is the link data transmission diagram?,The link data transmission diagram is a diagram showing the transmission path of link data in the system.,
What is the minimum required version of the client browser?,"The Firefox version needs to be greater than or equal to 49, and the Chrome version needs to be greater than or equal to 54.",
What is the HTTPS access port of Istio-Gateway VIP in the global cluster?,The HTTPS access port is TCP 443.,
Which port needs to be opened for access to the mirror warehouse?,The mirror warehouse access entrance needs to open TCP port 443.,
"If SpiderPool is used as CNI, which ports need to be opened?","TCP 5710, TCP 5711, TCP 5712, TCP 5720, TCP 5721, TCP 5722, TCP 5723 and TCP 5724 ports need to be opened.",
"In order for the global management cluster to access the working cluster, what ports need to be opened?",TCP port 22 (SSH access to each node) and TCP port 6443 (k8s API access entrance) need to be opened.,
What network requirements are needed to install DCE?,"When installing DCE, you need to reserve two IP address segments for use by Pod and Service, open ports, and ensure that the network speed is not lower than Gigabit. Ten Gigabit is recommended. You also need to ensure that there are accessible NTP server IP addresses and DNS server IP addresses.",
What are the hardware requirements for installing DCE 5.0?,"Before installing DCE 5.0, the CPU and memory must not be oversold, and the IOPS of the hard disk > 500 and the throughput > 200 MB/s.",
What operating systems does DCE 5.0 support?,"DCE 5.0 supports CentOS 7.X, Redhat 8.X/7.X, Ubuntu20.04, Tongxin UOS V20 (1020a), openEuler 22.03 and other operating systems. For details, see the installation and deployment document.",
What planning needs to be done before deploying DCE 5.0?,"Before deploying DCE 5.0, you need to complete software planning, hardware planning, and network planning.",
How to decompress the offline image package on the Tinder node?,"Use the tar command to decompress the downloaded offline image package, such as:\n```bash\n# Decompress the v0.4.0 version of the AMD architecture\ntar -xvf offline-centos7-$VERSION-amd64.tar\n```",
How to download offline image package on Tinder node?,"On the Tinder node, use the wget command and add the download link of the corresponding offline image package, such as:\n```shell\n# Download the v0.4.0 version of the AMD architecture\nexport VERSION=v0.4.0\nwget https ://proxy-qiniu-download-public.daocloud.io/DaoCloud_Enterprise/dce5/offline-centos7-$VERSION-amd64.tar\n````",
How to download ISO offline package?,- Confirm the required CPU architecture and operating system version. \n- Get the download address of the ISO offline package. \n- Go to the corresponding website to apply or download directly from the mirror site.,
How to download offline image package?,#NAME?,
How to solve the error when running `yum install docker`?,"You can try to install `glibc-langpack-en`. If the problem persists, you can execute the following command:\n```bash\nsed -i ""s/mirrorlist/#mirrorlist/g"" /etc/yum.repos.d/ CentOS-*\nsed -i ""s|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g"" /etc/yum.repos.d/CentOS-*\ nsudo yum update -y\n```",
How to solve the error when installing CentOS 7.6?,"Execute the `modprobe br_netfilter` command on each node where the global service cluster is installed, and load `br_netfilter` to solve the problem.",
How to solve the problem of community version fluent-bit installation failure?,"Check the Pod log to see if `getaddrinfo(host=""mcamel-common-es-cluster-masters-es-http.mcamel-system.svc.cluster.local"",errt11): Could not contact DNS servers` key information appears, if so Please refer to https://github.com/aws/aws-for-fluent-bit/issues/233 for solution.",
How to solve the problem of Redis getting stuck when reinstalling DCE 5.0 in Kind cluster?,"Delete rfs-mcamel-common-redis under the mcamel-system namespace, and then re-execute the installation command.",
How to solve the problem of Podman failing to create a container after disabling IPv6?,You can re-enable IPv6 or update the Tinder node base to Docker. Reference link: https://github.com/containers/podman/issues/13388,
How to restore the Podman container after restarting the Tinder node?,"Execute `podman exec` After entering the container, execute `sed -i ""s/server: .*:6443/server: https://127.0.0.1:6443/g"" /etc/kubernetes/controller-manager.conf` and `sed -i ""s/server: .*:6443/server: https://127.0.0.1:6443/g "" /etc/kubernetes/scheduler.conf` command.",
How to contact the official for further help?,"If you have any questions, please go to the official Github to submit feedback or scan the QR code to join the community communication group.",
What is the installation process for the commercial version?,"The installation process of the commercial version is: Deployment architecture --> Deployment requirements --> Preparation --> Download offline package --> Edit and configure the clusterConfig.yaml file --> Start installation. For specific steps, please refer to the official link.",
What is the installation process for the Community Edition?,The installation process of the community version is: Resource planning --> Prepare K8s cluster --> Install dependencies --> Install online through kind cluster or online/offline installation through standard k8s cluster. Detailed steps can be obtained at any time through the official link. .,
How many versions of DCE 5.0 are there? What modules does each include? How to obtain authorization for the community edition?,"DCE 5.0 has two editions: community edition and commercial edition. The community edition includes three modules: global management, container management and observability, and is licensed permanently for free. The commercial version contains multiple modules, which can be freely combined as needed. You need to contact the official for authorization.",
Which factor is directly proportional to the storage capacity of the indicator storage component vmstorage in the global service cluster? Which guide is used to size vmstorage disks for multi-cluster scaling?,"The storage capacity of the indicator storage component vmstorage in the global service cluster is directly proportional to the total number of container groups in each cluster. \n- Contact the platform administrator to adjust the disk capacity of vmstorage according to the cluster size, refer to [vmstorage disk capacity planning] (../../best-practice/vms-res-plan.md) \n- Adjust according to the scale of multiple clusters vmstorage disk, refer to [vmstorge disk expansion](../../best-practice/modify-vms-disk.md).",
"How is the resource consumption of Prometheus, the indicator collection component in Insight-Agent, related to the number of containers? How to adjust Prometheus resources?",The resource consumption of the indicator collection component Prometheus in Insight-Agent is directly proportional to the number of container groups running in the cluster. Please adjust the resources of Prometheus according to the cluster scale. Reference: [Prometheus Resource Planning](../../best-practice/prometheus-res.md).,
How to troubleshoot the abnormal status of insight-agent?,The following operations need to be performed:\n- Execute the command `helm list -n insight-system` to view the deployment status\n- Execute the command `kubectl get pods -n insight-system` or view it in `Observability->Collection Management` The status of components deployed in this cluster and restarting the abnormal container group.,
What may be the reason why insight-agent is in an abnormal state?,helm deployment failed or deployed components are in a non-running state.,
How to check the installation status of `insight-agent` in each cluster?,This can be viewed in the `Observability` -> `Collection Management` section.,
In which namespace does the user need to install `insight-agent`?,Installed under the `insight-system` namespace.,
"In DCE 5.0, which component can achieve unified collection of observation data from multiple clusters?",Insight.,
How do I configure notifications?,"When creating or editing an alarm policy, you need to configure notifications, including notification methods and recipients.",
What are the levels of alarm levels?,"Alarm levels include emergency, warning, and information.",
How to delete an alert policy?,Click `⋮` on the right side of the list and select `Delete` in the pop-up menu to delete the alarm policy.,
How to check which rules have been set under an alarm policy?,"Click on the alert policy, and the set rules will be displayed on the details page.",
What does an alert strategy consist of?,An alert policy consists of a set of alert rules.,
"In addition to the built-in alarm policies, what else does DCE 5.0 support?","In addition to the built-in alarm policies, the alarm policies of DCE 5.0 can also be customized to create new alarm policies.",
"In Alertmanager configuration, how to set the repeat sending interval of alarm notification?","In the Alertmanager configuration, you can specify the repeat sending interval of alarm notifications by setting the `repeat_interval` parameter. After Alertmanager sends an alert notification to the receiver, if alerts with the same label and content continue to be received within the time specified by the `repeat_interval` parameter, Alertmanager will send the alert notification repeatedly.",
"If no more alerts with the same label and content are received within the specified time, how will Alertmanager handle it?","If no more alerts with the same label and content are received within the specified time, Alertmanager will send all such alerts received to the receiver after that time.",
"In the Alertmanager configuration, how to set how long to wait before merging them into a single notification when there are several qualified alerts at the same time?","In the Alertmanager configuration, the `group_interval` parameter can be set to specify how long to wait before multiple qualifying alerts from the same group occur before they are merged into a single notification.",
What is the role of the `group_wait` parameter in Alertmanager?,"The `group_wait` parameter is used to set the waiting time for alarm notification. When Alertmanager receives a group of alerts, if there are no more alerts within the time specified by `group_wait`, Alertmanager will wait for a period of time to collect more alerts with the same label and content, and add all alerts that meet the conditions. to the same notification.",
"In the alarm policy, why is there a deviation in the actual sending interval of alarm notifications?","There are two parameters `group_interval` and `repeat_interval` in the native configuration of Alertmanager, which will cause deviation in the actual sending interval of alarm notifications. When the `group_wait`, `group_interval` and `repeat_interval` parameters are set at the same time, Alertmanager will process alarm notifications under the same group in a certain order.",
What are the advantages of Insight's Agent?,Insight's Agent is a lightweight Agent that supports one-click installation through Helm.,
Does Insight provide built-in alerting rules? How to implement monitoring out of the box?,"Yes, Insight provides built-in alerting rules. Without configuration, users can use built-in alarm rules to monitor basic indicators such as cluster resources and system components out of the box.",
What pre-built monitoring dashboards does Insight provide out of the box?,"Insight provides a variety of pre-made monitoring dashboards that can be used out of the box. Users can achieve comprehensive monitoring of clusters, nodes, workloads and other components through the built-in dashboards.",
How is multi-cluster management implemented?,"Multi-cluster management uses the Global cluster to uniformly store indicators, logs, and link data to achieve unified monitoring of multiple clusters. Users can query aggregated queries of multi-cluster data.",
What are the advantages of observability?,The advantages of observability are:\n-Multi-cluster management\n-Out-of-the-box\n-High availability\n-Open source compatibility,
What resource usage metrics can be seen in cluster monitoring?,"In cluster monitoring, you can see indicators such as CPU usage, CPU allocation rate, memory usage, and memory allocation rate.",
How can I view the details of a target cluster?,Select the target cluster in the cluster list to view the detailed information of the cluster.,
What conditions need to be met before using cluster monitoring?,"Before using cluster monitoring, insight-agent must be installed in the cluster and the application must be running.",
What information can be viewed through cluster monitoring?,"Cluster monitoring can view the basic information of the cluster, resource consumption in the cluster, and resource consumption trends over a period of time.",
In which version is the full life cycle management of the mirror warehouse added?,"In the v0.4 version, full life cycle management of the mirror warehouse is added.",
What new features are added in v0.5?,"The v0.5 version adds a number of new features, including support for creating hosted `harbor`, supporting `NodePort` mode exposure, and verifying whether the port is occupied, supporting creating hosted `harbor`, and supporting enabling `DCE 5.0 ODIC` access and implementation Use `DCE 5.0` user to log in to `Harbor` and so on.",
Which issue is fixed in v0.6.2?,The v0.6.2 version fixes the problem that the installation bytes of harbor-operator helm are too large.,
What new features are added in v0.7?,"The v0.7 version adds new functions such as supporting image synchronization between multiple instances of page configuration, supporting association of `jfrog` type warehouse in the workspace, and supporting the creation of managed `harbor` in an offline `ARM` environment.",
What does the version number in the image warehouse Release Notes mean?,"The version number is an identifier that identifies different versions in the image warehouse Release Notes, usually indicating different versions of software or systems.",
How to scan in native Harbor image?,"In the integrated warehouse list, hover the cursor over a warehouse, click the `Native Harbor` icon to jump to the native Harbor, and refer to the specific operation steps in its official documentation.",
How to check the scan results after manual scanning?,"After the manual scan is completed, hover the cursor on the scale bar to view the scan details.",
How to scan an image manually?,"Enter the `Image List` of the image warehouse, select an instance and image space, click on an image, and select `Scan` in the `⋮` on the right side of the list.",
What scanning methods does the image warehouse of DCE 5.0 support?,"The mirror warehouse of DCE 5.0 supports Trivy, Clair and Trivy scanning methods, depending on which plug-ins the user has installed.",
What is image scanning?,Image scanning is a proactive preventive measure used to scan images and obtain their security information to avoid malicious implantation of backdoors and improve image security.,
What permissions are required to add a target repository?,Adding a target repository requires administrator rights.,
How to add target warehouse?,"1. From the Warehouse Integration or Hosted Harbor page, click a warehouse. \n2. In the left navigation bar, click `Mirror Sync` -> `Target Warehouse`, and click the `Add Target Warehouse` button. \n3. You can click on the integrated warehouse or customize the warehouse.",
What is the purpose of the target mirror space?,The purpose of the target mirror space is to specify the target warehouse to which the mirror will be synchronized. The default is the same mirror space as the source warehouse.,
What are the options for sync mode?,"There are three options for synchronous mode: push, pull and overwrite.",
What permissions are required to create sync rules?,Creating sync rules requires administrator privileges.,
How to create sync rules?,"1. From the Warehouse Integration or Hosted Harbor page, click a warehouse. \n2. In the left navigation bar, click `Mirror Sync` -> `Synchronization Rules`, and click the `Create Synchronization Rule` button. \n3. After filling in various parameters, click `OK`.",
What is mirror synchronization?,Image synchronization refers to the process of synchronizing and updating the images between two or more image warehouses. It is often used in software development or system management to ensure that the same software or operating system image is shared between multiple servers so that the same software or operating system image can be shared between two or more image warehouses. Ability to ensure consistency and reduce workload when deploying.,
Which module is the mirror selection function linked to?,The image selection function is linked to the container management module.,
What does the image scan function do?,The image scanning function is used to identify image security risks.,
To whom can the mirror space be allocated?,Mirror space can be allocated to one or more workspaces (tenants).,
Which cloud-native infrastructure environments can the image warehouse be deployed to?,The image warehouse can be deployed to any cloud-native infrastructure environment.,
How many instances of life cycle management can the mirror warehouse support?,The mirror warehouse can support the life cycle management of multiple instances.,
"After integrating external Harbor or Docker Registry, which images can be selected for application deployment in the namespace under the workspace?",The namespace under the workspace can select the image in the warehouse for application deployment.,
What can workspace members see after integrating an external Harbor or Docker Registry?,Workspace members can see all images integrated with the warehouse in the image list.,
How can Workspace Admin of DCE 5.0 integrate external mirror repositories?,"Workspace Admin can integrate two external image warehouses, Harbor and Docker Registry, through warehouse integration.",
What information do I need to fill in to install Harbor Operator?,Name and namespace need to be filled in.,
How to find and install Harbor Operator in container management?,"Go to `Helm Application` -> `Helm Template` of `Container Management`, find and click the Harbor Operator card, select the version and click `Install`.",
What should I do if an exception prompt occurs when creating a Harbor instance?,Click `Go to Install` to enter the container management to install Harbor Operator.,
What is the minimum version to install Harbor Operator?,The minimum version requirement is 1.4.0.,
Which technology needs to be installed before creating managed Harbor?,Harbor Operator technology needs to be installed.,
"If I forget the password I set when creating hosted Harbor, how can I retrieve it?",No relevant information is provided in the text.,
How to pull a Docker image from the Harbor mirror space?,Use the command `docker pull <image warehouse address>/<image space name>/<image warehouse name>:<image version number>`,
How to push a marked Docker image to the Harbor repository?,Use the command `docker push <image warehouse address>/<image space name>/<image warehouse name>:<image version number>`,
How to tag an image?,Use the command `docker tag <image warehouse name>:<image version number> <image warehouse address>/<image space name>/<image warehouse name>:<image version number>`,
Which command can be used to log in to the image repository?,`docker login --username=<image warehouse login name> <image warehouse address>`,
How to create hosted Harbor and mirror space?,You can refer to the link provided in the article and follow the guidelines to create it.,
"How do I disable, edit or delete recycling rules?","In the image recycling list, click `⋮` on the right to choose to disable, edit or delete the corresponding recycling rules.",
How many recycling rules can a mirror space support at most?,A mirror space supports up to 15 recycling rules.,
How to create an image recycling rule for the current image space?,"Click `Mirror Space` on the left navigation bar and click a name in the list to enter the mirror space page. Click the `Image Recycling` tab, click the `Create Recycling Rules` button, follow the prompts to select the image and configure the rules.",
What resources can be released by image recycling?,Image reclamation frees up disk space to run other operations on your system.,
What is image recycling?,Image recycling is the process of deleting images that are no longer needed and recycling the resources used to create the images.,
What information and functionality is available on the overview page of the warehouse integration?,"The overview page of the warehouse integration displays the basic information and statistical information of the current warehouse, and also provides a quick start at the top to facilitate the management of image space, workspace, and application creation.",
"If you are a platform administrator (Admin role), how do you bind the mirror space to the workspace or set the mirror space to be public?","If you are a platform administrator (Admin role), you can assign a private image space to one or more workspaces (namespaces under the workspace) by binding the image space to the workspace. Or set the mirror space to be public for use by all namespaces on the platform. For specific operation steps, please refer to the illustrations and instructions given in the text.",
"If I have the Workspace Admin role, how to associate the existing image warehouse with the DCE platform?","If you have the Workspace Admin role, you can associate the existing image warehouse to the DCE platform through the warehouse integration (workspace) function. For specific operation steps, please refer to the illustrations and instructions given in the text.",
What are the two integration methods of DCE 5.0 mirror warehouse?,The two integration methods of DCE 5.0 mirror warehouse are: warehouse integration (workspace) and warehouse integration (administrator).,
What types of mirror warehouses can the DCE 5.0 mirror warehouse be used to manage?,DCE 5.0 image warehouse can be used to manage Harbor or Docker image warehouse.,
What is the Workspace Admin role?,The Workspace Admin role refers to users with permissions to manage workspaces.,
Why connect to Harbor instance?,"For Harbor instances, in addition to accessing the administrator account, you can also access the robot account to achieve the same access effect.",
How to quickly deploy applications?,"When members deploy applications in the namespace under the workspace, they can use the Select Image button to pull all public and private images under the workspace with one click to achieve rapid application deployment.",
What two types of warehouse instances are supported?,Supports association of Harbor and Docker warehouse instances.,
What is warehouse integration (workspace)?,Warehouse integration (workspace) is a convenient access mirror warehouse function provided for workspaces. The workspace administrator can flexibly access the image warehouse for the workspace as needed for use by workspace members.,
How to install dependencies in offline environment?,"The steps to install dependencies in an offline environment are:\n1. Upload all files needed in the offline environment to the target host. \n2. Execute the script, select community version or commercial version as needed, and specify the offline parameter.",
How to make offline package?,"The command to make an offline package is:\n- Community version: bash install_prerequisite.sh export community\n- Commercial version: bash install_prerequisite.sh export full\nAfter execution is completed, a file named pre_pkgs.tar.gz will be generated in the current directory. A compressed package that contains all the files required for installation.",
What are the steps to install dependencies online?,"The steps to install dependencies online are:\n1. Download the script install_prerequisite.sh. \n2. Add executable permissions to the script. \n3. Execute the script, select the community version or commercial version as needed, and specify the online parameters.",
What dependencies need to be installed before DCE 5.0 installation?,"Dependencies that need to be installed before installing DCE 5.0 include: podman, helm, skopeo, kind, kubectl, yq and minio client.",
How to specify the external repository type and URL to use in extended offline mode?,You can add the `externalRepoType` and `externalRepoURLs` parameters in the clusterConfig configuration file to specify the external resource library type and URL used. The version information and hardware architecture information can be dynamically obtained through the $releasever and $basearch variables in the URL.,
How to specify which installation configuration file to use on the command line?,"The corresponding configuration file can be used by adding the `--access-type` parameter to the command line and specifying the value as built-in or external. If this parameter is not added, the online mode configuration file will be used by default.",
What are the two installation methods in offline mode?,There are two installation methods in offline mode: built-in offline and extended offline.,
How to generate clusterConfig configuration file template?,The clusterConfig configuration file template can be generated by executing `./dce5-installer generate-config` on the command line and specifying parameters. Different parameter combinations can generate configuration file templates in different scenarios.,
What are the installation modes for DCE?,"DCE has three installation modes: online mode (official online or community version), built-in offline mode, and extended offline mode.",
"In the thin configuration example, builtin is installed in offline mode. What fields does it contain? Please list them and describe them.","The thin configuration example uses builtin mode in offline mode for installation, which contains the following fields: \n- clusterName: Global cluster name in KuBean Cluster. \n- masterNodes: Global cluster: Master node list, including several key fields: nodeName, ip, ansibleUser and ansiblePass. \n- workerNodes: Global cluster: Worker node list, including several key fields: nodeName, ip, ansibleUser and ansiblePass. \n- fullPackagePath: The path of the decompressed offline package. This field is required in offline mode. \n- osRepos: operating system software source information, where type is builtin. \n- isoPath: The path of the ISO file of the operating system. When type is builtin, it cannot be empty. \n- osPackagePath: The path of the system package file. When type is builtin, it cannot be empty. \n- imagesAndCharts: Image warehouse and Chart warehouse source information, where type is builtin. \n- addonPackage.path: The local file system path of the app store addon package. \n- binaries: Binary executable file information, where type is builtin.",
What are the key fields in this YAML file? Please list them and describe them.,"The key fields in this YAML file are: auditConfig, binaries, clusterName, fullPackagePath, addonPackage.path, imagesAndCharts, k8sVersion, loadBalancer.insightVip, loadBalancer.istioGatewayVip, loadBalancer.type, masterNodes, network.clusterCIDR, network.cni, network. serviceCIDR, ntpServer, osRepos.externalRepoType, osRepos.externalRepoURLs, osRepos.isoPath, osRepos.osPackagePath, osRepos.type, privateKeyPath, workerNodes, externalMiddlewares.database.ghippoApiserver, externalMiddlewares.database.ghippoAuditserver, externalMiddlewares.database.ghippoKeycloak, external Middlewares.database. kpanda[0].accessType, externalMiddlewares.database.kpanda[0].driver, externalMiddlewares.database.kpanda[0].dataSourceName, externalMiddlewares.database.kpanda[0].maxOpenConnections, externalMiddlewares.database.kpanda[0].maxIdleConnections and externalMiddleware.elasticsearch. They respectively represent the audit log configuration information (auditConfig), binary executable file information (binaries), cluster name (clusterName), offline package path (fullPackagePath), and application store addon package local file system path (addonPackage.path) of k8s api-server. ), mirror warehouse and Chart warehouse source information (imagesAndCharts), K8s version information (k8sVersion), LoadBalancer related information (loadBalancer.insightVip/loadBalancer.istioGatewayVip/loadBalancer.type), Global cluster Master node list (masterNodes), network information ( network.clusterCIDR/network.cni/network.serviceCIDR), available NTP server information (ntpServer), operating system software source (osRepos.externalRepoType/osRepos.externalRepoURLs/osRepos.isoPath/osRepos.osPackagePath/osRepos.type), SSH private Key file path information (privateKeyPath), Global cluster's Worker node list (workerNodes) and external middleware related information (externalMiddlewares.database.ghippoApiserver/externalMiddlewares.database.ghippoAuditserver/externalMiddlewares.database.ghippoKeycloak/externalMiddlewares.database.kpanda[0 ].accessType/externalMiddlewares.database.kpanda[0].driver/externalMiddlewares.database.kpanda[0].dataSourceName/externalMiddlewares.database.kpanda[0].maxOpenConnections/externalMiddlewares.database.kpanda[0].maxIdleConnections/externalMiddleware. elasticsearch).",
What is the imagesAndCharts field in the ClusterConfig file used for?,The imagesAndCharts field is used to specify information about the image warehouse and Chart warehouse sources for deploying applications and components.,
What does the binaries field in the cluster configuration file do?,The binaries field is used to specify information such as the access mode and access address of the binary executable file.,
"In the ClusterConfig file, which field specifies the node information for the cluster?",The masterNodes and workerNodes fields specify the node information of the cluster.,
What is a Tinder node?,The Tinder node refers to a node where KuBean is installed. It is also the management node of the cluster and is responsible for controlling the deployment and management of other nodes.,
What are the key fields in the cluster configuration file?,"The key fields in the cluster configuration file are: clusterName, fullPackagePath, binaries, imagesAndCharts, auditConfig, etc.",
Under what circumstances is it recommended to install DCE 5.0 in all-in-one mode?,It is recommended that individual customers use the all-in-one mode to install DCE 5.0.,
What is the work cluster used for?,The working cluster supports business applications and services (deploy the working cluster after successfully installing DCE).,
What is the global service cluster used for?,"The global service cluster is used to deploy all components of DCE 5.0, as well as Kubean, which is used to manage the life cycle of the cluster.",
What is a Tinder node?,"The Tinder node is also called the boostrap node, which installs the execution of the deployment program and runs the image warehouse and chart museum required by the platform.",
How many deployment architectures does DCE 5.0 provide? What are they?,"DCE 5.0 provides three deployment architectures: all-in-one, 4-node mode, and 7-node mode.",
How do I verify that the failover feature is working?,"Create a multi-cloud stateless load, choose to deploy it on multiple clusters, and select the aggregation/dynamic weight mode for the scheduling policy. When a cluster is marked as unhealthy and does not recover within the specified time range, the cluster will be tainted and enter the eviction state. At this time, the Pods with stateless load will be migrated based on the resources of the remaining clusters, etc., and will eventually become unhealthy. There will be no Pods in the (tainted) cluster.",
What parameters need to be configured after the failover function is enabled?,"Parameters such as ClusterMonitorPeriod, ClusterMonitorGracePeriod, ClusterStartupGracePeriod, FailoverEvictionTimeout, and ClusterTaintEvictionRetryFrequency need to be configured.",
How to enable failover function?,"Enter the multi-cloud orchestration module, click `System Settings`->`Advanced Configuration`, enable failover and save relevant parameters.",
What are the prerequisites for failover functionality?,"The scheduling policy for multi-cloud workloads can only select aggregation mode or dynamic weight mode, and then the failover function can take effect.",
What is the failover feature?,"When a cluster fails, Pod copies in the cluster will be automatically migrated to other available clusters to ensure service stability.",
How to achieve network interoperability among multiple working clusters in a multi-cloud instance?,"Grid instances can be created in a service mesh and each worker cluster can be managed into it. For specific operations, please refer to [Multi-cloud Network Interconnect](../../mspider/user-guide/multicluster/cluster-interconnect.md).",
Is it possible to delete only the multi-cloud instance but not the components of Karmada?,"You can choose whether to check the instance release function. If checked, the corresponding Karmada instance will be deleted simultaneously; if not deleted, you can continue to use it through the terminal, but the Karmada instance cannot be managed within multi-cloud orchestration. It is recommended to delete simultaneously.",
How to connect to Karmada cluster?,You can connect to Karmada's control plane in the `Console` in the upper right corner of the instance overview page.,
How to customize the `karmada` mirror source warehouse address?,"You can add the corresponding parameter configuration `--chat-repo-url` in the startup command, for example: \n```\nkarmada-operator --chat-repo-url=registry.com/karmada\n```",
"After creating a multi-cloud application through multi-cloud orchestration, how can we obtain relevant resource information through container management?","Multi-cloud orchestration uses a tricky move when creating multi-cloud orchestration instances. It will add the instance itself to the container management as a hidden cluster (not displayed in the container management), so that you can fully rely on the capabilities of the container management (such as collecting and accelerating the retrieval of resources, CRD, etc. of each K8s cluster), when in the interface When querying the resources of a multi-cloud orchestration instance (such as Deployment, PropagationPolicy, OverridePolicy, etc.), you can directly retrieve them through container management to separate reading and writing and speed up response time.",
How to query events of multiple clusters?,Multi-cloud orchestration completes product-level integration and displays all Karmada instance-level events.,
Permission issues with multiple clusters?,Karmada is closely integrated with the existing permission system of 5.0 and is connected with workspace to complete the binding of Karmada instances and workspaces to solve permission problems.,
How to achieve failover?,"Karmada natively supports failover functionality. When a member cluster fails, Karmada will perform intelligent rescheduling to complete failover.",
Is there production level support?,"Currently, Karmada is still in the TP stage, and the high availability of many internal components needs to be resolved (such as dependent etcd, etc.).",
Can Service realize cross-cluster service discovery?,"Karmada itself does not support it, but it can be supported with the help of the external solution multi-dns. Future versions may provide programmatic support.",
Can workloads communicate across clusters?,"Karmada itself does not support it, but it can be supported by the open source community Submariner. Future versions may provide programmatic support.",
Can monitoring information be presented in one view for workloads distributed to multiple clusters?,"Supports viewing multi-cloud applications in a unified view, and can monitor which clusters they are deployed to, corresponding services, propagation strategies, etc.",
Does it support cross-cluster application log collection?,"It is currently not supported, but this feature will be added in the future.",
How to seamlessly migrate single-cluster applications to multi-cloud orchestration?,You can use Karmada's new feature [One-click upgrade to multi-cloud workloads] (../workload/promote.md).,
What is the version of Karmada? Can I specify the version? Is it possible to upgrade?,"The default version of Karmada is v1.4.0, and users can upgrade it themselves.",
How does DCE create and delete multi-cloud resources?,"Creating and deleting multi-cloud resources can be done through interface or YAML writing. Idle resources can be deleted on the interface or terminal, but resources in use are not allowed to be deleted.",
What multi-cloud resources can be managed in DCE?,"DCE can manage multiple resources such as multi-cloud namespaces, multi-cloud storage claims, multi-cloud configuration items, multi-cloud keys and services, and routing.",
What methods does DCE support for deleting multi-cloud workloads?,"DCE supports deletion of multi-cloud workloads through page deletion and terminal deletion, and reminds you that a second confirmation is required during deletion.",
What methods does DCE support for configuration changes of multi-cloud workloads?,DCE supports page editing and YAML editing to change the configuration of multi-cloud workloads.,
What strategies does the Strategy Center offer?,"The Strategy Center provides two types of strategies: deployment strategy and differentiation strategy. They support viewing the list of current instances and their associated multi-cloud resources on the interface, maintaining creation and editing information in YAML, and deleting idle deployment policies.",
What functions does the resource management module provide?,"The resource management module supports the creation and deletion of multi-cloud namespaces, the creation and deletion of multi-cloud storage claims, and the management of multi-cloud configuration items and secret keys. In addition, viewing of service lists and creation and deletion of services and routes are also supported.",
What operations are available for multi-cloud workloads?,"Multi-cloud workloads support operations such as viewing the list of stateless workloads, creating stateless workloads (including image creation and YAML creation), editing stateless workload configuration information, and deleting stateless workloads.",
What information does the instance overview page contain?,"The instance overview page displays basic information, working cluster information, workload status, policy information, resource information, and recent developments.",
What are the operations for multi-cloud instance management?,"Multi-cloud instance management supports multi-cloud instance list viewing and retrieval, adding multi-cloud instances, and one-click import of clusters. At the same time, it also supports removing multi-cloud instances, but you need to ensure that the current instance does not contain any clusters before deleting it.",
What are the capabilities of multi-cloud orchestration?,"The functions of multi-cloud orchestration include: unified management plane, multiple instances, one-click cluster import, native API support, multi-cloud application distribution, application failover, observability, etc.",
How to update and delete a multi-cloud route?,"Click `⋮` on the right side of the list to update and delete the route. Note: If a route is deleted, the information related to the service will also disappear, so please operate with caution.",
What information needs to be filled in to create a multi-cloud route?,"You need to configure the deployment location, set routing rules, Ingress Class, whether to enable session persistence and other information.",
How to enter the multi-cloud routing creation page?,"After entering a certain multi-cloud instance, in the left navigation bar, click `Resource Management` -> `Multi-cloud Routing`, and click the `Create` button in the upper right corner.",
In what two ways can multi-cloud routing be created?,"Currently, two creation methods are provided: form creation and YAML creation.",
What is multi-cloud routing?,"Multi-cloud routing is a unified abstraction of the standard Kubernetes Ingress multi-cloud. By creating an Ingress and associating it with several multi-cloud services, it can be distributed to multiple clusters at the same time.",
What related information will be deleted when deleting a multi-cloud configuration item?,"If a configuration item is deleted, its related information will be deleted together. Please proceed with caution.",
How to view and edit multi-cloud configuration items that have been created?,"In the multi-cloud configuration item list, you can edit YAML, update, export, and delete by clicking `⋮` on the right side of the list.",
How to create multi-cloud configuration items in multi-cloud instances?,"After entering a certain multi-cloud instance, in the left navigation bar, click `Resource Management` -> `Multi-cloud Configuration Items`, click the `Create Configuration Items` button in the upper right corner, enter the name in the `Create Configuration Items` page, and select After entering the namespace and other information, click OK.",
What are the two creation methods for multi-cloud configuration items?,Multi-cloud configuration items include wizard creation and YAML creation.,
What are multi-cloud configuration items?,"A multi-cloud configuration item is an API object used to save non-confidential data into key-value pairs. Pods can use it as environment variables, command line parameters, or configuration files in storage volumes to decouple multi-cloud environment configuration information from container images, making it easy to modify the configuration of multi-cloud applications.",
How to edit a CR instance that has been created?,"You can enter the CR instance list page, find the CR instance you want to edit, and click to enter the details page for editing.",
How to view created CR instances?,You can enter the CR instance list page to view the created CR instances.,
How to create CR instance via YAML?,"You can follow the following steps to create:\n1. Enter the custom resource details and click the `YAML Create` button on the right side of the CR list. \n2. In the `YAML creation` page, first fill in the YAML information of CR. \n3. Fill in the deployment strategy again. Note that the YAML information of the deployment strategy needs to be filled in according to the information of the resources that need to be propagated. If there is no need for differentiation, the differentiation strategy does not need to be filled in.",
How to edit a multi-cloud custom resource (CRD) that has been created?,"You can follow the following steps to edit:\n1. Enter the custom resource list page, find the name of the CRD to be edited and click to enter the details page. \n2. You can edit YAML information in the details page to update CRD.",
How to create a multi-cloud custom resource (CRD)?,"You can create it by following the following steps:\n1. In the left navigation bar, click `Multi-cloud Custom Resources` to enter the custom resource page, and click the `YAML Create` button in the upper right corner. \n2. In the `YAML creation` page, fill in the YAML statement and click `OK`. Download and import functions are also supported. \n3. Return to the custom resource list page to view the custom resource just created.",
What is differentiated configuration? how to use?,"Differentiated configuration refers to setting different container configurations, labels and annotations for different clusters with the same load. When using it, enter the `differential configuration` page after creation, select personalized container configuration, labels and annotations, and click `OK`. Add the corresponding differentiated configuration items in the left list area. After adding a differentiated configuration item, you need to specify the corresponding cluster. The selectable range of clusters is only the cluster you just selected. The selected cluster will use the specified differentiated configuration; the clusters that are not specified will still use the default configuration.",
What information can be configured on the container configuration page?,"The container configuration page allows you to configure basic information about the container where the load is located. You can choose to configure life cycle, health check and other information.",
What are the options for concurrency strategies? What does difference mean?,"The concurrency strategy includes the following three options:\n- Allow: A new scheduled task can be created before the previous task is completed, and multiple tasks can be run in parallel. Too many tasks may occupy cluster resources. \n- Forbid: New tasks cannot be created before the previous task is completed. If the execution time of the new task arrives and the previous task has not been completed, CronJob will ignore the execution of the new task. \n- Replace: If the execution time of the new task arrives but the previous task has not been completed, the new task will replace the previous task.",
What setting methods are supported for timing rules?,"Timing rules support setting the time period for task execution based on minutes, hours, days, weeks, and months. Supports custom Cron expressions with numbers and *.",
What are the steps to create a scheduled task for mirroring?,"1. In the left navigation bar, click `Multi-cloud Workload`, enter the scheduled task page, and click the `Image Creation` button in the upper right corner. \n2. In the `Create Task` page, after configuring the basic information of the load, click `Next`. \n3. In the `Container Configuration` page, configure the basic information of the container where the load is located. You can choose to configure life cycle, health check and other information, and then click `Next`. \n4. On the `Scheduled Task Configuration` page, configure concurrency policy, timing rules, task records, other configuration and other information. \n5. In the `Advanced Configuration` page, you can configure task settings and labels and annotations. \n6. If differentiated configuration is required, select personalized container configuration, labels and annotations on the `Differential Configuration` page, and click `OK`",
What should I pay attention to when deleting an instance?,"Before deleting an instance, you need to delete the clusters, workloads, and services related to the instance. In order to ensure that no resource scheduling remains in the working cluster when deleting, we will restrict you to disassociate all working clusters; after you disassociate, the deletion at this time will be regarded as a safe Delete action. Of course, you need to operate with caution in any case to avoid unnecessary losses caused by misoperation.",
How to delete an instance?,"You can click the `...` button on the right in the multi-cloud instance list and select `Delete` in the pop-up menu. Enter the name of the instance in the pop-up window, confirm it is correct, and click the `Delete` button. It should be noted that before deleting an instance, you need to delete the clusters, workloads, and services related to the instance.",
What parts do you need to focus on when creating multi-cloud workloads?,"When configuring the number of replicas, you need to pay attention to the corresponding scheduling policy. Only when it is repeated, all configured replicas will be started in multiple clusters\n- Automatic propagation: By default, resources such as ConfigMap and Secret that are relied upon in the multi-cloud workload configuration are automatically detected. When the button is turned on, it means these resources will be automatically propagated along with the multi-cloud workload.",
How to choose a cluster to deploy?,"You can choose in the following three ways:\n-Specify cluster: Specify the working cluster you want to deploy from the current multi-cloud instance\n-Specify region: You can choose from three types: vendor, region, and availability zone, and multiple selections are supported. If the current selection cannot meet the requirements, you can expand the advanced deployment strategy, select whether to exclude clusters, configure cluster taint tolerance, and dynamic region selection. Since the above conditions are not intuitive, we will also display the expected scheduled cluster for your review. \n-Specify label: Supports adding one or more label information, which is related to the label of the working cluster. You can select the target cluster by filling in the label and selecting different operators `exists` or `equal`. If the current selection cannot meet the requirements, you can expand the advanced deployment strategy, select whether to exclude clusters, configure cluster taint tolerance, and dynamic region selection. Since the above conditions are not intuitive, we will also display the expected scheduled cluster for your review.",
What are the steps to create a multi-cloud stateless workload?,"Click the `Image Creation` button in the upper right corner of the multi-cloud stateless load page\n- Configure basic information and select deployment cluster\n- Configure container information\n- Configure advanced options, such as upgrade strategy, scheduling strategy, labels and annotations And DNS\n- Optional differentiated configuration items\n- Click Confirm to complete the creation of multi-cloud workloads",
What is the architecture of Kairship controller manager? What working modes are supported?,"Kairship controller manager has a high-availability architecture, an internal election mechanism, and can work in a single Pod. It supports two working modes: synchronization of global resources (such as CRD) and looping of control plane logic.",
"In multi-cloud orchestration, what are the components of the control plane? Where are they deployed?","In multi-cloud orchestration, the control plane has three components: kairship apiserver, kairship controller manager, and karmada operator. They are all deployed in the global service cluster.",
Where do all the Kubernetes clusters managed in Karmada instances come from? What will happen automatically after joining a cluster?,"All Kubernetes clusters managed in Karmada instances come from Kpanda clusters. After joining a cluster, CR synchronization will be automatically performed (Kpanda Cluster --> Karmada Cluster). At the same time, the multi-cloud orchestration management plane has control loop logic that will monitor Kpanda Cluster changes in real time, synchronize to the control plane as soon as possible, and further feed back to The Karmada Cluster corresponding to the Karmada instance currently mainly monitors changes in Kpanda cluster access credentials.",
What actions are used to obtain relevant resource information when multi-cloud orchestration creates an instance?,"When creating an instance in multi-cloud orchestration, the instance itself is added to the container management module as a hidden cluster (not displayed in the container management), thereby using the capabilities of the container management module to collect and accelerate the retrieval of resources, CRDs, etc. of each Kubernetes cluster.",
What is the Karmada control-plane?,The Karmada control-plane is a complete Kubernetes control plane without any nodes hosting workloads.,
What does a multi-cloud orchestration deployment topology look like?,"Multi-cloud orchestration consists of three components: kairship apiserver, kairship controller manager, and karmada operator, all deployed in the global service cluster. Among them, the kairship apiserver stateless service supports horizontal expansion; the kairship controller manager has a high-availability architecture, has an internal election mechanism, and can work in a single Pod.",
How does multi-cloud orchestration solve large-scale request performance issues?,"All multi-cloud orchestration access requests are passed directly to the multi-cloud orchestration instance located in the global service cluster. All read requests such as get/list will access the container management module, and write requests will access the Karmada instance. At the same time, when creating an instance, multi-cloud orchestration adds the instance itself as a hidden cluster to the container management module for querying related resource information. This achieves separation of reading and writing and speeds up response time.",
What control logic is the kairship controller-manager mainly responsible for?,"The kairship controller-manager is mainly responsible for instance status synchronization, resource collection, Karmada instance registration, global resource registration, etc. Specifically, they include virtual-cluster-sync-controller, resource statistics controller, status sync controller, instance registry controller and Ghippo webhook controller.",
What is the function of kairship apiserver?,"kairship apiserver is the multi-cloud orchestration data flow entrance and the entrance to all APIs (protobuf takes priority, all API interfaces are defined through proto, and corresponding front-end and back-end codes are generated, and grpw-gateway is used to support both http restful and grpc). Upon startup, the operator identity information is obtained from the global management module for subsequent AuthZ security verification.",
What are the two core components of multi-cloud orchestration?,Multi-cloud orchestration includes two core components: kairship apiserver and kairship controller-manager.,
What functions is the management plane of multi-cloud orchestration mainly responsible for?,"The management plane of multi-cloud orchestration is mainly responsible for the life cycle management of multi-cloud instances (based on Karmada), serving as a unified traffic entrance for multi-cloud products, proxying API requests for multi-cloud instances, aggregation of cluster information (monitoring, management, control) within multi-cloud instances, etc. Management and monitoring of resources such as multi-cloud workloads and possible subsequent permission operations.",
What is the impact of deleting a multi-cloud instance?,"When an instance is deleted, it is removed from the instance list for multi-cloud orchestration.",
Will the Karmada instance be released synchronously when the instance is released?,"In recent updates, users can choose whether to release Karmada instances synchronously as needed. The synchronous deletion function is turned on by default. After it is turned off, the user needs to recycle the instance by himself.",
What should I pay attention to when adding multi-cloud instances?,"When choosing to install a management plane cluster for multi-cloud instances, it is recommended to use a cluster that is running normally and has a storage volume claim (PVC) installed. Otherwise, there is a risk of installation failure. In addition, because multi-cloud instances exist in the form of virtual clusters, a prefix needs to be added before the cluster name to avoid name conflicts.",
How to add multi-cloud instances?,"The steps are as follows:\n1. In the multi-cloud instance list, click `Add Multi-cloud Instance` in the upper right corner. \n2. In the `Add Multi-Cloud Instance` window, you need to fill in the name, alias, drop down to select the management plane cluster, and fill in the label annotation.",
What is the multi-cloud module?,The multi-cloud module is the multi-instance management capability provided by DaoCloud based on Karmada. Users only need to follow simple steps to add instances to perform multi-cloud orchestration.,
"In the DCE multi-cloud module, how to implement centralized management?","In the DCE multi-cloud module, centralized management of public cloud, private cloud, edge and other clusters can be achieved without worrying about cluster location.",
Does the DCE multi-cloud module support commercial products from unbound vendors?,"Yes, the DCE multi-cloud module supports commercial products without binding vendors, and supports automatic allocation and free migration.",
"In the DCE multi-cloud module, how to perform multi-cluster scheduling?","The DCE multi-cloud module supports cluster affinity scheduling, multi-granule multi-cluster high-availability deployment, and supports multi-region, multi-availability zone, multi-cluster, multi-provider, etc.",
How compatible is the DCE multi-cloud module with Kubernetes?,"The DCE multi-cloud module has a fully compatible API with Kubernetes, allowing developers to use multi-cloud like a Kubernetes cluster.",
What are the advantages of DCE multi-cloud module?,"The DCE multi-cloud module has the advantages of compatibility with Kubernetes native API, commercial products without binding vendors, rich multi-cluster scheduling, open neutrality, and centralized management.",
What kind of Override Policy API does multi-cloud orchestration software provide?,"Multi-cloud orchestration software provides an independent Override Policy API for specifically automating cluster-related configurations, which can override image prefixes based on member cluster regions and override StorageClass based on the cloud provider adopted.",
How does multi-cloud orchestration software define multi-cluster scheduling and propagation requirements?,"Multi-cloud orchestration software provides an independent propagation policy API to define multi-cluster scheduling and propagation requirements, mapping and workload support 1:n. Users do not need to specify scheduling constraints every time they create a federated application.",
What is a resource template?,Resource templates are templates for multi-cloud orchestration software to use Kubernetes Native API definitions for federation and can be used to integrate existing tools that have been adopted by Kubernetes.,
How to enter native Harbor?,Click `...` on the right side of an instance to enter native Harbor.,
What should I do if a newly created instance in the managed Harbor instance list is in a waiting state?,Wait for the status to change from `Updating` to `Healthy` and you can use it normally.,
What information is required when creating a managed Harbor instance?,"Instance name, deployment location, database, Redis instance and mirror/Charts storage information, domain name, select ingress instance and enter administrator password.",
What is the first step to create a managed Harbor?,"Log in to the Web console as a user with the Admin role, and click `Mirror Warehouse` -> `Hosted Harbor` from the left navigation bar.",
What are the prerequisites for creating a managed Harbor?,Harbor Operator needs to be installed first.,
How to push an image to the current image space?,You can click the `Push Command` button on the right side of the image list page to generate a push command and then push it. Push commands should include `docker tag` and `docker push` commands.,
What does the mirror list show?,"The image list takes the workspace as the dimension and displays all public and private images available under the tenant. Among them, the public images come from all public images in the image warehouse integrated or hosted by the platform, and the private images are individually assigned to the workspace by binding the image space to the workspace.",
What are the main functions of the DCE mirror warehouse?,"The main functions of the DCE mirror warehouse include rapid application deployment, fine-grained allocation, image scanning, image push, and display of detailed information such as image version, level information, creator and creation time.",
Image recycling rules are calculated independently and apply to which eligible images?,Image recycling rules are calculated independently and apply to all eligible images.,
How to view the image list and recycling rules in a certain image space?,Click the name of a mirror space to view the mirror list and mirror recycling rules in the current space.,
How to view all mirror spaces under an instance?,Click on an instance to view all mirror spaces under it.,
What are the characteristics of a private image warehouse?,Private image warehouses can only be accessed by authorized users and usually store images of the image space itself.,
What are the characteristics of a public mirror warehouse?,The public image warehouse is accessible to all users and usually stores public images. DCE has a public image space by default.,
What are the two types of mirror spaces?,Mirror spaces are divided into two types: public and private.,
How to ensure security of private images?,"A private image must bind the image space to which it belongs and one or more workspaces. Only the namespace under the bound workspace can use the private image, ensuring the security of the private image.",
How to manage the integrated image warehouse?,You can perform operations such as `Delete Integration` and `Edit` on each tile after returning to the `Warehouse Integration (Administrator)` list. Health status is displayed in the tile.,
How to do warehouse integration?,"You can log in to the Web console as a user with the Admin role, click `Mirror Warehouse` -> `Warehouse Integration (Workspace)` in the left navigation bar, then click the `Warehouse Integration` button in the upper right corner and select the corresponding warehouse type. Fill in the integration name, address, username and password and confirm.",
What mainstream image warehouse types does warehouse integration (administrator) support?,Warehouse integration (administrator) supports mainstream image warehouse types: Harbor Registry and Docker Registry.,
What is Warehouse Integration (Administrator)?,Warehouse integration (administrator) is the entrance to the centralized management platform mirror warehouse. It supports the integration of external mirror warehouses and managed Harbors created by the automatic integration platform. The mirror space can be bound to the workspace or set to be public for use by all namespaces of the platform.,
How to use the platform to build a self-built Redis instance or connect to an external Redis instance?,You can use the platform to build a self-built Redis instance or connect to an external Redis instance for configuration.,
How to use the platform to build a self-built database or access an external database?,You can use the platform's self-built database or access an external database for configuration.,
How does Harbor achieve high availability?,Supports multi-copy deployment to achieve high availability.,
What are the functional advantages of Harbor?,Multiple image warehouse instances break the calling barriers between modules and provide a unified management control plane.,
What is the main purpose of Harbor?,"Harbor is used for secure hosting and efficient distribution of artifacts that comply with OCI standards, such as container images and Helm Charts.",
What prerequisites need to be met to share a private image for namespace use in a specified workspace?,"To realize private image sharing for namespace use in a specified workspace, the following prerequisites must be met:\n- The workspace has been created\n- The namespace has been bound to the workspace",
What is Hosted Harbor? Is it recommended?,Hosted Harbor refers to a self-built Harbor on DCE. Recommended.,
Which method does DCE 5.0 mainly recommend as a mirror warehouse to provide mirror services?,DCE 5.0 mainly promotes Harbor as a mirror warehouse to provide mirror services.,
How to share a private image to a namespace under a specified workspace?,Follow these steps:\n- Create a managed Harbor or connect to an external Harbor. \n- Create a mirror space and make it public. \n- Push the image. \n-The mirror space is bound to the workspace. \n- Deploy the application.,
How can I share a public image for all namespaces to use?,Follow these steps:\n- Create a managed Harbor or connect to an external Harbor. \n- Create a mirror space and make it public. \n- Push the image. \n- Deploy the application.,
What is the difference between allocating a mirror space to a workspace and integrating a warehouse under the workspace?,"Administrators can manage them uniformly and allocate one mirror space to multiple workspaces in batches without having to associate them individually. The workspace administrator can integrate external image warehouses for members to use as needed, making it more flexible to use.",
How to troubleshoot that the image in the image space cannot be selected when deploying an application in the Kubernetes namespace?,"First check whether the Kubernetes namespace is bound to the workspace and whether the image space is bound to the workspace where the Kubernetes namespace is located. If the problem still cannot be solved, you need to further check whether the mirror space is private or public.",
How to bind workspace and private image space?,"The administrator needs to enter the warehouse integration interface, click ""Mirror Space"" under a certain warehouse name, click ""⋮"" on the far right side of the private mirror space to be bound, select ""Bind/Unbind Workspace"", and then Select one or more workspaces and click OK.",
How to use private image?,The administrator needs to assign the private mirror space to the workspace (tenant) before it can be used by members of the workspace. The specific binding steps are described in the text.,
How to choose a public image for application deployment?,"When deploying an application in the container management module, you can select the image in all public image spaces in the warehouse integration by selecting the image button for application deployment.",
What is mirror space?,"Image space is a logical classification unit used to store images in the DCE image warehouse, and is divided into two types: public and private.",
What types of images are stored in the public image repository?,Usually stores public images.,
What workspaces can the private mirror space be bound to before it can be used?,Only after the administrator Admin binds the private image space to one or more workspaces (tenants) can it be pulled and used by the Kubernetes namespace under the workspaces (tenants).,
How to create a mirror space?,"Log in to the Web console as a user with the Admin role, click `Mirror Warehouse` -> `Warehouse Integration (Administrator)` from the left navigation bar, click a warehouse name, click `Mirror Space` in the left navigation bar, and Click `Create Mirror Space` in the upper right corner, fill in the name of the mirror space, check the type and click `OK`.",
What are the prerequisites for mirror space?,An external Harbor repository has been created or integrated.,
What two types of mirror spaces does Harbor provide?,Public mirror space and private mirror space.,
"When deploying after selecting a private Project image, it prompts that the image pull failed. What should I do?","When deploying after selecting the private Project image, it is prompted that the image pull fails. This may be because the `secret` named `registry-secret` is not generated in the target cluster namespace or the `dockerconfigjson` in the `secret` is incorrect. The solution is: confirm whether the `secret` named `registry-secret` has been generated, and then run the `kubectl get secret registry-secret -o jsonpath=""{.data.*}"" | base64 -d | jq` command Check whether `dockerconfigjson` is correct.",
Under what circumstances can I view instances on the image list page? How to deal with it?,"If you cannot view the instance on the image list page, it may be because the resources integrated with the warehouse are not healthy. At this time, you can solve the problem by confirming whether the resources integrated in the warehouse are healthy. Please refer to the above answer for specific methods.",
Under what circumstances does a warehouse become unhealthy after integration? How to troubleshoot?,"When the status of the warehouse is unhealthy after integration, you need to first confirm whether the instance is really healthy. If the instance is unhealthy, you need to troubleshoot the instance. If the instance is healthy, you need to check `registrysecrets.kangaroo' on the `kpanda-global-cluster` cluster. Whether the io` resource is created and check the `status` situation. The confirmation method is: run the `kubectl -n kangaroo-system get registrysecrets.kangaroo.io` command in the namespace `kangaroo-system` to check whether the resource is created, and then run `kubectl -n kangaroo-system get registrysecrets.kangaroo.io` Use the trust-test-xjw -o yaml` command to check the status of the resource.",
What should I do if I select a private `Project` image in the Kpanda image selector but when deploying it is prompted that the image pull failed?,"You can first confirm whether the `secret` named `registry-secret` has been generated. If it has been generated, you need to confirm whether the `dockerconfigjson` in the `secret` is correct.",
What should I do if I can’t see private projects on the mirror list page?,"You need to confirm whether the resources integrated in the warehouse are healthy. If they are not healthy, they will not be displayed on the mirror list page.",
What should I do if the warehouse is not in a healthy state after integration?,"First, you need to confirm whether the instance is really healthy. If the instance is not healthy, you need to troubleshoot the instance. If the instance is healthy, you can check whether the `registrysecrets.kangaroo.io` resource is created on the `kpanda-global-cluster` cluster, and Check the `status` situation.",
What should I do if I find that the image space and available storage on the page have not increased after creating a `Project` or uploading an image?,"This is because the statistical information on the `Hosted Harbor` homepage and warehouse integration details on the UI page is asynchronously obtained data, and there will be a certain delay, with a maximum delay of 10 minutes.",
What should I do if the status of the created managed Harbor is still unhealthy?,"The status on the created hosted Harbor page and the status of the warehouse integration are two-in-one. It is healthy when both statuses are healthy. Therefore, it may happen that the managed `Harbor` is already accessible, but the status is still unhealthy. In this case, you need to wait for a service detection cycle. A detection cycle is 10 minutes, and it will be restored to the original state after one cycle.",
How long does it take to query the private image after it is bound to the workspace?,"After the private image is bound to the workspace, the program needs to execute a lot of logic asynchronously, so it will not be visible immediately. This process will be affected by the system. If the system responds quickly, the asynchronous execution will be faster and can be seen within 1 minute. It should take no more than 5 minutes at most.",
Can private images in the mirror warehouse be seen in non-mirror warehouse modules?,"The mirror warehouse is strictly implemented in accordance with the permissions of DEC 5.0. In the mirror warehouse, a user must belong to a certain tenant to see the private mirror space under the current tenant. Otherwise, even the administrator cannot see it.",
Does Harbor support redis cluster mode?,Currently `Harbor` still cannot use `redis` cluster mode.,
What points should be checked if the Harbor Operator installation fails?,"You need to check whether `cert-manager` is installed successfully, whether `installCRDs` is set to `true`, and whether the `helm` task of installing `Harbor operator` is successful.",
"What are the links to download, install and apply for community free trial of DCE 5.0?",- [Download DCE 5.0](../../download/dce5.md)\n- [Install DCE 5.0](../../install/intro.md)\n- [Apply for community free trial]( ../../dce/license0.md),
What are the components in a logical architecture diagram?,"Management component (Master), work component (Node), data storage (ETCD), public components (Ingress, Registry, Log, Metrics).",
What are the functions of the policy management module?,"Supports the formulation of network policies, quota policies, resource limit policies, disaster recovery policies, and security policies at namespace or cluster granularity.",
What functions does the application management module have?,"One-stop deployment, elastic scaling of application loads, full life cycle of applications, and unified management capabilities across cluster loads.",
What functions does the cluster management module have?,"Unified cluster management, rapid cluster deployment, one-click cluster upgrade, cluster high availability, cluster full life cycle management, and open API capabilities.",
What are the main functions of the container management module?,"Cluster management, application management, policy management.",
What is container management?,Container management is a container management platform for cloud-native applications built based on Kubernetes open source technology. It realizes unified management of multi-cloud and multi-cluster and reduces operation and maintenance management and labor costs.,
How to obtain the relevant running information of the workload when the workload is in an abnormal or not ready state?,"When the workload is in an abnormal or not ready state, you can move the mouse over the status value of the load, and the system will display more detailed error information through a prompt box. You can also obtain workload-related running information by viewing logs or events.",
What are the different life cycle states of different types of workloads during operation? Please give an example.,"Different types of workloads will present different life cycle states during operation. For example, stateless load, stateful load and daemon process will show the status of ""waiting"", ""running"", ""deleting"", ""exception"" and ""not ready""; tasks will show ""waiting"", ""executing"" ”, “Execution Completed”, “Deleting” and “Exception” and other statuses; scheduled tasks will show statuses such as “Waiting”, “Started”, “Stopped” and “Deleting”.",
Based on what factors does the fifth-generation container management module design a built-in set of workload life cycle states?,"The fifth-generation container management module designs a built-in workload life cycle status set based on factors such as the status of the Pod and the number of copies, so that users can more realistically perceive the running status of the workload.",
What predefined lifecycle does a Pod follow?,"Pod follows a predefined life cycle, starting from the `Pending` stage. If at least one of the main containers starts normally, it enters `Running`, and then enters `Succeeded` depending on whether any container in the Pod ends in a failed state. or `Failed` stage.",
What built-in workload resources are provided in Kubernetes to manage Pods?,"Kubernetes provides five built-in workload resources to manage Pods, namely stateless workloads, stateful workloads, daemons, tasks, and scheduled tasks.",
How to execute manifest configuration file?,Log in to any Master node in the Global cluster and execute the command: `kubectl apply -f kubeanofflineversion.cr.patch.yaml`.,
Which file needs to be copied to update the Kubernetes version manifest of the Global cluster?,You need to copy the `kubeanofflineversion.cr.patch.yaml` manifest configuration file in the `/data` file of the networked node to the `/root` directory of any Master node in the Global cluster.,
How do I create an offline package manifest configuration file and what is the file name?,Create a file named `manifest.yaml` in the `/root` directory of the networked node.,
How to check the running status of the Docker service of a networked node?,Execute command: `ps aux|grep docker`.,
What offline scenarios does this document apply to?,"This document is only for upgrading the Kubernetes version of the working cluster created using the DCE 5.0 platform in offline mode, and does not include the upgrade of other Kubernetes components.",
What are the ideas for upgrading this document?,The overall upgrade idea is: Build the offline package on the networked node → Import the offline package to the Tinder node → Update the kubernetes version list of the Global cluster → Use the platform UI to upgrade the kubernetes version of the working cluster.,
How to view detailed logs of cluster upgrade?,"During the cluster upgrade process, you can click the `Real-time Log` button below the progress bar to view detailed logs.",
How long is the cluster upgrade expected to take?,The cluster upgrade is expected to take approximately 30 minutes to complete.,
What should you pay attention to when upgrading a cluster?,"After version upgrade, you cannot roll back to the previous version, so you need to operate with caution. In addition, cluster upgrades across minor versions are not allowed, for example, you cannot directly upgrade from 1.23 to 1.25. It should be noted that the global service cluster can only be upgraded through the terminal.",
How to confirm which version can be upgraded?,"When performing a cluster upgrade operation, you can view the list of versions that the target cluster can be upgraded to on the page.",
How to perform cluster upgrade operation?,"One-click upgrade operation can be performed through DCE's Web UI interface. First, select the target cluster in the cluster list, then click `Cluster Operation and Maintenance`->`Cluster Upgrade` in the left navigation bar, click `Version Upgrade` in the upper right corner of the page, select the upgradeable version and enter the cluster name to confirm. Can.",
How to use configuration items to set command line parameters of a container?,"You can use the environment variable substitution syntax $(VAR_NAME) to reference the Key/Value defined in the ConfigMap, and then make relevant settings in the Pod's spec.containers.command.",
How to use configuration items to set environment variables of a container?,"Configuration items can be used as environment variables of the container through the graphical interface or the terminal command line. For graphical operation, you need to select the environment variable interface on the image creation workload page, and then select configuration item import or configuration item key value import; for command line operation, you need to set the configuration item as an environment variable when creating the workload and use the valueFrom parameter to reference it. Key/Value in ConfigMap.",
What kind of flexibility do configuration items provide?,"Using configuration items separates configuration data from application code, providing a more flexible way to modify application configuration.",
What usage scenarios do configuration items provide?,"Configuration items can be used to set the container's environment variables, command line parameters, or serve as the container's data volume.",
What are configuration items?,"Configuration items are an API object of Kubernetes, used to save non-confidential data into key-value pairs, and can store configurations needed by other objects.",
How to use the key as the authentication credential for the image repository when kubelet pulls the container image?,"You can use the key as the image warehouse authentication credential through the graphical interface or the terminal command line. In the graphical interface, when selecting an image, select the private image repository name, and enter the image name and correct key information in the private repository. When creating a key, make sure to enter the correct image warehouse address, user name, password and select the correct image name.",
How to use the key as the data volume of the Pod?,"The key can be used as the Pod's data volume through the graphical interface or the terminal command line. In the graphical interface, when creating a workload in the image, select the storage type as `Key` in the `Data Storage` interface and enter the relevant information in sequence. In the command line, you need to set the secret to the data volume when creating the Pod, and use the `.spec.volumes` and `volumeMounts` parameters to mount the Secret.",
How do I set a container's environment variables using a key?,"You can use the key as an environment variable for the container through a graphical interface or a terminal command line. In the graphical interface, you can set environment variables for the container by selecting `Key Import` or `Key Key Value Import` in the `Environment Variables` interface when creating a workload in the image. In the command line, you can set the key as an environment variable and use the `valueFrom` parameter to reference the Key/Value in the Secret.",
What are the scenarios for using keys?,"Scenarios for using keys mainly include: using them as environment variables for containers, using keys as data volumes for Pods, and using them as identity authentication credentials for image warehouses when kubelet pulls container images.",
What is a key?,"A key is a resource object used to store and manage sensitive information such as passwords, OAuth tokens, SSH, TLS credentials, etc.",
What is the role of cross-cloud disaster recovery and backup of the DCE container management module?,"The cross-cloud disaster recovery and backup of the DCE container management module can ensure high business availability. At the same time, the business can be deployed on multiple cloud container platforms in different regions to help applications achieve multi-regional traffic distribution and realize cross-cloud application management on the same platform. Reduce operation and maintenance costs. When a cloud container platform fails, business traffic is automatically switched to other cloud container platforms through a unified traffic distribution mechanism.",
Which industry users is suitable for the multi-cloud combined deployment of DCE container management module?,"The multi-cloud combined deployment of the DCE container management module is suitable for users in industries with high security requirements, such as finance. Users can deploy some business applications in a private cloud environment based on the security and sensitivity requirements of business data, and deploy non-sensitive applications in cloud clusters for unified management.",
How does DCE’s container management module help enterprises cope with traffic peaks?,"DCE's container management module can realize elastic expansion and contraction of clusters and elastic expansion and contraction of applications, thereby helping enterprises cope with traffic peaks and scale down during business low periods to save costs.",
What is the role of the DCE container management module in unified management of cross-cloud clusters?,"The unified management of cross-cloud clusters by the DCE container management module can reduce additional management costs caused by different infrastructure and different cloud providers, unify the management process, and reduce costs.",
What infrastructure environments and container cloud platforms can DCE's container management module uniformly manage?,"DCE's container management module can uniformly manage Kubernetes platforms built on different infrastructure environments (public cloud, private cloud and hybrid cloud) or different container cloud vendors.",
How to refresh the scheduling status of a node?,"On the node management page, click the refresh icon on the right side of the search box to refresh the scheduling status of the node.",
What happens after a node is suspended from scheduling?,"After the node suspends scheduling, it will no longer accept new Pod scheduling.",
How to restore the schedule of a node?,"Click the name of the target cluster on the cluster list page to enter the node management page, click the operation icon on the right side of the target node, and select the Resume Scheduling button to restore the scheduling of the node.",
How to pause the scheduling of a node?,"Click the name of the target cluster on the cluster list page to enter the node management page, click the operation icon on the right side of the target node, and select the pause scheduling button to pause the scheduling of the node.",
How to set node labels in Kubernetes?,You can use the kubectl command line tool to set labels for nodes through the `kubectl label nodes <node-name> <label-key>=<label-value>` command.,
What is workload anti-affinity mainly used for?,"Workload anti-affinity is mainly used to determine which Pods of a workload cannot be deployed in the same topological domain. For example, the same Pod of a workload can be deployed in different topological domains to improve the stability of the load itself.",
"In container management, what are the two types of workload affinity?",There are two types of workload affinity: must-satisfy (`requiredDuringSchedulingIgnoredDuringExecution`) and try-to-satisfy (`preferredDuringSchedulingIgnoredDuringExecution`).,
"In container management, what is node anti-affinity?",Node anti-affinity refers to preventing Pods from being scheduled on nodes with specified labels.,
" In container management, what is node affinity?",Node affinity refers to scheduling Pods to nodes with specified labels.,
What is tolerance time?,The tolerance time refers to the time window for the system to reschedule the instance to other available nodes when the node where the workload instance is located is unavailable. The default is 300 seconds.,
What is workload anti-affinity?,Workload anti-affinity is mainly used to determine which Pods of the workload cannot be deployed in the same topological domain. It comes in two types: must-satisfy and try-to-satisfy.,
What is workload affinity?,Workload affinity is mainly used to determine which Pods of the workload can be deployed in the same topological domain. It comes in two types: must-satisfy and try-to-satisfy.,
What is node affinity?,Node affinity allows you to constrain which nodes a Pod can be scheduled to based on the labels on the nodes. It comes in two types: must-satisfy and try-to-satisfy.,
What are node selection constraints?,The node selection constraint is to constrain which nodes the Pod can be scheduled to by adding the `nodeSelector` field to the Pod's specification to set the node label you want the target node to have.,
"How do I add, modify or remove taints from a node?","1. Find the target cluster on the `Cluster List` page, click the cluster name, and enter the `Cluster Overview` page. \n2. In the left navigation bar, click `Node Management`, find the node that needs to modify the taint, click the `ⵗ` operation icon on the right and click the `Modify taint` button. \n3. Enter the key value information of the stain in the pop-up box, select the stain effect, click `➕ Add` to add multiple stains to the node, and click the `X` on the right side of the stain effect to delete the stain.",
What are the stain effects?,"Currently, three taint effects are supported:\n- NoSchedule: Pods that cannot tolerate a certain taint will not be scheduled to nodes where the taint exists. \n- PreferNoSchedule: Try to avoid Pods that cannot tolerate a certain stain from being scheduled to nodes where the stain exists. \n- NoExecute: Keep the status quo. If a Pod that cannot tolerate a certain taint is already running on the node before the node sets the taint, the Pod will not be evicted. If a Pod that cannot tolerate a certain taint is not running on the node before the node sets the taint, it will not be scheduled to the node.",
What is Taint?,Taints can cause a node to exclude a certain type of Pod and prevent Pods from being scheduled on that node.,
What information can users see after cluster creation fails?,"After cluster creation fails, users can view the operation log of the creation process to help users quickly locate the fault.",
What new configuration requirements does the workload support?,The workload supports multiple network card configurations and IP Pool display.,
What operations does NetworkPolicy policy management functionality include?,"NetworkPolicy policy management functions include creating, updating, deleting NetworkPolicy policies, and displaying NetworkPolicy policy details.",
What features does Pod List support?,"The Pod list supports viewing the total number and running number of container groups, as well as viewing container types.",
What features does the new Replicatsets productization include?,"The productization of Replicatsets includes the following functions: use the WEB terminal (CloudTTY) to manage Replicatsets, view Replicatsets monitoring, logs, Yaml, events, containers, view Replicatsets details, link the application workbench, and manage the entire life cycle of Replicatsets through Grayscale Release.",
"In container management, which page provides the function of uploading files to the container and downloading files from the Pod to the local?",Use the WEB terminal page to provide the functions of uploading files to the container and downloading files from the Pod to the local.,
Which version of container management adds the VPA (vertical scaling) function?,The v0.14 version of container management adds a new VPA (vertical scaling) function.,
Which version of container management adds the application backup function?,The v0.16.0 version of container management adds a new application backup function.,
Which version of container management adds the ability to create a cluster and enable Cilium dual-stack networking?,The v0.17.0 version of container management adds the function of creating a cluster and enabling Cilium dual-stack network.,
This document mainly introduces the Release Notes of which product?,This document mainly introduces Release Notes for container management.,
How is Containerd and Docker runtime supported on different operating systems?,"CentOS, RedHatOS and KylinOS operating systems all support a range of versions of Continerd and Docker runtimes, but the specific supported versions are different. In KylinOS, Docker is not supported as the runtime under the X86 architecture. Docker is only supported under the ARM architecture.",
In which version of Kubernetes did support for the Dockershim component be removed?,Kubernetes removed support for the Dockershim component in version 1.24.,
In which version of Kubernetes will Containerd be the default container runtime?,Kubernetes makes Containerd the default container runtime in version 1.19.,
Why is Containerd recommended as a container runtime?,"Compared with the Docker runtime, we recommend you to use the lightweight Containerd as your container runtime, because this has become the current mainstream runtime choice.",
What is a container runtime?,The container runtime is an important component in Kubernetes that manages the life cycle of containers and container images.,
How to deauthorize a storage pool to a namespace?,Click `Remove Authorization` under the action bar on the right side of the list to remove the authorization and stop sharing this storage pool to the namespace.,
How to authorize a storage pool to a namespace?,"Find the storage pool that needs to be shared in the storage pool list, click `Authorized Namespace` under the operation bar on the right, and then click `Customized Namespace` to select which namespaces you need to share this storage pool to one by one. Click `Authorize all namespaces` to share this storage pool to all namespaces under the current cluster at one time.",
How many namespaces does the DCE 5.0 container management module support sharing a storage pool with?,The DCE 5.0 container management module supports sharing storage pools to multiple namespaces.,
How to delete a storage pool?,"On the storage pool list page, find the storage pool that needs to be deleted and select Delete in the operation bar on the right.",
How to update a storage pool?,"On the storage pool list page, find the storage pool that needs to be updated, and select `Edit` under the operation bar on the right to update the storage pool.",
"Can the storage pool name, driver, and recycling policy be modified after creation?",Can't.,
Are the steps to create a form cumbersome?,"Yes, the form creation steps are more tedious than YAML creation.",
In what two ways can a storage pool be created?,Storage pools can be created in two ways: YAML and forms. Both methods have their own advantages and disadvantages and can meet the needs of different users.,
What is a storage pool?,"A storage pool refers to the formation of many physical disks into a large storage resource pool. This platform supports the creation of block storage pools, local storage pools, and custom storage pools after connecting to various storage vendors, and then dynamically configures data volumes for workloads.",
Does container management currently support custom roles?,"Currently, container management does not support custom role management through the graphical interface, but permission rules created through kubectl can also take effect.",
What operating permissions does the `NS View` role have?,`NS View` has the following operation permissions: 1. Can view the corresponding namespace; 2. Can view all workloads and custom resources under the corresponding namespace.,
What operation permissions does the `NS Edit` role have?,"`NS Edit` has the following operation permissions: 1. Can view the corresponding namespace with permission; 2. Manage, edit, and view all workloads under the namespace.",
"Among the current default roles for container management, which role has the highest authority?","Among the current default roles for container management, the role with the highest authority is `NS Admin`.",
Which Kubernetes capability are namespace permissions based on?,Namespace permissions are implemented based on Kubernetes RBAC capability authorization.,
What permissions does `Cluster Admin` have?,"`Cluster Admin` has the following permissions: 1. Can manage, edit, and view the corresponding cluster; 2. Can manage, edit, and view all workloads under the namespace and all resources in the cluster; 3. Can authorize users to roles in the cluster (Cluster Admin, NS Admin, NS Edit, NS View).",
What is the current default cluster role for container management?,"Currently, the default cluster role for container management is `Cluster Admin`.",
What two levels of permission control do container management permissions support?,Supports cluster-level and namespace-level permission control.,
What is the basis of the container management permission system?,Container management permissions are based on a multi-dimensional permission management system created by global permission management and Kubernetes RBAC permission management.,
How can I view detailed descriptions of security levels and security modes?,"On the container group security policy configuration page, you can click ""Policy Configuration Item Description"" in the upper right corner of the page to view detailed information.",
What are the security levels and security modes in the container group security policy? What is the description of each of them?,"Security levels include Privileged, Baseline, and Restricted; security modes include Audit, Warn, and Enforce. Their descriptions are: Privileged policy provides the largest possible range of permissions; Baseline policy prohibits known policy enhancements and is the least restrictive; Restricted policy follows the current best practices for protecting Pods. If the Audit mode violates the specified policy, a new audit event will be added to the audit log, and the Pod can be created; if the Warn mode violates the specified policy, a warning message visible to the user will be returned, and the Pod can be created; if the Enforce mode violates the specified policy, the Pod cannot be created.",
How to configure container group security policy for namespace?,"You can enter the namespace details page where the container group security policy needs to be configured through the container management interface. Click ""Configure Policy"" on the ""Container Group Security Policy"" page to enter the configuration page. Then click ""Add Policy"", select the appropriate security level and security mode, set the check items as needed, and finally click OK.",
What are the prerequisites for container group security policy?,"Prerequisites include: the container management module has been connected to the Kubernetes cluster or a Kubernetes cluster has been created; the cluster version needs to be v1.22 or above; the creation of a namespace and user has been completed, and the user has been granted NS Admin or higher permissions.",
What is a container group security policy?,Container group security policy refers to controlling the behavior of Pods in all aspects of security by configuring different levels and modes for specified namespaces in the Kubernetes cluster. Only Pods that meet certain conditions will be accepted by the system. It sets three levels and three modes.,
How to delete a data volume?,"On the data volume list page, find the data that needs to be deleted and select `Delete` under the operation bar on the right.",
What can be updated in a data volume?,"Only updating the alias, capacity, access mode, recycling policy, label, and annotation of the data volume is supported.",
How to clone a data volume?,"On the data volume list page, find the data volume that needs to be cloned, and select `Clone` under the operation bar on the right. Use the original configuration directly, or modify it as needed, and then click OK at the bottom of the page.",
How to view details of a single data volume?,"Click the name of the data volume to view the basic configuration, storage pool information, labels, annotations and other information of the data volume.",
How to view all data volumes in the current cluster?,"Click the name of the target cluster in the cluster list, and then click `Container Storage`->`Data Volume (PV)` in the left navigation bar.",
How to create a data volume through a form?,"1. Click the name of the target cluster in the cluster list, and then click `Container Storage`->`Data Volume (PV)`->`Create Data Volume (PV)` in the left navigation bar. 2. Fill in the basic information.",
How to create a data volume through YAML?,"1. Click the name of the target cluster in the cluster list, and then click `Container Storage`->`Data Volume (PV)`->`YAML Create` in the left navigation bar. 2. Enter or paste the prepared YAML file in the pop-up box, and then click `OK` at the bottom of the pop-up box.",
In what two ways can data volumes be created?,Data volumes can be created in two ways: YAML and forms.,
What is a data volume (PV)?,"A data volume is a piece of storage in the cluster, which can be prepared in advance by the administrator or dynamically prepared using a storage class. PV is a cluster resource, but it has an independent life cycle and will not be deleted when the Pod process ends. Mounting the PV to the workload can achieve data persistence of the workload.",
 How to delete data volume claims?,"On the data volume declaration list page, find the data that needs to be deleted and select `Delete` in the operation bar on the right to delete it.",
 How to expand the data volume statement?,"On the data volume declaration list page, find the data volume declaration whose capacity needs to be adjusted, select `Expand' under the operation bar on the right, enter the target capacity and confirm.",
 How to view data volume declaration?,"On the data volume declaration list page, you can view all data volume declarations in the current cluster, as well as the status, capacity, namespace and other information of each data volume declaration.",
 How to create a data volume claim?,"It is created in two ways: YAML and forms. YAML creation has fewer steps and is more efficient, but it requires familiarity with YAML file configuration. Form creation is intuitive and simple, just fill in the corresponding values according to the prompts.",
 What is a data volume claim (PVC)?,The data volume declaration expresses the user's request for storage. You can apply for a data volume with a specific size and specific access mode.,
What is Repository? what's the effect?,Repository A repository for publishing and storing Charts. It provides a centralized repository from which users can obtain Charts and can obtain more Charts by adding custom repositories.,
What is release? what's the effect?,"Release A Chart instance running on the Kubernetes cluster. A Chart can be installed multiple times in the same cluster, and each installation will create a new Release. Its role is to run an application instance on a Kubernetes cluster, and the instance can be managed through the helm command.",
What is Chart? what's the effect?,"Chart is a Helm installation package, which contains the images, dependencies and resource definitions required to run an application. It may also contain service definitions in the Kubernetes cluster, similar to the formula in Homebrew, APT's dpkg or Yum's rpm file. Its role is to provide a standardized deployment template that can be reused in different environments.",
What are the key concepts in Helm?,"Key concepts in Helm include Chart, Release, and Repository.",
What is Helm? What does it do?,"Helm is a package management tool for Kubernetes that facilitates users to quickly discover, share, and use applications built with Kubernetes. Its function is to facilitate application deployment, share application deployment templates, and improve efficiency.",
What is HwameiStor? What is its role in DCE 5.0 container management?,HwameiStor is an independent open source storage component. Its role in DCE 5.0 container management is to support viewing local storage resource overview and other information in container storage.,
What are the new parameters of Job? What are their functions?,"Job has the following new parameters:\n- backoffLimit: Specifies the number of retries before marking this Job as failed. The default value is 6. \n- completions: Specify the number of Pods that the Job should run and expect to complete successfully. Setting to nil means that the success of any Pod marks the success of all Pods, and allows parallelism to be set to any positive value. Setting to 1 means that parallelism is limited to 1, and the success of this Pod marks the success of the task. \n- parallelism: Specifies the upper limit on the number of Pods the Job should expect to run at any given time. When (.spec.completions - .status.successful) < .spec.parallelism, that is, when the remaining work is less than the maximum parallelism, the actual number of Pods running in steady state will be less than this number. \n- activeDeadlineSeconds: The number of seconds the job can remain active before the system attempts to terminate the job. This length of time is relative to startTime; the field value must be a positive integer. If the job is suspended, this timer is stopped and reset when the job is resumed again.",
What new features are added to DCE 5.0 container management?,"DCE 5.0 container management adds the following new functions:\n- New interface to query PVC events\n- Job new parameters such as backofflimit, completions, parallelism, activeDeadlineSeconds, etc.\n- Integration of independent open source storage component HwameiStor\n- New cluster Inspection function\n- Added application backup function\n- Added platform backup function\n- Added global management of custom role management cluster",
How to fill in the private key content into the interface key input box?,Copy the private key content and paste it into the key input box on the interface.,
"For step 3, why do you need to perform the same operation on each node of the cluster to be created?","Because the public key needs to be loaded onto each node of the cluster to be created, the authentication of these nodes can be achieved. Therefore, the operation of loading the public key needs to be performed on each node.",
What are the public and private key files?,The public and private key files are:\n- `id_rsa`: private key\n- `id_rsa.pub`: public key,
What commands are required to use SSH keys for node authentication?,"To use SSH keys for node authentication, you need to execute the following commands:\n- Generate public and private keys: `cd /root/.ssh`, `ssh-keygen -t rsa`\n- Check whether the key is created successfully: `ls`\ n- Load the public key to the node: `ssh-copy-id -i /root/.ssh/id_rsa.pub root@10.0.0.0`\n- View the private key file: `cat /root/.ssh/id_rsa`",
How to use SSH keys for node authentication?,The following are the steps for using SSH keys for node authentication:\n1. Generate public and private keys on any node in the management cluster where the cluster is to be created. \n2. Check whether the private key file `id_rsa` is created successfully. \n3. Load the public key file `id_rsa.pub` to all nodes of the cluster to be created. \n4. View the private key file `id_rsa` created in step 1.,
What is the maximum allowed difference between host times?,The synchronization error between all hosts is less than 10 seconds.,
What are the CPU and memory resource requirements?,"CPU resources need to be greater than 4 Core, and memory resources need to be greater than 8 GB.",
How should SELinux and firewall be configured?,should be closed.,
Which operating system architectures are supported?,"The ARM architecture supports Kylin Linux Advanced Server release V10 (Sword) SP2, UOS Linux and openEuler. The x86 architecture supports CentOS 7.x, Redhat 7.x/8.x, Flatcar Container Linux by Kinvolk, Debian Bullseye/Buster/Jessie/Stretch, Ubuntu 16.04/18.04/20.04/22.04, Fedora 35/36/Fedora CoreOS/ openSUSE Leap 15.x/Tumbleweed/Oracle Linux 7/8/9/Alma Linux 8/9/Rocky Linux 8/9 and Amazon Linux 2.",
What node configuration checks are required when creating a cluster or adding nodes to an existing cluster?,"Operating system, SELinux, firewall, architectural consistency, host time, network connectivity, CPU and memory resources.",
 How to view monitoring information on the node details page?,"On the node details page, click the ""Monitoring"" tab to view the monitoring chart of resource usage on the node.",
 How to view container group information on the node details page?,"On the node details page, click the ""Container Group"" tab to view detailed information about all container groups running on the node.",
 What information can be viewed on the node details page?,"On the node details page, you can view overview information, container group information, label annotation information, event list and status, etc. In addition, you can also view the node's YAML file, monitoring information, labels and annotations, etc.",
 How to enter the node details page?,"Click the name of the target cluster on the cluster list page, click Node Management in the left navigation bar, and click the name of the target node to enter the node details page.",
 How to view information about each node in the cluster?,"Click the name of the target cluster on the cluster list page, and click Node Management in the left navigation bar to view the node status, roles, labels, CPU/memory usage, IP address, and creation time. Click the node name to enter the node details page to view more information, including overview information, container group information, label annotation information, event list, status, etc.",
Can the RoleBinding and ClusterRoleBinding of custom roles take effect?,"Currently, custom roles cannot be managed through the graphical interface, but custom role rules created through kubectl can also take effect.",
What is the relationship between RBAC global permissions and container management permissions?,"Global permissions are only coarse-grained permissions and can manage the creation, editing, and deletion of all clusters. For fine-grained permissions, such as management permissions for a single cluster or management, editing, and deletion permissions for a single namespace, container management permissions based on Kubernetes RBAC need to be implemented. Normally, you only need to authorize it in the container management.",
"In these permission control rules, which resource scale and status are authorized?","scale for the replicationcontrollers/scale, deployments/scale, replicasets/scale, and statefulsets/scale resources, and status for the persistentvolumeclaims/status, pods/status, replicationcontrollers/status, deployments/status, and horizontalpodautoscalers/status resources are authorized.",
Which Kubernetes resources are the above permission control rules for?,"These permission control rules are for Kubernetes resources such as configmaps, endpoints, persistentvolumeclaims, persistentvolumeclaims/status, pods, replicationcontrollers, replicationcontrollers/scale, serviceaccounts, services, and services/status.",
How to view the logs of a Job Pod?,"You can use the command `kubectl logs <PodName>` to view the logs of a Job Pod, where `<PodName>` refers to the name of the Pod to be viewed.",
How to check a started job?,You can use the command `kubectl get job` to view the started jobs.,
How to start a Job?,"You can use the command `kubectl apply -f myjob.yaml` to start a Job. Among them, `myjob.yaml` is the file name that saves the Job configuration.",
What does the .spec.activeDeadlineSeconds parameter do? What does it have to do with the backoffLimit parameter?,"`.spec.activeDeadlineSeconds` represents the Pod running time. Once this time is reached, the Job, that is, all its Pods, will stop. The relationship with `backoffLimit` is that the Job arriving at `activeDeadlineSeconds` will ignore the setting of `backoffLimit`.",
What does the spec.backoffLimit parameter do?,`spec.backoffLimit` indicates the maximum number of retries for a failed Pod. Retries will not continue beyond this number.,
Please explain what .spec.completions and .spec.parallelism do.,"`.spec.completions` represents the number of Pods that need to run successfully before the job ends, and the default is 1; `.spec.parallelism` represents the number of Pods that run in parallel, and the default is 1.",
What is a job type?,"According to the settings of `.spec.completions` and `.spec.Parallelism`, tasks (Jobs) can be divided into three types: non-parallel Jobs, parallel Jobs with a certain completion count, and parallel Jobs.",
How to delete a certain operation record?,"On this page, you can delete an operation record.",
How many Helm operation records can be set to be retained?,You can set a reasonable number of retained records according to the actual situation. The specific steps are: set how many Helm operation records need to be retained in the target cluster's `Recent Operations`->`Helm Operations`->`Set the Number of Reserved Records`.,
Where can I view recent cluster operation records and Helm operation records?,"On this page, you can view recent cluster operation records and Helm operation records.",
How to delete a network policy?,"There are two ways to delete a network policy. Supports deletion via form or YAML file. \n- On the network policy list page, find the policy that needs to be deleted, and select `Delete` in the operation bar on the right.\n- Click on the name of the network policy, enter the details page of the network policy, and select ` in the upper right corner of the page Update` can be updated through the form, select `Edit YAML` to delete through YAML.",
How to update network policy?,"There are two ways to update network policies. Supports updating network policies through forms or YAML files. \n- On the network policy list page, find the policy that needs to be updated. Select `Update` under the operation bar on the right to update through the form. Select `Edit YAML` to update through YAML. \n- Click the name of the network policy, enter the details page of the network policy, select `Update` in the upper right corner of the page to update through the form, select `Edit YAML` to update through YAML.",
How to view network policy?,"1. Click the name of the target cluster in the cluster list, then click `Container Network`->`Network Policy` in the left navigation bar, and click the name of the network policy. \n2. View the basic configuration, associated instance information, inbound traffic policy, and outbound traffic policy of the policy.",
How to create a network policy through a form?,"1. Click the name of the target cluster in the cluster list, and then click `Container Network`->`Network Policy`->`Create Policy` in the left navigation bar. \n2. Fill in the basic information and policy configuration.",
How to create a network policy through YAML?,"1. Click the name of the target cluster in the cluster list, and then click `Container Network`->`Network Policy`->`YAML Create` in the left navigation bar. \n2. Enter or paste the prepared YAML file in the pop-up box, and then click `OK` at the bottom of the pop-up box.",
How to create a network policy?,"Currently, network policies are created through YAML and forms. Creating through YAML has fewer steps and is more efficient, but the threshold requirements are higher and you need to be familiar with the YAML file configuration of network policies. Creating a form is more intuitive and simpler. Just fill in the corresponding values according to the prompts, but the steps are more complicated.",
What is network strategy?,"Network policies can control network traffic at the IP address or port level (OSI Layer 3 or 4). The container management module supports the creation of network policies based on Pods or namespaces, and supports label selectors to set which traffic can enter or leave Pods with specific labels.",
"When installing the `kubernetes-cronhpa-controller` plugin, what parameters should be entered for name, namespace and version?","Parameter name: Enter the plug-in name. Please note that the name can be up to 63 characters long, can only contain lowercase letters, numbers and separators (""-""), and must start and end with lowercase letters or numbers. \n- Namespace: Select the namespace in which the plugin will be installed. \n- Version: The version of the plug-in.",
"If you need to delete the `kubernetes-cronhpa-controller` plug-in, which page should you do it on?","The operation should be performed on the `Helm application` list page, otherwise only the workload copy of the plug-in will be deleted, and the plug-in itself will not be deleted. An error will also be prompted when the plug-in is reinstalled later.",
What are the steps to install the `kubernetes-cronhpa-controller` plug-in?,"The steps are as follows:\n1. Open the workload details tab of the target cluster, click the Auto Scaling tab, and then click the Install button on the right side of CronHPA. \n2. Read the relevant introduction of the plug-in and select the version and click the install button. \n3. Enter the plug-in name, select the namespace and version, and configure parameters such as ready waiting, failed deletion, and detail logs as needed. \n4. Click OK and jump to the Helm application list page, where you can see the application you just installed. Return to the Auto Scaling tab under the workload details page, and you can see that the interface shows that the plug-in has been installed.",
What prerequisites need to be met before installing the `kubernetes-cronhpa-controller` plugin?,"The following prerequisites need to be met:\n- Connect to the Kubernetes cluster or create a Kubernetes cluster in the container management module, and be able to access the UI interface of the cluster. \n- Create a namespace. \n- The current operating user should have `NS Edit` or higher permissions.",
Why do I need to install the `metrics-server` plugin before using CronHPA?,Because CronHPA needs to use the `metrics-server` plugin to obtain resource usage information.,
How to remove `metrics-server` plugin?,"When deleting the `metrics-server` plug-in, the plug-in can be completely deleted only on the Helm application list page. If you only delete `metrics-server` on the workload page, this will only delete the workload copy of the application. The application itself will not be deleted, and an error will be prompted when you reinstall the plug-in later.",
What steps are required to install the `metrics-server` plugin?,"1. On the Auto Scaling page under workload details, click the Install button to enter the `metrics-server` plug-in installation interface. \n2. Read the introduction of the `metrics-server` plug-in, select the version and click the install button. \n3. Configure basic parameters in the installation configuration interface. \n4. Configure advanced parameters based on factors such as whether the cluster network can access the k8s.gcr.io warehouse. \n5. Click the OK button to complete the installation of the `metrics-server` plug-in.",
Which component of Kubernetes is `metrics-server`?,`metrics-server` is Kubernetes' built-in resource usage indicator collection component.,
What are the prerequisites for installing the `metrics-server` plugin?,"1. The container management module has been connected to the Kubernetes cluster or has created a Kubernetes cluster, and can access the UI interface of the cluster. \n2. The creation of a namespace has been completed. \n3. The current operating user should have NS Edit or higher permissions.",
How to complete the installation of Velero plug-in?,"The steps to install the Velero plug-in are as follows:\n1. On the cluster list page, find the target cluster where the `velero` plug-in needs to be installed, click the name of the cluster, click `Helm Application` -> `Helm Template` in the left navigation bar, and click Enter `velero` in the search bar to search. \n2. Read the introduction of `velero` plug-in, select the version and click the `Install` button. This article will take `3.0.0` version as an example for installation. It is recommended that you install `3.0.0` and higher versions. \n3. Configure basic parameters in the installation configuration interface. \n4. Installation parameter configuration. \n5. Click the OK button to complete the Velero plug-in installation.",
What are the optional parameters of the Velero plug-in?,"Optional parameters for the Velero plug-in include:\n- Backupstoragelocation: The location where velero backup data is stored. \n- Name: The name of the BackupStorageLocation object that has been created. \n- Bucket: The name of the bucket used to save backup data. \n- Accessmode: velero's access mode to data, you can choose one of ReadWrite, ReadOnly, and WriteOnly. \n- Region: The geographical area of cloud storage. The `us-east-1` parameter is used by default, and is provided by the system administrator\n- S3forcepathstyle: Enable or disable access using the S3 path style, that is, using the bucket name as part of the URL path. \n- Use secret: Enable or disable using secret to access object storage.",
What parameter configurations are required when installing the Velero plug-in?,"Velero plug-in installation requires the following parameter configuration:\n- Name: Enter the plug-in name. Please note that the name can be up to 63 characters, can only contain lowercase letters, numbers and separators (""-""), and must start with a lowercase letter or number. and the end, such as metrics-server-01. \n- Namespace: Select the namespace for plug-in installation, which must be the `velero` namespace. \n- Version: The version of the plug-in, here we take the `3.0.0` version as an example. \n- Ready waiting: When enabled, it will wait for all associated resources under the application to be in a ready state before marking the application as successfully installed. \n- Failed deletion: When enabled, synchronization will be enabled by default and ready to wait. If the installation fails, installation-related resources will be deleted. \n- Detailed log: Enable detailed output of the installation process log. \n- S3url: Object storage access address (currently only Minio has been verified for compatibility). \n- Use existing secret: The secret name used to record the username and password of the object storage. \n- Features: enabled kubernetes feature plug-in module.",
What are the prerequisites before installing the Velero plug-in?,"Before installing the `velero` plug-in, you need to meet the following prerequisites:\n- Container management module [Connected to Kubernetes cluster] (../clusters/integrate-cluster.md) or [Kubernetes cluster created] (../clusters /create-cluster.md), and can access the UI interface of the cluster. \n- Completed a `velero` [namespace creation](../namespaces/createns.md). \n- The current operating user should have [`NS Edit`](../permissions/permission-brief.md#ns-edit) or higher permissions. For details, please refer to [Namespace Authorization](../namespaces/createns .md).",
What are Velero plugins?,"Velero is an open source tool for backing up and restoring Kubernetes cluster resources. It can back up resources in a Kubernetes cluster to a cloud storage service, local storage, or other locations, and restore those resources to the same or a different cluster when needed.",
What should you pay attention to if you need to delete the `vpa` plug-in?,"If you want to delete the `vpa` plug-in, you should go to the `Helm application` list page to delete it completely. If you delete a plug-in under the Auto Scaling tab of the workload, this will only delete the workload copy of the plug-in. The plug-in itself will not be deleted, and an error will be prompted when you reinstall the plug-in later.",
How to install `vpa` plugin?,"1. On the `Cluster List` page, find the target cluster where you need to install this plug-in, click the name of the cluster, then click `Workload` -> `Stateless Workload` on the left, and click the name of the target workload. \n2. On the workload details page, click the `Auto Scaling` tab, and click `Install` on the right side of `VPA`. \n3. Read the relevant introduction of the plug-in, select the version and click the `Install` button. It is recommended to install `1.5.0` or higher. \n4. View the following description of configuration parameters. \n- Name: Enter the plug-in name. Please note that the name can be up to 63 characters, can only contain lowercase letters, numbers and separators (""-""), and must start and end with lowercase letters or numbers, such as kubernetes-cronhpa- controller. \n- Namespace: Select the namespace in which the plug-in will be installed. Here we take `default` as an example. \n- Version: The version of the plug-in, here we take the `1.5.0` version as an example. \n- Ready waiting: When enabled, it will wait for all associated resources under the application to be in a ready state before marking the application as successfully installed. \n- Failed deletion: If the plug-in installation fails, delete the associated resources that have been installed. Once enabled, `Ready Waiting` will be enabled synchronously by default. \n- Detailed log: When enabled, detailed logs of the installation process will be recorded. \n5. Click `OK` in the lower right corner of the page, and the system will automatically jump to the `Helm application` list page. Wait a few minutes and refresh the page to see the application you just installed.",
What are the prerequisites for installing the `vpa` plugin?,"You need to [Connect to Kubernetes cluster](../clusters/integrate-cluster.md) or [Create Kubernetes cluster](../clusters) in the [Container Management](../../intro/what.md) module /create-cluster.md), and be able to access the UI interface of the cluster, create a [namespace](../namespaces/createns.md), the current operating user should have [`NS Edit`](../permissions/ permission-brief.md#ns-edit) or higher.",
What is a vpa plugin?,`vpa` is a key component to achieve vertical expansion and contraction of containers.,
What is the vertical scaling strategy for containers?,"The container vertical expansion and contraction strategy (Vertical Pod Autoscaler, VPA) can make cluster resource allocation more reasonable and avoid cluster resource waste.",
What should I do if the cluster status is always `Connecting`?,"If the cluster status is always ""Connecting"", you need to confirm whether the access script is successfully executed on the corresponding cluster.",
What characteristics should a cluster name have? Can it be modified?,"The cluster name should be unique and cannot be changed after setting. It can be up to 63 characters long, can only contain lowercase letters, numbers and delimiters (""-""), and must start and end with lowercase letters or numbers.",
What are the steps to connect to a cluster?,"The steps are as follows:\n1. Enter the `Cluster List` page and click the `Connect to Cluster` button in the upper right corner. \n2. Fill in the basic information, including name, alias and release version. \n3. Fill in the KubeConfig of the target cluster and verify it before successful access. \n4. Confirm that all parameters are filled in correctly, and click the `OK` button in the lower right corner of the page.",
What are the prerequisites for accessing the cluster?,"Prerequisites include: preparing a cluster to be connected, ensuring that the network between the container management cluster and the cluster to be connected is smooth, and the Kubernetes version of the cluster is 1.22+. The current operating user should have `NS Edit` or higher permissions.",
Which container clusters does the container management module support?,"The container management module supports access to a variety of mainstream container clusters, such as DaoCloud KubeSpray, DaoCloud ClusterAPI, DaoCloud Enterprise 4.0, Redhat Openshift, SUSE Rancher, VMware Tanzu, Amazon EKS, Aliyun ACK, Huawei CCE, Tencent TKE, and standard Kubernetes clusters.",
What types of services are supported in the container management feature?,Currently supported service types include: intra-cluster access (ClusterIP) and node access (NodePort).,
What does the cloud native application load include in the container management function?,"Application full life cycle management, one-stop application load creation, cross-cluster application load management, application load expansion and contraction, container life cycle settings, container readiness check and survival check settings, container environment variable settings and automatic scheduling of containers .",
What does the cluster full life cycle management in the container management function include?,"Unified cluster management, rapid cluster creation, one-click cluster upgrade, cluster high availability, certificate hot update and node management.",
What are the main features of container management capabilities?,"Cluster full life cycle management, cloud native application load, services and routing, namespace management, container storage, policy management, extension plug-ins, permission management and cluster operation and maintenance.",
How to protect slow start container before starting?,"You can use pre-start checks to protect slow-start containers by setting appropriate `failureThreshold * periodSeconds` parameters to deal with scenarios where startup takes a long time, and use a separate probe task to perform pre-start checks. For example, you can set `failureThreshold: 30` and `periodSeconds: 10` to wait up to 5 minutes for the startup process to complete.",
How to use command check to check the health status of containers?,"You can execute a command in the container and configure the command check through the `exec` field. For example, `command: [cat, /tmp/healthy]` means to execute the `cat /tmp/healthy` command in the container at regular intervals. Health status check.",
How to set the execution frequency of liveness detection in YAML file?,"You can use the `periodSeconds` field to set the execution frequency of the survival probe, for example `periodSeconds: 5` means to execute the survival probe every 5 seconds.",
What is survival detection?,"Liveness detection is a mechanism for checking whether a container is alive. When the liveness detection fails, the container will be restarted.",
How to protect slow-start containers? What type of detection can be used?,"You can use startup probes to perform liveness checks on slow-start containers to prevent them from being killed before they start running. Parameters such as delay time, failure threshold, and cycle time should be set appropriately.",
How to use TCP port for container health check? What should I pay attention to?,"For containers that provide TCP communication services, you need to specify the port that the container listens on and perform detection according to the set rules. Pay attention to setting parameters such as delay time and timeout time.",
"In the HTTP GET parameters, what do the delay time and timeout time mean?","The delay time means that after the container is started, it waits for a certain period of time before performing a health check. The unit is seconds. The timeout period indicates the maximum waiting time for health check execution, in seconds.",
What are the configuration parameters for liveness and readiness checks?,"The configuration parameters of survival check and readiness check are similar, mainly including parameters such as path, port, access protocol, delay time, timeout time, success threshold and maximum number of failures.",
What are container health checks? What types are there?,"Container health check is an operation to check the health status of the container based on user needs. Depending on specific needs, Kubernetes provides three types of checks: Liveness, Readiness, and Startup.",
How to delete Helm app?,"Follow the steps below to delete the Helm application:\n1. Enter the cluster details page and click `Helm Application` in the left navigation bar. \n2. Select the application you want to delete and click `Delete` in the operation menu on the right. \n3. Enter the application name in the pop-up confirmation box, and then click `Delete` to complete the deletion.",
How to view Helm operation records?,"You can select `Cluster Operations` -> `Recent Operations` -> `Helm Operations` in the left navigation bar to view the detailed operation records and logs of each Helm installation, update, and deletion.",
How do I update an installed Helm application?,"Follow the steps below to update the installed Helm application:\n1. Enter the cluster details page and click `Helm Application` in the left navigation bar. \n2. Select the application that needs to be updated and click `Update` in the operation menu on the right. \n3. Modify the corresponding parameters, compare the file changes before and after modification, and then click `OK` to complete the update.",
What operations can be performed through the container management interface?,"Helm application creation, full life cycle management, update and deletion operations can be performed through the container management interface.",
How do I install the Helm app?,"Follow the steps below to install the Helm application:\n1. Enter the cluster details page and click `Helm Application` -> `Helm Template` in the left navigation bar. \n2. Select the Chart you want to install, select the version and click the `Install` button. \n3. Configure the name, namespace and version information, customize the parameters by modifying YAML, and then click `OK`.",
What management functions does the container management module support for Helm?,"The container management module supports interface management of Helm, including using Helm templates to create Helm instances, customizing Helm instance parameters, and performing full life cycle management of Helm instances.",
What are the prerequisites for adding a third-party Helm warehouse?,"You need to connect to the Kubernetes cluster and access the UI interface of the cluster. You also need to create a namespace and user, and grant the user NS Admin or higher permissions. If you use a private warehouse, the user needs to have read and write permissions on the private warehouse. .",
How to delete an unnecessary Helm repository?,"Click Helm application -> Helm warehouse in the left navigation bar of the cluster details page, enter the Helm warehouse list page, find the Helm warehouse that needs to be deleted, click the ⋮ button on the right side of the list, click Delete in the pop-up menu, and confirm the deletion Enter the name on the interface and click Delete to delete the Helm warehouse.",
How to update the address information of Helm warehouse?,"Click Helm Application -> Helm Warehouse in the left navigation bar of the cluster details page, enter the Helm warehouse list page, find the Helm warehouse that needs to be updated, click the ⋮ button on the right side of the list, click Update in the pop-up menu, and click Update on the edit page After making the update, click OK to update the address information of the Helm warehouse.",
How to add a third-party Helm repository?,"You can click Helm Application -> Helm Warehouse in the left navigation bar of the cluster details page to enter the Helm Warehouse page, then click the Create Warehouse button, fill in the relevant parameters (such as name, address, authentication method, etc.) on the creation page, and finally click Confirm to add the third-party Helm warehouse.",
What is a Helm repository? What preset warehouses are there?,"The Helm warehouse is a repository used to store and publish charts. The system has four preset warehouses built in by default: partner, system, addon, and community.",
"If there is a problem with the etcd database, how to manually restore the cluster?",You can manually restore the cluster according to the following steps:\n1. Back up etcd data\n2. Stop kube-apiserver service\n3. Stop kubelet service\n4. Reset etcd database\n5. Restore etcd database\n6. Start kubelet service and wait Synchronization completed\n7. Restart the kube-apiserver service,
How to check whether etcd member node is normal?,You can check the node status using the following etcdctl command:\n```shell\netcdctl endpoint status --endpoints=<etcd node list> --cacert=<etcd CA certificate path> --cert=<client certificate path> -- key=<client key path>\n```,
What operations need to be performed when restoring data on other nodes?,Start the etcd Pod and wait for data synchronization to complete to check whether the cluster status is normal.,
What parameters need to be set when executing the etcdbrctl command?,"You need to set the --data-dir parameter (etcd data directory), --store-container parameter (S3 storage location), --storage-provider parameter (data storage type), --initial-cluster parameter (etcd initialization configuration), --initial-advertise-peer-urls parameter (etcd member access address between clusters).",
What is the command line tool used to perform restore operations?,The command line tool used to perform restore operations is etcdbrctl.,
Where are the backup files of etcd data stored?,"The backup files of etcd data are stored in S3 storage, corresponding to the bucket in MinIO.",
What environment variables need to be set when restoring etcd data?,"Three environment variables, ECS_ENDPOINT, ECS_ACCESS_KEY_ID, and ECS_SECRET_ACCESS_KEY, need to be set.",
Which command line tool should be executed during restore?,Execute the etcdbrctl command line tool to perform the restore operation.,
Which open source tool needs to be installed on any k8s node before restoration?,The etcdbrctl open source tool needs to be installed on any k8s node.,
Under what circumstances can etcd backup and restore only be used?,DCE 5.0 ETCD backup and restore is limited to backup and restore of the same cluster (the number of nodes and IP addresses have not changed).,
How to shut down a cluster?,"Move the static Pod manifest file outside the `/etc/kubernetes/manifest` directory, and the cluster will remove the corresponding Pod to shut down the service.",
Why check whether the backup data and backup data exist in S3 storage?,"Before restoring, you need to check whether the backup data and the backup data in S3 storage exist to ensure the availability and correctness of the backup data.",
How to view backup policy details?,"Click to enter the backup policy details, including basic information and backup records. You can view the backup point. After selecting a cluster, you can view all backup information under the cluster. Each time a backup is performed, a backup point is generated, and the application can be quickly restored through the successful backup point.",
How to create etcd backup?,"You can follow the following steps to create:\n1. On the Backup and Recovery-ETCD Backup page, you can see all current backup policies. Click Create Backup Policy on the right to create an ETCD backup policy for the target cluster. \n2. Fill in the basic information and click Next after completing the filling. The connectivity of ETCD will be automatically verified. If the verification passes, proceed to the next step. \n3. Select the backup method, which is divided into manual backup and scheduled backup. \n4. Storage location. \n5. After successful creation, a piece of data will be generated in the backup policy list. Operations include log, view YAML, update policy, stop, and execute immediately. When the backup method is manual, you can click Execute Now to perform the backup. When the backup method is scheduled backup, backup will be performed according to the configured time.",
What are the prerequisites for ETCD backup?,"You need to have connected to the Kubernetes cluster or created a Kubernetes cluster, and be able to access the UI interface of the cluster; you have completed the creation of a namespace and user, and granted the user NS Admin or higher permissions; prepare a MinIO instance.",
What is ETCD backup?,"ETCD backup is a backup with cluster data as the core. In scenarios such as hardware device damage, development and test configuration errors, etc., the backup cluster data can be restored.",
How to set up an exclusive node for a namespace?,"Exclusive nodes can be set for the namespace through the UI management interface of DCE 5.0. Specific steps: Click the cluster name on the cluster list page, then click `Namespace` on the left navigation bar -> Click on the namespace name -> Click on the `Exclusive Node` tab -> Select to make the namespace exclusive on the left side of the page Which nodes are shared? You can clear or delete a selected node on the right, and finally click OK at the bottom.",
How to enable the `PodNodeSelector` and `PodTolerationRestriction` admission controllers on the cluster API server?,"First, you need to check whether the `PodNodeSelector` and `PodTolerationRestriction` admission controllers are enabled on the API server of the current cluster. You can execute the command to check. If it is not enabled, you need to modify the `kube-apiserver.yaml` configuration file on any Master node in the current cluster and add the `,PodNodeSelector,PodTolerationRestriction` parameters.",
What is a namespace exclusive node?,"Namespace exclusive nodes refer to the method of setting taint and taint tolerance in the kubernetes cluster to achieve the exclusive use of one or more node CPU, memory and other resources by a specific namespace, allowing important applications to exclusively use part of the computing resources, thereby sharing with other Applications are physically isolated.",
How to create a namespace via YAML?,"Click `Namespace` in the left navigation bar of the cluster details page, then click the `YAML Create` button on the right side of the page, enter or paste the prepared YAML content, or directly import the existing YAML file from the local, and finally in Click OK in the lower right corner of the pop-up box.",
How to create namespace via form?,"Click `Namespace` on the left navigation bar of the cluster details page, then click the `Create` button on the right side of the page, fill in the namespace name, configure workspace and label (optional settings) in the pop-up window, and then click `OK `.",
What can be bound to a namespace?,"A namespace can be bound to a workspace, and after binding, the resources of the namespace will be shared with the bound workspace.",
Why does a namespace need to be created?,It is recommended to create other namespaces instead of using the default `default` namespace directly for easier management.,
What is a namespace?,"Namespace is an abstraction used in Kubernetes for resource isolation. A cluster can contain multiple namespaces with non-overlapping names, and the resources in each namespace are isolated from each other.",
How to disconnect or uninstall?,"Find the cluster that needs to be uninstalled/uninstalled on the `Cluster List` page, click `...` on the right and click `Unconnected` or `Uninstall Cluster` in the drop-down list, enter the cluster name to confirm, and then click `delete`.",
What happens after the cluster is uninstalled?,"After the cluster is uninstalled, it will be destroyed and the data of all nodes will be reset. It is recommended to back up the data that needs to be saved in advance.",
What do I need to do before uninstalling the cluster?,"Before uninstalling the cluster, you should turn off `Cluster Deletion Protection` in `Cluster Settings`->`Advanced Configuration`, otherwise the `Uninstall Cluster` option will not be displayed.",
What permissions can perform uninstallation or detachment operations?,The current operating user must have Admin or Kpanda Owner permissions to perform uninstallation or disconnection operations.,
What is the difference between detaching and uninstalling a cluster?,Disconnecting only removes the cluster from the container management module. It does not destroy the cluster or destroy the data. Uninstalling a cluster will destroy the cluster and reset the data of all nodes in the cluster. All data will be destroyed and backups are recommended. A cluster must be re-created when needed later.,
How to scale down nodes in a cluster?,"1. Click the name of the target cluster on the `Cluster List` page. \n2. Click `Node Management` in the left navigation bar, find the node that needs to be uninstalled, click `ⵗ` and select `Remove Node`. \n3. Enter the node name and confirm deletion.",
What precautions should be taken when scaling down nodes?,"When scaling down nodes, they need to be uninstalled one by one and cannot be uninstalled in batches. And the number of cluster controller nodes needs to be an odd number. In addition, the first controller node cannot be taken offline.",
What conditions need to be met for a cluster controller node to be uninstalled?,"If you need to uninstall cluster controller nodes, you need to ensure that the final number of controller nodes is an odd number. And the first controller node cannot be taken offline. If this operation must be performed, please contact the after-sales engineer.",
What are the prerequisites for cluster node scaling?,"The prerequisites for cluster node scaling include: the current operating user has `Cluster Admin` role authorization, clusters created through the container management module only support this operation, and before uninstalling a node, it is necessary to suspend the scheduling of the node and expel the applications on the node to other locations. node.",
What is node scaling?,"Node shrinking refers to uninstalling redundant nodes in the cluster to save resource costs, but it will cause the application on the node to be unable to continue running.",
How do I view backup plan details?,"After the backup is completed, you can click the name of the backup plan to view the details of the backup plan.",
What information configuration needs to be paid attention to during the backup process?,"The following information configuration needs to be noted during the backup process:\n- Name: The name of the new backup plan. \n- Source cluster: The cluster where the application backup is scheduled to be performed. \n- Object storage location: The access path to the object storage configured when installing velero on the source cluster. \n- Namespace: The namespace that needs to be backed up. Multiple selection is supported. \n- Advanced configuration: Back up specific resources in the namespace, such as an application, based on resource tags, or not back up specific resources in the namespace based on resource tags during backup. \n- Backup frequency: Set the time period for task execution based on minutes, hours, days, weeks, and months. Supports custom Cron expressions with numbers and *. Retention time (days): Set the time for saving backup resources. \n- Backup data volume (PV): whether to back up the data in the data volume (PV), supports direct copying and using CSI snapshot.",
What prerequisites need to be met before performing a stateless workload backup?,"Before backing up stateless workloads, the following prerequisites need to be met:\n- Connect to the Kubernetes cluster or create a Kubernetes cluster in the container management module, and be able to access the UI interface of the cluster. \n- Create a namespace and user. \n- The current operating user should have NS Edit or higher permissions. For details, please refer to Namespace Authorization. \n-The velero component is installed and running normally. \n- Create a stateless workload (the workload in this tutorial is named dao-2048) and label the stateless workload app: dao-2048.",
This article describes how to perform backup of which workload?,This article describes how to back up data for stateless workloads by applying the backup module.,
How to use Pod field as the value of environment variable?,"You can use the Pod field as the value of an environment variable, that is, use the ""variable/variable reference"" configuration method to configure environment variables for the Pod.",
How to use the fields defined by Container as the value of environment variables?,"You can use the fields defined by the Container as the value of the environment variable, that is, use the ""resource reference"" configuration method to configure the environment variable for the Pod.",
How to use custom key-value pairs as environment variables of the container?,"You can use customized key-value pairs as environment variables of the container, that is, use the ""key-value pair"" configuration method to configure environment variables for the Pod.",
What configuration methods does DCE container management support for configuring environment variables for Pods?,"DCE container management supports the following configuration methods for configuring environment variables for Pods: key-value pairs, resource references, variables/variable references, configuration item key-value import, key key-value import, key import, and configuration item import.",
What are environment variables?,"An environment variable refers to a variable set in the container's running environment and is used to add environment flags to Pods or transfer configurations, etc.",
How to add labels and annotations to workloads and container groups?,You can click the Add button to add labels and annotations to workloads and container groups.,
What are some ways to create stateful payloads?,Stateful loads can be created in two ways: images and YAML files.,
What does a controller with stateful loads guarantee?,StatefulSet can ensure that each Pod has a unique and stable network identifier and stable storage volume in the cluster.,
What is stateful load?,"Stateful workloads refer to applications that require persistent storage and require an independent network identifier, such as databases.",
What is a controller for stateful loads?,The controller for stateful loads is StatefulSet.,
How to create stateful payloads from YAML files?,"In the Workload->Stateful Load column on the cluster details page, click the YAML Create button, enter or paste the prepared YAML file and click OK to complete the creation. Detailed steps and sample YAML can be found in the documentation.",
How to perform advanced configuration?,"Advanced configuration includes four parts: network configuration, upgrade strategy, container management strategy and scheduling strategy. For specific operations, please refer to the screenshots and instructions corresponding to each section in the document.",
How to add services for stateful workloads?,"Click the Create Service button to create a service. To create a service, you need to refer to the Create Service Document. For specific steps, please refer to the screenshots in the document.",
What stages does the life cycle of stateful workloads include?,"The life cycle of a stateful load includes when the container starts, after starting, and before stopping. For the commands that need to be executed, please refer to the container life cycle configuration.",
What configuration items are there for stateful load?,"There are life cycle configuration, health check configuration, environment variable configuration, data storage configuration, security settings, etc. Among them, life cycle, health check and data storage are optional, and security settings are required.",
How to configure services for stateful workloads?,"To configure services for stateful loads so that stateful loads can be accessed externally, you can follow the following steps:\n1. Select the target cluster->Workload->Stateful Load->Service Configuration->Create Service in the container management module . \n2. Refer to the requirements for creating a service and fill in the basic information, port mapping and other configurations of the service. \n3. Click OK to complete the creation of the service.",
What does container configuration include?,"Container configuration is divided into six parts: basic information, life cycle, health check, environment variables, data storage, and security settings.",
What prerequisites need to be met when creating a StatefulSet image?,"- Connect to or create a Kubernetes cluster and be able to access the cluster's UI interface. \n- Create a namespace and user. \n- The current operating user should have NS Edit or higher permissions. \n- When you have multiple containers in a single instance, make sure that the ports used by the containers do not conflict.",
How to create a stateful load?,"There are two ways: image creation and YAML file creation. \n- Image creation: Select the target cluster -> Workload -> Stateful Load -> Image creation in the container management module, fill in the basic information, container configuration, service configuration, and advanced configuration, and click OK to complete the creation. \n- YAML file creation: Select the target cluster->Workload->Stateful Load->YAML creation in the container management module, define the detailed configuration of the StatefulSet through the YAML file, and click OK to complete the creation.",
What is the difference between StatefulSet and Deployment?,"StatefulSet is stateful and is mainly used to manage stateful applications. Pods in a StatefulSet have permanent IDs, making it easy to identify the corresponding Pod when matching storage volumes. Deployment is stateless and does not save data.",
Which two scaling modes does VPA vertical scaling support?,"Currently, vertical scaling supports manual and automatic scaling modes.",
What rules should we pay attention to when naming vertical scaling strategies?,"The vertical scaling policy name can only be up to 63 characters long, can only contain lowercase letters, numbers, and delimiters (""-""), and must start and end with lowercase letters or numbers. For example vpa-my-dep.",
Why can using VPA improve the overall resource utilization of the cluster?,"Using VPA can more rationally allocate resources to each Pod in the cluster, improve the overall resource utilization of the cluster, and avoid wasting cluster resources.",
What two ways does DCE 5.0 support to modify resource request values?,DCE 5.0 supports manual and automatic modification of resource request values.,
What is a container vertical scaling strategy?,"The container vertical scaling strategy (Vertical Pod Autoscaler, VPA) calculates the most suitable CPU and memory request values for the Pod by monitoring the resource request and usage of the Pod over a period of time.",
What is a custom resource example CR?,Custom resource example CR (Custom Resource) refers to a specific instance written based on an already created custom resource. It can be created through the container management module interface.,
How to create a custom resource example via YAML?,"First, you need to enter the cluster details page, click ""Custom Resources"" in the left navigation bar, then click the corresponding custom resource to enter the details page, then click the ""YAML Create"" button in the upper right corner, fill in the YAML statement and click ""OK"" .",
How to create custom resources via YAML?,"First, you need to enter the cluster details page, click ""Custom Resources"" in the left navigation bar, then click the ""YAML Create"" button in the upper right corner, fill in the YAML statement and click ""OK"".",
What interface management functions does the container management module provide for custom resources?,"The container management module supports obtaining the list and detailed information of custom resources under the cluster, creating custom resources based on YAML, creating custom resource example CR (Custom Resource) based on YAML, and deleting custom resources.",
What problem are custom resources created to solve?,Custom resources are created to expand API capabilities in Kubernetes and meet business needs in some special scenarios where existing preset resources cannot meet business needs.,
What configuration information needs to be filled in when creating a key in YAML?,"When creating a key in YAML, you need to fill in the configuration information such as API version, type, metadata and data, for example:\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\nname: secretdemo\ntype: Opaque\ndata :\nusername: ******\npassword: ******\n```",
What configuration information do you need to pay attention to when creating a key in a graphical form?,"When creating the graphical form, you need to pay attention to: 1. The name of the key must be unique in the same namespace; 2. Key type: default (Opaque), TLS, image warehouse information, user name and password, and custom; 3. Key data: Different types need to fill in different parameters. For example, the default type and custom type can fill in multiple key-value pairs. The TLS type needs to fill in certificate certificate and private key data. The mirror warehouse information type needs to fill in private. The account and password of the mirror warehouse, user name and password type need to be specified.",
What two methods are supported for creating keys?,Creating keys supports two methods: graphical form creation and YAML creation.,
What are the usage scenarios for keys?,The key can be used as an environment variable of the container to provide some necessary information required during the running of the container; the key can be used as the data volume of the Pod; and used as the identity authentication credential of the image warehouse when kubelet pulls the container image.,
What is a key?,"Keys are a resource object used to store and manage sensitive information such as passwords, OAuth tokens, SSH, TLS credentials, etc. Using keys allows you to avoid including sensitive confidential data in your application code.",
How to modify or delete a created service?,Click `︙` on the right side of the service list to modify or delete the selected service.,
How to add protocol port in LoadBalancer service?,"You need to first select the port protocol type (TCP or UDP), and then enter the custom port name, the access port for the Pod to provide external services, the container port that the workload actually listens to, and the port of the node to complete the protocol port addition.",
What is the role of the MetalLB IP pool in the LoadBalancer service?,"When the load balancing type is MetalLB, the LoadBalancer Service will allocate IP addresses from the IP pool by default and announce all IP addresses in this pool through APR.",
Why set up an external traffic policy?,"External traffic policies are used to set the scope of traffic forwarding. Cluster means that traffic can be forwarded to Pods on all nodes in the cluster, and Local means that traffic will only be sent to Pods on this node.",
Is access type required in LoadBalancer service?,"It is a required field. You need to specify the method of Pod service discovery. Here, select node access (NodePort).",
What is the difference between the LoadBalancer service and the NodePort service and ClusterIP service?,"The LoadBalancer service uses the cloud provider's load balancer to expose services to the outside and can route traffic to the automatically created NodePort and ClusterIP services, which only expose services within the cluster and on the nodes.",
What is the difference between ClusterIP service and NodePort service?,The ClusterIP service exposes services through the cluster's internal IP and can only be accessed within the cluster; the NodePort service exposes services through the IP and static port on each node and can also be accessed outside the cluster through the node IP and port.,
What types of services can be created in Kubernetes?,"Three types of services can be created in Kubernetes: ClusterIP, NodePort, and LoadBalancer.",
Why do you need to create a service?,"In a Kubernetes cluster, each Pod has an internal independent IP address, but Pods in the workload may be created and deleted at any time. Direct use of the Pod IP address cannot provide external services. Creating a service gives you a fixed IP address, decoupling the workload front-end and back-end, and providing load balancing capabilities so external users can access the workload.",
What is a service in Kubernetes?,"Services in Kubernetes are a mechanism that decouples the front-end and back-end by providing a fixed IP address for the workload, and provides load balancing functions to enable external users to access the workload.",
What volumes are mounted on the metrics-exporter container?,The metrics-exporter container does not have any volumes mounted.,
What is the image corresponding to the sidecar container?,The image corresponding to the sidecar container is docker.m.daocloud.io/bitpoke/mysql-operator-sidecar-5.7:v0.6.1.,
What is the image corresponding to the metrics-exporter container?,The image corresponding to the metrics-exporter container is prom/mysqld-exporter:v0.13.0.,
What is the readinessProbe of this container?,"The container's readinessProbe is an HTTP GET request, the path is /health, the port is 8080, and the protocol is HTTP. initialDelaySeconds is 30s, timeoutSeconds is 5s, periodSeconds is 5s, successThreshold is 1, and failureThreshold is 3.",
What containers are described in this document?,"This document describes three containers: sidecar, metrics-exporter, and pt-heartbeat.",
What options does the resource type include?,Resource types include Pod and Service.,
What are the restrictions on policy names?,"The policy name can be up to 63 characters long, can only contain lowercase letters, numbers, and delimiters (""-""), and must start and end with lowercase letters or numbers.",
What parameters need to be filled in to create a custom indicator auto-scaling policy?,"Policy name, namespace, workload, resource type, metrics, data type.",
What are the steps to configure a workload auto-scaling policy?,"- Enter the cluster details page, click `Workload` in the left navigation bar to enter the workload list, click a load name to enter the `Workload Details` page. \n- Click the `Auto Scaling` tab to view the current cluster's elastic scaling configuration. \n- If the relevant plug-in is not installed or the plug-in is in an abnormal state, you will not be able to see the custom indicator auto-scaling entry. \n- Click the `New Scaling` button to create custom indicator elastic scaling strategy parameters.",
What are the prerequisites for configuring workload auto-scaling policies?,"Requires `NS Edit` or higher permissions, the metrics-server plug-in installation has been completed, the Insight plug-in and the Prometheus-adapter plug-in have been installed.",
How does HPA calculate the required number of scaling replicas if both built-in metrics and multiple custom metrics are used?,"If you use built-in indicators and multiple custom indicators at the same time, HPA will calculate the number of required scaling replicas based on multiple indicators, and take the larger value (but not exceeding the maximum number of replicas configured when setting the HPA policy) for auto scaling.",
"If I create an HPA policy based on CPU utilization, what are the prerequisites?","If you create an HPA policy based on CPU utilization, you need to set the configuration limit (Limit) for the workload in advance before configuring the workload, otherwise the CPU utilization cannot be calculated.",
"In DaoCloud Enterprise 5.0, which workloads support creating HPA?","In DaoCloud Enterprise 5.0, only Deployment and StatefulSet support creating HPA.",
What indicators can HPA be based on for elastic scaling?,"HPA can be elastically scaled based on CPU utilization, memory usage and custom indicators.",
What is HPA?,"HPA refers to Horizontal Pod Autoscaling, which is a function of DaoCloud Enterprise 5.0 that supports elastic scaling of Pod resources based on indicators.",
What are the three types of session persistence? How to enable session persistence?,"Session persistence is divided into three types: L4 source address hash, Cookie Key and L7 Header Name. To enable session persistence, you need to select the corresponding session persistence type in the forwarding policy and set it according to the corresponding rules.",
What are the two types of load balancers?,Load balancer types include platform-level load balancers and tenant-level load balancers.,
What is the default value for domain name?,The default value of domain name is the domain name of the cluster.,
What protocols can be selected to authorize inbound access to cluster services?,"You can choose between HTTP and HTTPS protocols to authorize inbound access to cluster services. HTTP does not require authentication, while HTTPS requires authentication to be configured.",
What are the required parameters that need to be entered when creating a route?,"Route name, namespace, protocol, domain name and secret key are required parameters.",
How to add labels and annotations to routes?,"When creating a route, you can choose to add labels or annotations. Labels and annotations are both key-value pairs, and you need to enter the key and value when adding them.",
How to create an HTTPS protocol route?,The following configuration is required when creating an HTTPS protocol route:\n- Route name\n- Namespace\n- Protocol\n- Domain name\n- Secret key\n- Forwarding policy (optional)\n- Load balancer type\ n- Ingress Class (optional)\n- Session persistence (optional),
How to create an HTTP protocol route?,The following configuration is required when creating an HTTP protocol route:\n-Route name\n-Namespace\n-Protocol\n-Domain name\n-Load balancer type\n-Ingress Class (optional)\n-Session persistence ( Optional)\n- Path rewriting (optional)\n- Redirect (optional)\n- Traffic distribution (optional)\n- Label (optional)\n- Annotation (optional),
What two protocol types of traffic can Ingress be used to control?,Ingress can be used to control traffic of HTTP and HTTPS protocol types.,
What is Ingress?,"Ingress is an API object that manages external access to services in a Kubernetes cluster, providing load balancing, SSL termination, and name-based virtual hosting.",
How to create tasks from YAML files?,"Click `Cluster List` on the left navigation bar, then click the name of the target cluster to enter the `Cluster Details` page. On the cluster details page, click `Workload` -> `Task` in the left navigation bar, and then click the `YAML Create` button in the upper right corner of the page. Enter or paste the prepared YAML file and click OK to complete the creation.",
How to configure data storage for tasks?,"In the container configuration, you can set parameters related to data volumes and data persistence. Just set the relevant path and mount point in the pop-up window.",
How to configure environment variables for tasks?,"In the container configuration, environment variables can be added. Click the Add button and enter the environment variable name and value in the pop-up window.",
How to configure resource limits for tasks?,"In the container configuration, you can set resource limits, including maximum values for CPU and memory. For example, if the maximum value of CPU is set to 250m, the container can only use the computing power of 0.25 CPUs at most.",
How to create a task?,"On the cluster details page, click `Workload` -> `Task` in the left navigation bar, and then click the `Create Task` button in the upper right corner of the page. In the pop-up window, enter or select the relevant parameters and click OK to complete the creation.",
What does advanced configuration include?,"Advanced configuration includes four parts: network configuration of the load, upgrade strategy, scheduling strategy, labels and annotations.",
How to create a service?,"Click the ""Create Service"" button and enter relevant configurations in the service parameter page. For details, please refer to ""Create Service"".",
What are the requirements for container names and container images?,"The container name can contain up to 63 characters and supports lowercase letters, numbers, and delimiters (""-""). It must start and end with lowercase letters or numbers. For container images, you need to enter the image address or name. When entering the image name, it defaults to the official one. DockerHub pulls the image.",
What are the parts of a container configuration?,"Container configuration is divided into six parts: basic information, life cycle, health check, environment variables, data storage, and security settings.",
What is the role of DaemonSet?,"The daemon ensures that a copy of a Pod runs on all or some nodes through node affinity and taint functions. When a new node joins the cluster, it automatically deploys the corresponding Pod on the new node and tracks the running status of the Pod. When a node is removed, the DaemonSet deletes all Pods it created. Common use cases include: running cluster daemons on each node, log collection daemons, monitoring daemons, etc.",
How to set the scaling time window in Kubernetes?,"When creating or editing a workload, you can set the value of the scaling time window. The scaling time window refers to the command execution time window before the load stops.",
What is a YAML example for creating a stateless payload?,```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nspec:\nselector:\nmatchLabels:\napp: nginx\nreplicas: 2 # Tell the Deployment to run 2 matching templates Pod\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80 \n```,
How to create a stateless payload using YAML files?,"1. On the cluster details page, select `Workload` -> `Stateless Load`, and then click the `YAML Create` button in the upper right corner of the page. \n2. Enter or paste the prepared YAML file and click `OK` to complete the creation.",
What scheduling strategies are available in Kubernetes?,#NAME?,
How to add tags and annotations?,You can click the `Add` button on the workload or container group details page to add labels and annotations to them.,
How to create a stateless payload?,"Stateless payloads can be created through images or YAML files. In the cluster details page, select `Workload` -> `Stateless Load`, and then click the `YAML Create` button in the upper right corner of the page to create it through a YAML file.",
How to configure the number of workload replicas?,"When creating or editing a workload, you can set the value for the number of workload replicas.",
What is the number of load copies?,The number of load copies refers to the number of Pod instances that the specified load needs to run.,
What is a container group?,container group is a group of containers running within a Pod that share the same network namespace and storage volume.,
What is load?,Loads refer to applications or services that are managed in Kubernetes and run within Pods.,
How to configure data storage?,"You can mount and persist data storage settings by adding volumes and volumeMounts objects to the Pod configuration. For details, please refer to [Container Data Storage Configuration] (pod-config/env-variables.md).",
What are container health checks? How to configure health check?,"Container health checks are used to determine the health status of containers and applications, helping to improve application availability. Health check configuration can be performed by adding livenessProbe and readinessProbe objects to the Pod's configuration. For details, please refer to [Container Health Check Configuration] (pod-config/health-check.md).",
What is container lifecycle configuration? How to configure container life cycle?,"Container life cycle configuration means that when the container is running, you can control the life cycle of the container by configuring the commands that need to be executed before, after, and before stopping. These commands can be specified in a Kubernetes cluster for each container in a Pod. For details, please refer to [Container Lifecycle Configuration] (pod-config/lifecycle.md).",
How to create a stateless workload through mirroring?,"Refer to the following steps to create a stateless load using mirroring:\n1. Click the cluster list on the left navigation bar, and then click the name of the target cluster to enter the cluster details page. \n2. On the cluster details page, click Workload -> Stateless Load in the left navigation bar, and then click the image creation button in the upper right corner of the page. \n3. After filling in the basic information, container configuration, service configuration, and advanced configuration in order, click OK in the lower right corner of the page to complete the creation.",
What prerequisites need to be met before using mirroring to create a stateless workload?,"Before using an image to create a stateless load, you need to meet the following prerequisites:\n- Connect to the Kubernetes cluster or create a Kubernetes cluster in the container management module, and be able to access the UI interface of the cluster. \n- Create a namespace and user. \n- The current operating user should have NS Edit or higher permissions. \n- When there are multiple containers in a single instance, please ensure that the ports used by the containers do not conflict, otherwise the deployment will fail.",
What is a stateless load (Deployment)?,"Stateless load (Deployment) is a resource in Kubernetes that provides declarative updates for Pods and ReplicaSets, and supports elastic scaling, rolling upgrades, version rollback and other functions. Declare the desired Pod state in the Deployment, and the Deployment Controller will modify the current state through ReplicaSet to make it reach the pre-declared desired state. Deployment is stateless and does not support data persistence. It is suitable for deploying stateless applications that do not need to save data and can be restarted and rolled back at any time.",
How to avoid private image warehouse certificate issues causing the container engine to deny access?,"When creating a cluster using a private image warehouse, you need to fill in the private image warehouse address in the advanced configuration and bypass the certificate authentication of the container engine in Insecure_registries to obtain the image.",
Will it take a long time to create a cluster? How to check the cluster creation status?,"Yes, creating a cluster takes a long time. You can click the `Return to cluster list` button to return to the cluster list page and wait for the cluster creation to be completed. To view the current status, click `Real-time Log`.",
How to create a working cluster?,"You can create a working cluster by following the steps below:\n1. In the `Cluster List` page, click the `Create Cluster` button. \n2. Fill in and confirm the basic information. \n3. Fill in and confirm the node configuration information. \n4. Fill in and confirm the network configuration information. \n5. Fill in and confirm the plug-in configuration information. \n6. Fill in and confirm the advanced configuration information, and click `OK`.",
What prerequisites need to be met before creating a working cluster?,"The following prerequisites need to be met before creating a working cluster:\n- Prepare a certain number of nodes according to business needs. \n- Kubernetes version 1.24.7 is recommended. For specific version range, please refer to DCE 5.0 cluster version support system. \n- The target host needs to allow IPv4 forwarding. If the Pod and Service use IPv6, the target server needs to allow IPv6 forwarding. \n- DCE does not currently provide firewall management functions. You need to define the target host firewall rules in advance. To avoid problems during cluster creation, it is recommended to disable the firewall of the target host. \n- See node availability check.",
What are the cluster roles in the DCE container management module?,"The cluster roles in the DCE container management module are divided into global service cluster, management cluster, working cluster and access cluster.",
"How to edit, update, export and delete configuration items?","Click More on the right side of the configuration item to edit YAML, update, export, delete and other operations.",
How to create configuration items? What two creation methods are supported?,"There are two creation methods: graphical form creation and YAML creation:\n- Graphical form creation: Select `Configuration and Key`->`Configuration Item`->Click the `Create Configuration Item` button in the upper right corner on the cluster details page , fill in the configuration information on the page and click OK. \n- YAML creation: On the cluster details page, select `Configuration and Key`->`Configuration Items`->Click the `YAML Create` button in the upper right corner, fill in or paste the prepared configuration file in the pop-up window, and click OK.",
Does the configuration item provide privacy encryption capabilities? What tools are recommended if you want to store encrypted data?,"Configuration items do not provide confidentiality or encryption functions. If you want to store encrypted data, it is recommended to use a secret or other third-party tools to ensure the privacy of the data.",
What is the size limit for data saved by configuration items? What should I do if I need to store larger data?,"The data size saved by the configuration item cannot exceed 1 MiB. If you need to store larger data, it is recommended to mount a storage volume or use an independent database or file service.",
What is the function of configuration items?,"Configuration items store non-confidential data in the form of key-value pairs, achieving the effect of decoupling configuration data and application code from each other. Configuration items can be used as environment variables for the container, command line parameters, or configuration files in the storage volume.",
What does the advanced configuration of scheduled tasks include?,The advanced configuration of scheduled tasks mainly involves labels and annotations. You can add labels and annotations to the Pod through the Add button.,
How to create a scheduled task?,"In addition to mirroring, you can also create scheduled tasks more quickly through YAML files. The specific operations are as follows:\n1. Enter the `Cluster Details` page of the target cluster;\n2. Click the `Workload` -> `Schedule Task` -> `YAML Create` button in the left navigation bar;\n3. Enter or paste For the YAML file prepared in advance, click `OK` to complete the creation.",
How to set timing rules? What custom cron expressions are supported?,"Timing rules can set execution time periods based on minutes, hours, days, weeks, and months, and support custom Cron expressions using numbers and `*`. After entering an expression, the meaning of the current expression will be prompted below. For detailed expression syntax rules, please refer to [Cron schedule syntax](https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/cron-jobs/#cron-schedule-syntax).",
What are the concurrency strategies for scheduled tasks? What's the difference?,"There are three concurrency strategies for scheduled tasks: `Allow`, `Forbid`, and `Replace`. Their differences are as follows: \n- `Allow`: New scheduled tasks can be created when the previous task is not completed, and multiple tasks can be run in parallel. However, too many tasks may occupy cluster resources. \n- `Forbid`: New tasks cannot be created until the previous task is completed. If the execution time of the new task arrives and the previous task has not yet been completed, CronJob will ignore the execution of the new task. \n- `Replace`: If the execution time of the new task arrives but the previous task has not been completed, the new task will replace the previous task.",
What are the advantages of creating scheduled tasks with YAML?,Create scheduled tasks more quickly through YAML files.,
What are the steps to create a scheduled task for mirroring?,"The steps are as follows:\n1. Enter the cluster details page of the target cluster. \n2. On the cluster details page, click Workload -> Scheduled Tasks in the left navigation bar. Then click the image creation button in the upper right corner of the page. \n3. After filling in the basic information, container configuration, scheduled task configuration, and advanced configuration in order, click OK in the lower right corner of the page to complete the creation.",
What prerequisites need to be met to create a scheduled task (CronJob)?,"Before creating a scheduled task (CronJob), you need to meet the following prerequisites: access the Kubernetes cluster or create a Kubernetes cluster in the container management module, and be able to access the UI interface of the cluster; create a namespace and user; the current operating user should have NS Edit or higher.",
What scenarios are scheduled tasks suitable for?,"Scheduled tasks are suitable for performing periodic operations, such as backup, report generation, etc.",
What's included in the YAML example for a daemon?,"The YAML example of the daemon process includes DaemonSet, metadata, spec, selector, template, volumes, containers, command, args, env, etc., used to create the configuration of the daemon process.",
What is an example YAML for a daemon?,"```yaml\nkind: DaemonSet\napiVersion: apps/v1\nmetadata:\nname: hwameistor-local-disk-manager\nnamespace: hwameistor\nuid: ccbdc098-7de3-4a8a-96dd-d1cee159c92b\nresourceVersion: ""90999552""\ ngeneration: 1\ncreationTimestamp: ""2022-12-15T09:03:44Z""\nlabels:\napp.kubernetes.io/managed-by: Helm\nannotations:\ndeprecated.daemonset.template.generation: ""1""\nmeta. helm.sh/release-name: hwameistor\nmeta.helm.sh/release-namespace: hwameistor\nspec:\nselector:\nmatchLabels:\napp: hwameistor-local-disk-manager\ntemplate:\nmetadata:\ncreationTimestamp: null \nlabels:\napp: hwameistor-local-disk-manager\nspec:\nvolumes:\n- name: udev\nhostPath:\npath: /run/udev\ntype: Directory\n- name: procmount\nhostPath:\npath : /proc \ntype : Directory \n- name : devmount \nhostPath : \npath : /dev \ntype : Directory \n- name : socket-dir \nhostPath : \npath : /var/lib/kubelet/plugins/disk. hwameistor.io \ntype : DirectoryOrCreate \n- name : registration-dir \nhostPath : \npath : /var/lib/kubelet/plugins_registry/ \ntype : Directory \n- name: plugin-dir\nhostPath:\npath: /var /lib/kubelet/plugins\ntype: DirectoryOrCreate\n- name: pods-mount-dir\nhostPath:\npath: /var/lib/kubelet/pods\ntype: DirectoryOrCreate\ncontainers:\n- name: registrar\nimage: k8s-gcr.m.daocloud.io/sig-storage/csi-node-driver-registrar:v2.5.0\nargs:\n- ""--v=5""\n- ""--csi-address=/csi /csi.sock""\n- >-\n--kubelet-registration-path=/var/lib/kubelet/plugins/disk.hwameistor.io/csi.sock\nenv:\n- name: KUBE_NODE_NAME\nvalueFrom: \nfieldRef:\napiVersion: v1\nfieldPath: spec.nodeName\nresources: {}\nvolumeMounts:\n- name: socket-dir\nmountPath: /csi \n- name : registration-dir \nmountPath : /registration \nlifecycle: \npreStop:\nexec:\ncommand: [""/bin/sh"",""-c"",""rm -rf /registration/disk.hwameistor.io/*""]\nterminationMessagePath: /dev/termination-log\nterminationMessagePolicy: File \nimagePullPolicy : IfNotPresent \n- name : manager \nimagePullPolicy : IfNotPresent \nimage : ghcr.m.daocloud.io/hwameistor/local-disk-manager:v0.6.1 \ncommand :\n- /local-disk-manager\ nargs:\n- ""--endpoint=$(CSI_ENDPOINT)""\n- ""--nodeid=$(NODENAME)""\n- ""--csi-enable=true""\nenv:\n# CSI_ENDPOINT is the endpoint for the driver. It""sa unix socket\n# mounted by Kubelet, and only visible inside the container.\n# It should be used to make calls to the driver.\n- name: CSI_ENDPOINT\nvalue: unix:// var/lib/kubelet/plugins/disk.hwameistor.io/csi.sock\n# NAMESPACE is the Pod""s namespace.\n- name: NAMESPACE\nvalueFrom:\nfieldRef:\napiVersion: v1\nfieldPath: metadata.namespace \n# WATCH_NAMESPACE and POD_NAME are used by some drivers to construct a unique Pod identity. They can be ignored in this example.\n- name: WATCH_NAMESPACE\nvalueFrom:\nfieldRef:\napiVersion: v1\nfieldPath: metadata.namespace\n - name: POD_NAME \nvalueFrom:\nfieldRef:\napiVersion : v1 \nfieldPath : metadata.name \n# NODENAME is a required env variable for all CSI drivers. It""s used to identify this node when making CSI calls.\n- name: NODENAME\nvalueFrom:\nfieldRef:\napiVersion: v1\nfieldPath: spec.nodeName \n# OPERATOR_NAME is an optional env variable for all CSI drivers which can be used to customize logging, error messages, and metrics.\n- name : OPERATOR_NAME \nvalue : local-disk-manager \nresources : {} \nvolumeMounts:\n- name : udev \nmountPath : /run/udev \n- name : procmount \nreadOnly : true \nmountPath : /host/proc \n - name : devmount \nmountPath : /dev \n- name : registration-dir \nmountPath : /var/lib/kubelet/plugins_registry \n- name: plugin-dir\nmountPath: /var/lib/kubelet/plugins\nmountPropagation: Bidirectional\n- name: pods-mount-dir\nmountPath: /var/lib/kubelet/pods\nmountPropagation: Bidirectional\nterminationMessagePath: /dev/termination-log\nterminationMessagePolicy: File \nsecurityContext:\nprivileged:true \nrestartPolicy: Always \nterminationGracePeriodSeconds : 30 \ndnsPolicy : ClusterFirst \nserviceAccountName : hwameistor-admin \nserviceAccount : hwameistor-admin \nhostNetwork:true \nhostPID:true \nsecurityContext:{} \nschedulerName : default-scheduler \n```",
How to create a daemon process?,"On the cluster details page, click `Workload` -> `Daemon` in the left navigation bar, then click the `YAML Create` button in the upper right corner of the page, enter or paste the prepared YAML file, and click `OK` Complete creation.",
How is cluster deletion protection set up?,"In the cluster settings, click the name of the target cluster, click `Recent Operations`->`Cluster Settings`->`Addon Plug-in` in the left navigation bar, and find the configuration item whether to enable cluster deletion protection. You can configure whether to enable this feature.",
What does cluster deletion protection mean?,"Cluster deletion protection is a security feature. When enabled, it means that the cluster cannot be uninstalled directly. To uninstall the cluster, you need to disable deletion protection first. You can configure whether to enable this feature in the cluster settings.",
How to set the Helm warehouse refresh cycle and the number of operation records retained?,"In the cluster settings, click the name of the target cluster, click `Recent Operations`->`Cluster Settings`->`Addon Plug-in` in the left navigation bar, find Helm-related configuration items there, and you can perform Helm operation base mirroring, Warehouse refresh cycle, number of operation records retained and other related configurations.",
How to enable GPU on cluster?,"In the cluster settings, enabling the GPU requires pre-installation of the GPU card and corresponding driver plug-in on the cluster. Click the name of the target cluster, click `Recent Operations`->`Cluster Settings`->`Addon Plug-in` in the left navigation bar, and then check Enable GPU.",
What advanced features can be customized with cluster settings?,"Cluster settings are used to customize advanced feature settings, including whether to enable GPU, Helm warehouse refresh cycle, Helm operation record retention, whether to enable cluster deletion protection, etc.",
How to view cluster roles in the container management module?,See the cluster-role.md document for more information about cluster roles.,
Under what circumstances does the displayed cluster data not represent the real data?,"When the cluster is in an unknown state, the displayed data is the cached data before the connection was lost and does not represent the real data. At the same time, any operations performed while disconnected will not take effect. Please check the cluster network connectivity or host status.",
What are the statuses of the access cluster?,"Connecting, Unconnecting, Running and Unknown. Each status has a corresponding description.",
What are the statuses of a self-built cluster?,"Creating, updating, deleting, running, unknown and creation failed. Each status has a corresponding description.",
What two types of clusters does the container management module support?,Access clusters and self-built clusters.,
What is the relationship between version ranges and community versions for using the interface to create worker clusters in DCE 5?,"The relationship between the version range of the working cluster created using the interface and the community version in DCE 5 is always one version lower than the community version, and a stable version will be recommended to users while maintaining a high degree of synchronization with the community.",
Will the version range of the interface used to create worker clusters in DCE 5 be highly synchronized with the community?,"Yes, the version range of the working cluster created using the interface in DCE 5 is highly synchronized with the community, and when the community version is incremented, the version range of the working cluster created using the interface in DCE 5 will also be incremented by one version simultaneously.",
What is the recommended stable version for using the interface to create worker clusters in DCE 5?,The recommended stable version for using the interface to create worker clusters in DCE 5 is 1.24.7.,
"How many Kubernetes version ranges does the community support? What happens when the community releases a new version, increasing the supported version range?","The community supports three Kubernetes version ranges, such as 1.24, 1.25, and 1.26. When the community releases a new version, the community-supported version range will be incremented.",
What kind of version support mechanism is adopted in DCE 5 for accessing clusters and self-built work clusters using the interface?,"DCE 5 adopts different version support mechanisms for access clusters and self-built working clusters using the interface. The version range of the self-built working cluster using the interface is always one version lower than the community version, and a stable version will be recommended to users while maintaining a high degree of synchronization with the community.",
"What are environment variables, and what are their functions in Kubernetes?","An environment variable refers to a variable set in the container running environment. You can set no more than 30 environment variables when creating a container template. Environment variables can be modified after the workload is deployed, providing great flexibility for the workload.",
What is resource limit Limit Range?,"Limit Range is used to add a resource limit to the namespace, including minimum, maximum and default resources. When the container group is created, resource allocation using the limits parameter is enforced.",
What is Resource Quota?,Resource quota (Resource,
What is PodAntiAffinity?,Container group anti-affinity PodAntiAffinity specifies that workloads are deployed on different nodes. Multiple instances of the same workload are deployed with anti-affinity to reduce the impact of downtime; applications that interfere with each other are deployed with anti-affinity to avoid interference.,
What is container group load affinity PodAffinity?,"Container group load affinity PodAffinity specifies that the workload is deployed on the same node. Users can deploy workloads nearby according to business needs, and route communication between containers nearby to reduce network consumption.",
What is NodeAntiAffinity?,Node Anti-Affinity NodeAntiAffinity can restrict container groups from being scheduled to specific nodes by selecting labels.,
What is node affinityNodeAffinity?,Node affinity NodeAffinity allows you to restrict container groups from being scheduled to specific nodes by selecting labels.,
"What are affinity and anti-affinity, and what are their functions?","Affinity and anti-affinity are used to constrain the scheduling of Pods or container groups. Affinity can enable nearby deployment, enhance network capabilities, implement nearby routing of communications, and reduce network losses; anti-affinity is mainly due to high reliability considerations, and instances should be dispersed as much as possible. When a node fails, the application will be affected. The impact is only one-Nth or just one instance.",
What is HPA and what does it do in Kubernetes?,Horizontal Pod Autoscaling (HPA) is a function in Kubernetes that implements horizontal automatic scaling of Pods. Kubernetes clusters can expand or shrink services through the scaling mechanism of Replication Controller to achieve scalable services.,
What is PersistentVolumeClaim? How does it claim storage resources?,"PersistentVolumeClaim (PVC) is a claim request for PV. PVC is similar to Pod: Pod consumes Node resources, while PVC consumes PV resources; Pod can request CPU and memory resources, while PVC requests data volumes of specific size and access mode.",
What is PersistentVolume and how does it provide persistent volumes?,"PersistentVolume (PV) is a storage resource that provides network storage resources. PV can be NFS, iSCSI, GlusterFS, Ceph, RBD and other types of volumes.",
What are PersistentVolume and PersistentVolumeClaim?,"PersistentVolume (PV) and PersistentVolumeClaim (PVC) provide convenient persistent volumes: PV provides network storage resources, while PVC claims storage resources.",
What is annotation?,"Annotation can associate Kubernetes resource objects to arbitrary non-identifying metadata, which can be retrieved through annotations. Annotation is similar to Label and is also defined in the form of Key/Value pairs.",
What is LabelSelector?,"Label Selector is the core grouping mechanism of Kubernetes. Through Label Selector, the client/user can identify a group of resource objects with common characteristics or attributes.",
What are labels and what do they do?,"label is actually a key/value pair, which is associated with an object, such as a Pod. The use of labels tends to indicate the characteristics of the object and is meaningful to the user. Labels can be specified when the object is created or after the object is created.",
What is Secret and what does it do?,"Secret is similar to Configmap, but is used to save configuration information of confidential data (such as passwords, tokens, keys, etc.). Secret decouples sensitive information from container images, eliminating the need to include confidential data in application code.",
What is ConfigMap and what does it do?,"ConfigMap is used to save configuration non-confidential data into key-value pairs. When used, the container group can use it as an environment variable, a command line parameter, or a configuration file in a storage volume. ConfigMap decouples your environment configuration information from container images, making it easy to modify application configurations.",
Which directory on the local node does "kubectl" search for a file named "config" by default?,The "kubectl" tool will by default look for a file named "config" from the "$HOME/.kube" directory of the local node.,
How to access the cluster through kubectl?,"The following conditions need to be met: the local node and the cloud cluster are interconnected, the cluster certificate has been downloaded to the local node, and the kubectl tool has been installed on the local node. After downloading the certificate, copy the certificate content to the config file of the local node and execute the command ""kubectl get pod -n default"" to verify the connectivity.",
How to access the cluster through CloudShell?,"On the cluster list page, select the cluster that needs to be accessed through CloudShell, click the ""..."" operation icon on the right and click ""Console"" in the drop-down list. Execute the ""kubectl get node"" command on the CloudShell console to verify connectivity. .",
What permissions are required to access the cluster?,The user should have Cluster Admin permissions or higher.,
In what ways can I access the clusters connected or created by the DCE platform?,"It can be accessed directly through the UI interface, online through CloudShell, or accessed through kubectl after downloading the cluster certificate.",
How to expand node capacity?,"After entering a cluster details page that supports node expansion and contraction, click `Node Management` in the left navigation bar, and then click `Access Node` in the upper right corner of the page. Enter the host name and IP to be added and confirm.",
How to determine whether a cluster supports node expansion and contraction?,"After entering the details page of a cluster, if the cluster role has the label of `access cluster`, it means that the cluster does not support node expansion and contraction.",
How to enter the cluster list page?,"In the container management module, click `Cluster List` in the left navigation bar to enter the cluster list page.",
What is cluster node expansion?,Cluster node expansion refers to expanding the cluster node capacity based on KubeSpray to alleviate resource pressure. Only clusters created through the container management module support node expansion and contraction. Clusters connected from the outside do not support this operation.,
How does the container management module ensure safety and reliability?,"The container management module defaults to high-availability deployment mode to ensure high business availability. At the same time, it supports the deployment of different container clusters across regions to achieve high availability of cross-regional applications, and integrates the Kubernetes RBAC permission system to provide a complete user permission system.",
How does the container management module implement unified distribution of policies?,"The container management module supports the formulation of network policies, quota policies, resource restriction policies, disaster recovery strategies, security policies and other policies at the granularity of namespace or cluster, and supports policy distribution at the granularity of cluster/namespace.",
How does the container management module make applications production-ready?,"The container management module can distribute applications through images, YAML, and Helm, and achieve unified management across clouds and clusters. At the same time, it supports distributed deployment of applications, supports automatic switching of traffic at single points of failure, and provides rich monitoring indicators to achieve all-round monitoring of applications and provide early warning of application traffic peaks and application failures.",
How does the container management module realize unified management of the cluster?,"The container management module supports the unified management of different clusters, supports any Kubernetes cluster within a specific version range to be included in the scope of container management, and realizes management capabilities such as cluster creation and cluster node expansion and contraction through the Web interface, and one-click operation through the Web interface Complete the smooth upgrade of the Kubernetes cluster. At the same time, the one-click certificate rolling function can realize certificate rolling in self-built clusters.",
What are the advantages of the container management module?,The container management module has the following advantages:\n- Unified management of clusters\n- Application production ready\n- Unified distribution of policies\n- Safe and reliable\n- Heterogeneous compatibility\n- Open and compatible,
How to add namespace permissions?,"After the user logs in to the platform, click Permission Management under Container Management on the left menu bar, select the Namespace Permissions tab, click the Add Authorization button, and select the target cluster, target namespace, and user to be authorized on the Add Namespace Permissions page. After entering the user group, click OK.",
How to add cluster permissions?,"After the user logs in to the platform, click Permission Management under Container Management on the left menu bar, select the Cluster Permissions tab, click the Add Authorization button, select the target cluster and the user/user group to be authorized on the Add Cluster Permissions page, and click OK. Can.",
Which roles have cluster permissions and namespace permissions?,"Currently, the only supported cluster role is Cluster Admin, and the currently supported namespace roles are NS Admin, NS Edit, and NS View.",
How to prepare the prerequisites before authorizing users/user groups?,"Preparatory work includes: Users/user groups to be authorized have been created in global management. Only Kpanda Owner and the Cluster Admin of the current cluster have cluster authorization capabilities. Only Kpanda Owner, the Cluster Admin of the current cluster and the NS Admin of the current namespace have the capability. Namespace authorization capabilities.",
What are cluster permissions?,"Cluster permissions are a permission granted by container management based on global permission management and global user/user group management. Authorized users can create, manage, and delete all clusters.",
What types of standard K8s clusters are mainly used to connect to the access cluster?,"Access clusters are mainly used to access existing standard K8s clusters, including but not limited to self-built clusters in local data centers, clusters provided by public cloud vendors, clusters provided by private cloud vendors, edge clusters, Xinchuang clusters, and heterogeneous clusters. , DaoCloud different distribution clusters, etc.",
What type of load is a worker cluster primarily used to carry?,Work clusters are mainly used to carry business loads.,
What type of cluster is the management cluster generally used to manage?,The management cluster is generally used to manage working clusters and does not carry business loads.,
What components are generally used to run global service clusters?,"The global service cluster is generally used to run DaoCloud Enterprise 5.0 components, such as container management, global management, observability, mirror warehouse, etc., and does not carry business load.",
On what basis does DaoCloud Enterprise 5.0 classify cluster roles?,DaoCloud Enterprise 5.0 classifies the roles of clusters based on their different functional positions.,
Does the Kafka module support multi-language clients?,"Yes, the Kafka module supports multi-language clients.",
What reliability guarantee mechanisms does Kafka module have?,The Kafka module supports a reliability guarantee mechanism to ensure that messages will not be lost or duplicated during transmission. It also supports custom parameter configuration and cluster high availability.,
What operations does the Kafka module support?,Currently supported operations include creating/updating/deleting Kafka instances.,
What features does Kafka have?,"Kafka is a distributed message flow processing middleware with high throughput, durability, horizontal scalability, and support for streaming data processing. It adopts a distributed message publishing and subscription mechanism to perform log collection and streaming data transmission. , online/offline system analysis, real-time monitoring and other fields have a wide range of applications.",
What is a Kafka module?,The Kafka module is a distributed message queue service based on the open source software Kafka. DaoCloud has developed a simple and easy-to-use graphical interface for it.,
How to ensure the security of MySQL database?,Various measures can be taken to ensure that MyS,
What are the common application scenarios for MySQL?,MyS,
How to create a MySQL instance?,You can refer to Create MyS,
What protocol is MySQL based on?,MyS,
What is MySQL? What are the features?,MyS,
What functions can ElasticSearch achieve?,"ElasticSearch can provide distributed search services, providing users with structured and unstructured text and multi-condition retrieval, statistics, and reports based on AI vectors. Fully compatible with Elasticsearch native interface. It can help websites and APPs build search boxes to improve users’ search experience; it can also be used to build log analysis platforms to help enterprises achieve data-driven maintenance and operations; its vector retrieval capabilities can help customers quickly build AI-based image search , recommendation, semantic search, face recognition and other rich applications.",
What is Elasticsearch?,"Elasticsearch is a full-text search engine that can quickly store, search, and analyze massive amounts of data. Its bottom layer is the open source library Lucene, but Lucene cannot be used directly. You must write your own code to call its interface. Elasticsearch is a package of Lucene and provides a REST API operation interface that can be used out of the box. The built-in search service of DCE 5.0 is also based on Elasticsearch.",
What convenient management methods does the Redis cache service provide?,"The Redis cache service provides a visual web management interface to complete operations such as instance restart and parameter modification online. In addition, a RESTful API is provided to facilitate users to further realize automated management of instances.",
How does the Redis cache service achieve elastic scaling?,"The Redis cache service provides online expansion and reduction services for instance memory specifications, helping users achieve cost control based on actual business volume and achieve on-demand usage.",
How to ensure the security and reliability of Redis instance data?,"With the help of security management services such as DaoCloud global management and audit logs, the storage and access of instance data are fully protected. In addition, DaoCloud also provides flexible disaster recovery strategies. The active and standby/cluster instances can be deployed in a single cluster to support multi-cluster and multi-cloud deployment.",
How to create a Redis instance?,"It can be created directly through the UI console without separately preparing server resources. All Redis versions are deployed in containers and can be created in seconds. For details, please refer to [Creating a Redis instance](../user-guide/create.md).",
What memory specifications does DaoCloud's Redis cache service support?,DaoCloud's Redis cache service supports rich memory specifications from 128M to 1024G.,
What types of cache instances does Redis Cache Service provide?,"The Redis cache service provides stand-alone, high-availability cluster, Cluster cluster, and read-write separation type cache instances.",
What is Redis cache service?,"Redis cache service is an in-memory database cache service provided by DaoCloud. It is compatible with two in-memory database engines, Redis and Memcached, and provides users with online distributed caching capabilities to meet the business needs of high concurrency and fast data access.",
What is Redis?,"Redis is an in-memory database and an open source, high-performance key-value storage database. It supports a variety of data structures and is highly available and scalable.",
What is MinIO?,"MinIO is a very popular lightweight object storage solution. It is a high-performance, distributed object storage system with an Amazon S3 compatible interface and a POSIX file system interface.",
 In what scenarios is Kafka commonly used?,Kafka is commonly used as a data pipeline for message transmission. It can be used to build real-time streaming data processing applications and streaming computing.,
What is Elasticsearch?,"Elasticsearch is a full-text search engine and an open source distributed search engine that can quickly store, search and analyze large amounts of text data. It supports RESTful API and clients in almost all programming languages.",
What are the data service middlewares?,"Data service middleware includes: Elasticsearch, Kafka, MinIO, MyS",
"How to check the access settings, resource quotas, Pod list and other information of a RabbitMQ message queue?","1. On the Message Queue page, click a name. \n2. Enter the message queue overview to view information such as access settings, resource quotas, and Pod lists. \nFor detailed steps, please see the above content and corresponding screenshots.",
How to delete an Elasticsearch instance?,"You can click the ""Delete"" button of the corresponding instance and confirm the deletion to delete the instance. It should be noted that the deletion operation is irreversible, so please operate with caution.",
How to rename an Elasticsearch instance?,You can click the "Rename" button of the corresponding instance and enter a new name to complete the renaming operation.,
How to view details of an Elasticsearch instance?,"You can click the ""View Details"" button of the corresponding instance to view detailed information in the pop-up window, including IP address, port, resource usage, etc.",
What information does an Elasticsearch list have?,"The Elasticsearch list contains the following information:\n- Name\n- Version number\n- Number of instances\n- Status\n- Creation time\n- Action buttons (view details, rename, delete).",
What does this page show?,This page shows the Elasticsearch list.,
How to create a RabbitMQ instance?,You can refer to the corresponding documentation or official website to create it. The link is as follows: [Create RabbitM,
What can MinIO be run as?,MinIO can run as lightweight containers and be managed by external orchestration services such as Kubernetes.,
What object storage protocols does MinIO support?,MinIO is perfectly compatible with the AWS S3 protocol.,
What are the features of MinIO?,"High performance, with read/write speeds up to hundreds of GB per second;\n- Scalability, supporting a simple scaling model;\n- Cloud native support, in line with the architecture and construction process of cloud native computing, and including the latest Cloud computing technology and concepts;\n- Pure open source, 100% open source based on Apache V2 license;\n- Compatible with S3 storage;\n- Minimalism, minimalism is the guiding design principle of MinIO;\n- Support partly cloudy.",
What is MinIO?,"MinIO is a popular, lightweight, open source object storage solution that is perfectly compatible with the AWS S3 protocol and friendly supports K8s.",
How to check the update results after the update is successful?,"After the update is successful, you can see the update result prompt in the upper right corner.",
How to enter the PostgreSQL instance list?,"After logging in to your account in Daocloud, select `Console` -> `Cloud Database PostgreS",
What PostgreSQL resource configurations can be modified?,"On the update instance page, you can modify the description, specification configuration, service settings, etc.",
How to update or modify PostgreSQL resource configuration?,"In the instance list, click the `...` button on the right, select `Update Instance` in the pop-up menu, and then follow the page prompts, including modifying the description, specification configuration, service settings, etc. After the update is successful, you can see the update result prompt in the upper right corner.",
Will updating the Redis instance affect existing data?,"When updating a Redis instance, operations such as data migration and network switching may be involved, which may result in data being inaccessible for a certain period of time. Please make sure you have backed up your data and understand the risks before proceeding.",
How to modify the description information of a Redis instance?,"When updating a Redis instance, you can modify the instance description information on the basic information page.",
What should I pay attention to when updating the specifications and configuration of a Redis instance?,Updating specifications and configurations may result in data loss of the instance. Please make sure you have backed up the data and understand the risks before proceeding.,
What can be modified when updating a Redis instance?,"You can modify the instance description, specification configuration, service settings, etc.",
How to update the resource configuration of a Redis instance?,"You can click the `...` button on the right in the instance list, select `Update Instance` in the pop-up menu, and then follow the page prompts to modify the basic information, specification configuration and service settings to update the resource configuration of the Redis instance. .",
Which cloud platform does the MinIO authorization policy file have the same pattern as?,The pattern of the MinIO authorization policy file is the same as the Amazon Cloud IAM Policy.,
What strategies are built into MinIO?,"MinIO has four built-in policies that can be assigned directly to users or user groups: readonly, readwrite, diagnostics and writeonly.",
What is the role of user groups in MinIO?,"User groups can manage the permissions of a group of users in batches. Resource permissions can be assigned to user groups through authorization policies, and users in this group will inherit the resource permissions of the user group.",
How to create a service account?,Service accounts can be created through the Web Console and mc command line.,
How to create a normal user?,"Ordinary users can be created through the Web Console, mc command line, and Operator CR.",
What information does a user in MinIO consist of?,A user consists of a pair of access key and secret key.,
What identity management methods does MinIO support?,"By default, MinIO uses the built-in IDentity Provider (IDP) to complete identity management. In addition to IDP, third-party OIDC and LDAP methods are also supported.",
What information can be viewed on the Kafka message queue overview page?,"The Kafka message queue overview page allows you to view basic information, access settings, resource quotas, Pod lists and other information.",
How to view Kafka message queue?,"On the message queue page, click the corresponding name to enter the message queue overview and view basic information, access settings, resource quotas, Pod list and other information.",
What operations are supported on the instance details page?,#NAME?,
How to view instance details in the MinIO instance list?,#NAME?,
"How to implement queries such as ""latest comments"" in social apps, reduce database pressure and improve app response speed?","You can use Redis's List (linked list) to store the latest comments. When the number of requested comments is within this range, there is no need to access the disk database and is returned directly from the cache, which can reduce database pressure and improve the response speed of the App.",
"In online games, how to display the ranking list of users with the highest scores or combat power in real time?",You can use Redis's ordered collection to store user rankings. It is very simple to use and provides up to 20 commands for operating collections.,
"Which data structure of Redis is suitable for storing information such as the online user list, gift rankings, and barrage messages in the live broadcast room?","The SortedSet structure in Redis is suitable for storing information such as online user lists, gift rankings, and barrage messages in live broadcast rooms.",
What are the advantages of Redis over traditional disk databases?,"Compared with traditional disk databases, Redis has the advantages of fast access speed, reducing database disk IO, improving data query efficiency, and reducing database storage costs.",
What scenarios is Redis suitable for?,"Redis is suitable for large-scale data access, requires high data query efficiency, simple data structure, and does not involve too many related query scenarios, such as flash sales on e-commerce websites, message barrages for live video broadcasts, game rankings for game applications, and social networking App’s latest comments/replies, etc.",
How to modify the service settings of a MySQL instance?,"You can follow the following steps:\n1. In the instance list, click the `...` button on the right and select `Update Instance` in the pop-up menu. \n2. After modifying the service configuration items that need to be modified on the service settings page, click Confirm to update.",
How to modify the specifications and configuration of a MySQL instance?,"You can follow the following steps:\n1. In the instance list, click the `...` button on the right and select `Update Instance` in the pop-up menu. \n2. Select the resource configuration items that need to be modified on the specification configuration page and make adjustments, then click Next.",
How to modify the description of a MySQL instance?,"You can follow the following steps:\n1. In the instance list, click the `...` button on the right and select `Update Instance` in the pop-up menu. \n2. After modifying the description information on the basic information page, click Next.",
What information can be modified during the MySQL instance update process?,In MyS,
How to update the resource configuration of a MySQL instance?,"You can follow the following steps:\n1. In the instance list, click the `...` button on the right and select `Update Instance` in the pop-up menu. \n2. In the pop-up update instance page, select the basic information, specification configuration and service settings to modify, and then click Confirm to update.",
How to confirm whether the update is successful after updating Kafka?,"On the message queue page, a message will be displayed in the upper right corner of the screen: `Update instance successful`.",
How to update Kafka's resource configuration?,"1. In the message queue, click the `...` button on the right and select `Update Instance` in the pop-up menu. \n2. After modifying the basic information, specification configuration, and service settings, click `Confirm` to complete the update. \n(See the picture shown above for specific operation steps)",
How do I check the success status of an updated instance?,Return to the instance list and the message: `Update instance successful` will be displayed in the upper right corner of the screen.,
Which configurations of MinIO can be modified?,"MinIO's basic information, CPU quota, and memory quota can be modified. However, the version, deployment mode, storage class, and capacity cannot be modified.",
How to update or modify MinIO's resource configuration?,"1. In the instance list, click the `...` button on the right and select `Update Instance` in the pop-up menu. \n2. Modify the basic information and click `Next`. \n-Only modification of description information is supported\n-The instance name and deployment location cannot be modified\n3. Modify the specifications and configuration, and then click `Next`. \n-Only supports modification: CPU quota and memory quota\n-Cannot modify: version, deployment mode, storage class, capacity\n4. Modify the service settings, and then click `Confirm`. \n5. Return to the instance list, and a message will be displayed in the upper right corner of the screen: `Update instance successful`.",
What are the specific steps to update an instance of RabbitMQ?,The specific steps are as follows:\n1. Open RabbitM,
What kind of prompt message will appear after RabbitMQ successfully updates the instance?,"Return to the message queue, and the message: `Update instance successful` will be displayed in the upper right corner of the screen.",
How to open the update instance operation interface of RabbitMQ?,"In the message queue, click the `...` button on the right and select `Update Instance` in the pop-up menu.",
How to update RabbitMQ's resource configuration?,"You can follow the following steps:\n1. In the message queue, click the `...` button on the right and select `Update Instance` in the pop-up menu. \n2. After modifying the basic information, click `Next`. \n3. After modifying the specification configuration, click `Next`. \n4. After modifying the service settings, click `Confirm`. \n5. Return to the message queue, and the message: `Update instance successful` will be displayed in the upper right corner of the screen.",
What new API functions and documentation are added in v0.2.2?,"In the v0.2.2 version, there is a new interface for obtaining user lists, support for arm architecture, full life cycle management of redis instances, monitoring and deployment of redis instances, support for redis sentinel, one-click deployment of singletons and clusters, support for ws permission isolation and Online dynamic expansion and other API functions, and the release notes script has been upgraded.",
What API issues have been fixed and what new features have been added in v0.2.6?,"The v0.2.6 version fixes API issues such as errors in verifying some fields when updating Redis, adjusting password verification to MCamel low password strength, version upgrades that rely on sentinel mode, and service address display errors, and adds a new master and backup installation for ARM environment. Mode Redis cluster, public fields when returning lists or details, adding functions such as returning alarm lists and verifying Service annotations.",
What new API functions and documents are added in v0.5.0?,"In the v0.5.0 version, API functions such as Helm-docs template files, Operators in the app store can only be installed on mcamel-system, cloud shell support, separate registration of the navigation bar, and log viewing have been added; offline mirror detection has also been upgraded Script, and add log viewing operation instructions.",
What new features and documentation are added in v0.6.2?,"In the v0.6.2 version, an automated backup and recovery function has been added; the offline mirroring problem of exporting backup and recovery has been fixed, and several known issues have been fixed; a backup function usage document has been added.",
What new features are added in v0.7.1?,"In the v0.7.1 version, a new detail page is added to display related events, and the list interface supports Cluster and Namespace field filtering and custom role functions.",
What is the Redis cache service Release Notes used for?,Redis Cache Service Release Notes are documents used to introduce the update content and feature changes of different versions of Redis Cache Service.,
Why do we need to use RabbitMQ to achieve traffic peak reduction during flash sale activities?,"Since a large number of requests will be generated in a short period of time during the flash sale event, use RabbitM",
How does RabbitMQ implement traffic peak shaving?,RabbitM,
What is the role of RabbitMQ in application decoupling?,"In application decoupling, RabbitM",
What is the role of RabbitMQ in asynchronous processing?,"In asynchronous processing, RabbitM",
What scenarios are RabbitMQ suitable for?,RabbitM,
How does Elasticsearch support business analytics?,Elasticsearch can help users better understand business conditions and make more accurate decisions by aggregating and analyzing large amounts of business data and displaying it visually.,
How does Elasticsearch support application performance monitoring?,"Elasticsearch can help users understand the running status of the application in real time and perform optimization by monitoring the logs, indicators and other information generated by the application.",
How does Elasticsearch support geospatial data analysis and visualization?,"Elasticsearch supports geospatial data indexing and querying, and supports map-based visualization, which has advantages in geospatial data analysis and visualization.",
How does Elasticsearch support security analytics?,"Elasticsearch supports security policies and access control, which can help users ensure data security. At the same time, it supports real-time data aggregation and visualization, which can help users better discover potential security issues.",
What are the advantages of Elasticsearch in log processing and analysis?,"Elasticsearch can quickly process large amounts of log data and supports real-time data aggregation and visualization, which can help users better understand the operating status of their systems.",
What is enterprise search?,Enterprise search refers to retrieving various information within the enterprise through full-text search technology so that employees can quickly and accurately find the information they need.,
What is App Search?,Application search refers to the retrieval of data in applications through full-text search technology so that users can quickly and accurately find the information they need.,
What types of content can Elasticsearch index?,Elasticsearch is capable of indexing many types of content.,
What does Elasticsearch do well?,Elasticsearch excels in speed and scalability.,
What scenarios can Elasticsearch be used for?,"Elasticsearch can be used in scenarios such as application search, website search, enterprise search, log processing and analysis, infrastructure metrics and container monitoring, application performance monitoring, geospatial data analysis and visualization, security analysis and business analysis.",
How does each tenant ensure data security and isolation when using MinIO?,"Each tenant runs its own MinIO cluster, which is completely isolated from other tenants, allowing tenants to be protected from any disruption from upgrades, updates, and security incidents.",
What is MinIO designed for?,"MinIO is designed for large-scale, multi-data center cloud storage services.",
How many drives are included in each wipe set by default?,"By default, each wipe set contains 16 drives.",
How does MinIO achieve high concurrency?,MinIO runs in user space and uses lightweight coroutines to achieve high concurrency.,
What process is each node of MinIO?,Each MinIO node is a process.,
What types of data is MinIO suitable for storing?,"MinIO is ideal for storing large volumes of unstructured data, such as images, videos, log files, backup data, and container/virtual machine images.",
What is MinIO?,MinIO is an object storage service based on the Apache License v2.0 open source agreement.,
What applications does Kafka have for log synchronization?,"Log messages are synchronized to the Kafka message queue in a reliable asynchronous manner, and then other components are used to perform real-time or offline analysis of the logs. It can also be used to collect key log information for application monitoring.",
How does Kafka cope with traffic floods such as e-commerce promotions?,"Kafka can accumulate information such as cached orders through the queue service, and process the messages when the downstream system is capable of processing them to avoid the collapse of the downstream subscription system due to sudden traffic; at the same time, the Kafka message queue can be used to pass requests to prevent back-end applications from being overwhelmed.",
In what scenarios is Kafka suitable?,"Kafka is suitable for massive data collection and transmission scenarios, such as log collection and centralized analysis, asynchronous communication, peak-shift flow control and traffic peak shaving, log synchronization, etc.",
How does the service availability of Kafka and RabbitMQ compare?,"Kafka adopts cluster deployment, partitioning and multi-copy design. Single-agent downtime has no impact on the service, and supports linear improvement of message capacity; RabbitM",
How does the client support of Kafka and RabbitMQ compare?,"Kafka only supports Kafka custom protocols, written in Scala and Java, and supports SSL/SASL authentication and read and write permission control; RabbitM",
What are the differences between the consumption models of Kafka and RabbitMQ?,"Kafka uses the pull method to consume messages, supporting message filtering, client active pulling, message traceback and broadcast consumption; while RabbitM",
What are the functional differences between Kafka and RabbitMQ?,"Kafka supports persistence, transaction messages, and single-partition-level ordering; RabbitM",
How do the reliability of Kafka and RabbitMQ compare?,Kafka and RabbitM,
What is the performance difference between Kafka and RabbitMQ?,Kafka single node,
 Which version of MySQL Release Notes adds the function of connecting to HwameiStor?,v0.8.1,
 Which version of MySQL Release Notes has the new backup and recovery function?,v0.4.0 and v0.3,
 Which version of MySQL Release Notes adds the feature of docking insight based on grafana crd?,v0.4.0 and v0.3,
 Which version of MySQL Release Notes adds the custom role feature?,v0.8.1,
Which version of MySQL Release Notes supports NodePort port conflict early detection?,v0.5.0,
What has been optimized in v0.6.4?,"In the v0.6.4 version, the password verification is optimized and adjusted to MCamel medium password strength.",
What are the new features in v0.8.0?,"The v0.8.0 version adds helm-docs template files, the Operator in the app store can only be installed on mcamel-system, supports cloud shell, supports separate registration of the navigation bar, log viewing operation instructions, and supports viewing logs.",
"When installing mcamel-rabbitmq, what parameters can be configured to enable link tracking?","During the installation of mcamel-rabbitmq, link tracking can be enabled according to parameter configuration.",
What new features are added in v0.10.1?,"The v0.10.1 version adds three new functions: displaying related events, customizing roles, and access link tracking.",
What versions of Release Notes are listed?,This page lists RabbitM from 2022-10-27 to 2023-04-27,
List what new features are added in MinIO Object Storage v0.1.2?,#NAME?,
List what new features are added in MinIO Object Storage v0.1.4?,- Configure Bucket when creating a MinIO cluster\n- Common fields when returning lists or details\n- Return alarm list\n- Verify Service comments\n- minio supports built-in BUCKET creation when created\n- Interconnection alarm capabilities\n - Added function to determine whether SC supports capacity expansion and prompt in advance,
List what issues have been fixed in MinIO Object Storage v0.2.0?,- `mcamel-minio` fixes the problem of abnormal status display when running a single instance. \n- `mcamel-minio` did not verify the name when creating the instance.,
List what new features are added in MinIO Object Storage v0.3.0?,- Added `mcamel-minio` helm-docs template file. \n- The new `mcamel-minio` Operator in the app store can only be installed on mcamel-system. \n- Added `mcamel-minio` to support cloud shell. \n- Added `mcamel-minio` to support separate registration of navigation bar. \n- Added `mcamel-minio` to support viewing logs. \n- Added `mcamel-minio` Operator to interface with chart-syncer.,
List what issues have been fixed in MinIO Object Storage v0.4.1?,- The `mcamel-minio` page displays an incorrect LoadBalancer address. \n- `mcamel-minio` should not verify wild storage configuration issues when removing MinIO. \n- `mcamel-minio` fixes occasional failure to create Bucket.,
Lists the new features and optimizations of MinIO Object Storage v0.5.1.,- Added `mcamel-minio` details page to display related events\n- Added `mcamel-minio` to support custom roles\n- Optimized `mcamel-minio` scheduling strategy to add sliding buttons,
What are the Release Notes of MinIO Object Storage used for?,The Release Notes of MinIO Object Storage are used to understand the evolution path and feature changes of each version.,
In which version does mcamel-elasticsearch add a new user list interface?,"In version v0.1.1, mcamel-elasticsearch adds a new interface for obtaining user lists.",
"In which version, mcamel-kafka added the function of synchronizing Pod status to the instance details page?","In version v0.1.2, mcamel-kafka adds the function of synchronizing Pod status to the instance details page.",
"In which version, mcamel-kafka supports middleware link tracking adaptation?","In version v0.4.0, mcamel-kafka supports middleware link tracking adaptation.",
"In which version, mcamel-kafka added the function of supporting custom roles?","In version v0.5.1, mcamel-kafka has added the function of supporting custom roles.",
What information does Kafka Message Queue Release Notes provide?,"Provides Kafka message queue version release records, including new features, optimizations, fixes and other details.",
In which version did a new interface for obtaining the list of NodePorts assigned to the cluster be added?,"In version v0.4.0, a new interface is added to obtain the list of NodePorts assigned to the cluster. This version also adds features such as status details and node affinity configuration, and fixes multiple issues.",
What new features are added in v0.5.0?,"The v0.5.0 version adds helm-docs template files, Operators in the app store can only be installed in mcamel-system, and supports separate registration of cloud shell and navigation bar. At the same time, multiple issues such as the instance name being too long causing custom resources to fail to be created were fixed.",
Which dependent libraries have been upgraded in v0.6.0?,Version v0.6.0 upgrades golang.org/x/net to v0.7.0 and GHippo SDK to v0.14.0.,
What new features are added in v0.5.1?,"The v0.5.1 version adds a new event details page, supports custom roles, and adds sliding buttons to optimize scheduling strategies. At the same time, a problem that may interrupt retries when hosting a cluster has been fixed.",
What is the role of Elasticsearch Index Service Release Notes?,Elasticsearch Index Service Release Notes lists the evolution path and feature changes of each version to facilitate users to understand the development history and new features of Elasticsearch Index Service.,
What issues need to be paid attention to when migrating RabbitMQ data?,"1. Due to business needs, please ensure that data migration is performed offline. \n2. When using the shovel plug-in to transfer data between clusters, there will be a cache area between the source cluster and the target cluster. Therefore, before you begin, you need to determine the cache size and ensure that its capacity does not exceed the disk capacity. \n3. During the migration process, messages in the source cluster will be transferred to the target cluster. Therefore, the messages in the source cluster will be cleared. If you do not want to delete local messages, please set `Auto-delete` to `never`. \n4. Since data migration involves data transfer between multiple clusters, the data transfer speed may be affected by network bandwidth and latency.",
What are the two options for RabbitMQ data migration? What are their respective operating procedures?,"Option 1: Without migrating data, switch to the production side first, and then switch to the consumer side. The specific operation process has two steps:\n1. Switch the message production end to the cluster `rabbitmq-cluster-b`, and no longer produce messages to the `rabbitmq-cluster-a` cluster. \n2. The consumer consumes messages from the `rabbitmq-cluster-a` and `rabbitmq-cluster-b` clusters at the same time. When all the messages in the `rabbitmq-cluster-a` cluster are consumed, switch the message consumer to ` In the rabbitmq-cluster-b` cluster, data migration is completed. \nOption 2: Migrate the data first, and then switch the production side and the consumer side at the same time. The specific operation process is as follows:\n1. Open the `shovel` plug-in and configure the corresponding information. \n2. When the shovel status is ""running"", it means the migration has started. After the data migration is completed, switch the production end and consumer end to the `rabbitmq-cluster-b` cluster to complete the migration process.",
What are some tips for instance monitoring that can help us better manage Elasticsearch?,"Instance monitoring provides some tips, such as optimizing index management and query operations based on CPU and memory usage, and using reserved CPU capacity to avoid blocking operations.",
How to switch the time range on the instance monitoring screen?,"You can select different time ranges in the upper right corner, including options such as `Last 6 hours`, `Last 12 hours`, `Last 24 hours`, `Last 3 days` and `Last 7 days`.",
How to view the monitoring information of the Elasticsearch cluster?,"Enter the Elasticsearch cluster list, click a cluster name, and then click `Cluster Monitoring` in the left navigation bar to view the monitoring information of the cluster.",
What information about Elasticsearch is displayed on the monitoring screen?,"The large monitoring screen displays Elasticsearch's CPU, memory, disk usage, number of indexes, number of documents, and network I/O information.",
How to enter the monitoring screen of an Elasticsearch instance?,"Click an instance name in the Elasticsearch instance list, and then click `Instance Monitoring` in the left navigation bar to enter the large monitoring screen.",
 How to delete PostgreSQL instance?,Select PostgreS in the DCE console,
 How to update the configuration of a MinIO instance?,"Select the MinIO module in the DCE console, and then click the ""Update Configuration"" option in the ""Action"" drop-down menu of the corresponding instance to update the configuration parameters of the MinIO instance.",
 How to create a MinIO instance?,"Select the MinIO module in the DCE console, then click ""Create Instance"" and fill in the corresponding parameter information as prompted to create a MinIO instance.",
 How to search for Kafka instances?,"Select the Kafka module in the DCE console, and then search for Kafka instances by entering the instance name in the search box above the instance list page.",
 How to view the list of Kafka instances?,You can view a list of Kafka instances by selecting the Kafka module in the DCE console and clicking Instance List.,
Which middleware modules of the data service correspond to which module permission mappings of the workspace?,The middleware module and permission mapping correspond to the following table:\n| Middleware module | Menu object | Operation | Workspace Admin | Workspace Editor | Workspace Viewer |\n| --- | --- | --- | --- | --- | --- |\n| MyS,
Which module is the data service module built on?,The data services module is built on top of the workspace.,
List the new features of v0.0.2 version.,Added `Mcamel-PostgreS,
List the new features of v0.1.1 version.,- Added `Mcamel-PostgreS,
List the upgrade functions of v0.1.2 version.,- Optimize `Mcamel-PostgreS,
List the new features of v0.1.2 version.,- Added `Mcamel-PostgreS,
What is this page for?,This page lists PostgreS,
How to export RabbitMQ logs?,"There is an export button in the upper right corner, which can export all currently displayed logs to a text file.",
How to perform keyword search log?,Just enter keywords in the search area on the left to search.,
What time range selections can be made on the RabbitMQ log viewing page?,In RabbitM,
How to enter the log viewing page of the RabbitMQ instance?,In RabbitM,
What common operations does the RabbitMQ log viewing page support?,RabbitM,
Does it support exporting logs?,"Yes, there is an export button in the upper right corner of the page to export the currently displayed log.",
How to display log volume distribution?,"The distribution of log volume is displayed in a histogram, and the upper middle area displays the distribution of log volume within the time range.",
Does it support keyword search logs?,"Yes, the left search area supports keyword search and supports viewing more log information.",
How to switch the time range for viewing logs?,"In the upper right corner of the log page, you can find the option to customize the time range. Click it to select the time period you want to view.",
Where can I view Elasticsearch logs?,You can access the details page of each Elasticsearch instance and find the "Log View" option in the left menu bar of the instance. You can view the Elasticsearch logs on this page.,
In which area can I view the log quantity distribution within a time range?,"In the upper-middle area histogram of the log volume distribution view, you can view the log volume distribution within the time range.",
What operations are supported on the PostgreSQL log viewing page?,"Supports customizing log time range, keyword search logs, viewing log volume distribution, viewing log context and exporting logs.",
How to enter the log viewing page of a PostgreSQL instance?,In PostgreS,
What problems might occur during migration of PVs? How to solve?,"rclone may lose permissions during the migration process, causing es to fail to start. The solution is to add an initcontainer in the CR of es to modify the permissions.",
How to restore common-es?,"Follow the stop sequence to restore, first start the elastic-operator, and then start the es pods one by one.",
How to check the migration results?,Use the command kubectl get lvr.,
What is LocalVolumeMigrate?,LocalVolumeMigrate is a Kubernetes custom resource used to migrate local PVs within the cluster.,
Which middleware module is mentioned in this article?,DCE.,
What is the purpose of adding initcontainer to ES pod?,The purpose is to restore the permissions required for ES to run.,
How do I check the migration status? How do I check the migration results after the migration is completed?,You can use the `kubectl get localvolumemigrates.hwameistor.io` command to view the migration status; you can use the `kubectl get lvr` command to view the migration results.,
What preparations are required before migrating PV?,"Before migrating PVs, you need to clarify the PV information that needs to be migrated, stop the data service middleware, and create a migration task in hwameistor.",
"In the experimental scenario introduced in this article, how many nodes are there in the cluster? On which nodes is ES installed?","In the experimental scenario, the cluster has 6 nodes; ES is installed on three nodes: prod-worker1, prod-worker2, and prod-worker3.",
What topic is this article about?,This article introduces the Elasticsearch migration practice based on Hwameistor.,
How to customize the MinIO log query time range?,You can switch the log query time range in the upper right corner of the MinIO log page.,
How to view MinIO logs?,"You can view MinIO's logs by visiting the instance details page of each MinIO and clicking `Log View` on the left menu bar. You can adjust the time range and refresh cycle of log queries as needed, and you can also perform common operations such as retrieving logs by keyword, viewing the time distribution of logs, viewing the context of logs, and exporting logs.",
How to customize the log time range and keyword search on the Kakfa log viewing page?,"In the upper right corner of the log page, you can customize the time range and enter keywords on the left to search.",
What operations are supported to easily view Kafka logs?,You can easily view Kafka logs in the following ways:\n- Custom time range\n- Keyword search\n- Log volume distribution graph\n- View context\n- Export log,
How to view the logs of a Kafka instance?,"1. In the Kafka instance list, select the instance you want to view, and click the instance name to enter the details page;\n2. Select ""Log View"" in the left menu bar of the instance;\n3. After entering the log view page, you can customize Time range, keyword search, distribution chart viewing, context and other operations.",
How to export Redis logs?,Click the "Export" button in the upper right corner of the page to export the Redis logs of the current page or all pages.,
Where can I find more Redis log information?,The search area on the left supports viewing more Redis log information.,
How to customize the time range of Redis logs?,"In the upper right corner of the log page, you can easily switch the time range for viewing logs and customize the time range as needed.",
What are the common operations on the log viewing page?,"Commonly used operations include supporting operations such as customizing the log time range, retrieving logs by keyword, viewing log volume distribution, and viewing log context. Exporting logs is also supported.",
Where can I view Redis logs?,You can view Redis logs by accessing the instance details page of each Redis and finding the "Log View" option in the left menu bar.,
"On the log viewing page, what are the commonly used operating instructions?",* Supports customizing log time range;\n* Supports keyword search for logs;\n* Supports viewing more log information;\n* Supports viewing log volume distribution;\n* Supports viewing log context;\n* Supported Export log.,
Which menu bar option on the MySQL instance details page can view logs?,"In the left menu bar of the instance, you can find a menu bar option for log viewing.",
Which page can be used to view MySQL logs?,Each MyS can be accessed via,
What features will be supported after RabbitMQ is deployed in DCE 5.0?,Deploying RabbitM in DCE 5.0,
What management interfaces does RabbitMQ provide?,RabbitM,
What message queue protocols does RabbitMQ support?,RabbitM,
How is the reliability of RabbitMQ ensured?,RabbitM,
What are the common functional features of RabbitMQ?,RabbitM,
What are the ways to import ElasticSearch data?,ElasticSearch data import includes Maxcompute data import tool and full and incremental collection methods.,
What features will be supported after deploying Elasticsearch in DCE 5.0?,"After deploying Elasticsearch in DCE 5.0, it will also support Elasticsearch dedicated node, hot data node, cold data node, and data node role deployment, integrate Kibana, expose indicators based on elasticsearch-exporter, and integrate Elasticsearch Dashboard based on Grafana Operator to display monitoring data. And use ServiceMonitor to interface with Prometheus to capture indicators and other features.",
What common features does Elasticsearch support?,"Elasticsearch supports common functional features such as distributed clusters, search management, full-text search, data collection, and service authentication.",
What does Elasticsearch store and build during the indexing process?,"During the indexing process, Elasticsearch stores documents and builds an inverted index.",
What data structure does Elasticsearch use for full text search?,Elasticsearch uses a data structure called an inverted index for full-text search.,
 What are the specific steps for choosing a workspace?,"1. Select the corresponding component in the left navigation bar, such as `Middleware` -> `MinIO Storage`. \n2. After selecting a workspace in the pop-up window, click `Confirm`. If no pop-up window appears/you want to switch workspaces, you can manually click the switch icon to select a new workspace. \n3. If this is the first time to use this component, you need to click [`Deploy Now`] (minio/user-guide/create.md) to create an instance.",
" Why do I need to select a workspace when using the Elasticsearch, Kafka, MinIO, MySQL, RabbitMQ, and Redis components of DCE 5.0?","Because using these components requires selecting a workspace, workspace is the concept of DCE 5.0, please refer to [Workspace and Hierarchy](../ghippo/user-guide/workspace/ws-folder.md). Select the corresponding component in the left navigation bar, and then select a workspace in the pop-up window.",
How to delete a message queue in RabbitMQ?,"You can perform the following steps:\n1. In the message queue, click the `...` button on the right and select `Delete Instance` in the pop-up menu. \n2. Enter the name of the message queue in the pop-up window. After confirming it is correct, click the `Delete` button. Please note that after deleting an instance, all messages related to the instance will also be deleted.",
How to delete Kafka instance?,"1. In the message queue, click the `...` button on the right and select `Delete Instance` in the pop-up menu. \n2. Enter the name of the message queue in the pop-up window. After confirming it is correct, click the `Delete` button. \nNote: After deleting an instance, all messages related to the instance will also be deleted, so please operate with caution.",
What happens after deleting a Redis instance?,"After deleting an instance, all information related to the instance will also be deleted, so please operate with caution.",
How to delete a Redis instance?,"You can click the `...` button on the right in the Redis instance list, select `Delete instance` in the pop-up menu, and enter the name of the instance in the pop-up window. After confirming that it is correct, click the `Delete` button.",
What features does MySQL have in DCE?,MyS,
Which features of MySQL are supported in DCE 5.0?,"In DCE 5.0, MyS",
What are the steps to create a Redis instance?,1. Click the New Deployment button\n2. Configure basic information\n3. Configure specification configuration\n4. Configure service settings\n5. Confirm after confirming that everything is correct\n6. Wait for the instance status to change to Running.,
What happens after deleting the MySQL instance?,"After deleting an instance, all information related to the instance will also be deleted. Please proceed with caution.",
How to delete MySQL instance?,Available on MyS,
What are the consequences of deleting a PostgreSQL instance?,"After deleting an instance, all information related to the instance will also be deleted. Please proceed with caution.",
How to delete a PostgreSQL instance?,1. In PostgreS,
How to confirm that the instance configuration information is correct and complete the creation?,"After confirming that the instance configuration information is correct, you need to click the ""Confirm"" button to complete the creation.",
"After creating a Kafka instance, what will the status become first?","After creating a Kafka instance, the status will first change to ""Not Ready"", and it will take a few minutes before the status changes to ""Running"".",
"When creating a Kafka instance, how to select the storage volume and total storage space?","When creating a Kafka instance, you can select the storage volume and total storage space of the Kafka instance.",
"When creating a Kafka instance, what number of replicas are supported?","When creating a Kafka instance, the number of replicas is 1, 3, 5, and 7.",
What basic information needs to be set when creating a Kafka instance?,"When creating a Kafka instance, you need to set basic information such as the instance's name, cluster, region, and node.",
How to optimize the initial configuration of PostgreSQL?,Create PostgreS,
How to access PostgreSQL database?,ClusterIP is used as the access method by default. You can connect to PostgreS through the connection information on the DaoCloud console,
How to create a PostgreSQL instance?,Refer to the following steps to create PostgreS,
 How to set the username and password to connect to the RabbitMQ instance?,Creating RabbitM,
 What number of replicas does RabbitMQ support?,RabbitM,
 How to check RabbitMQ instance status?,View the instance status on the instance list page.,
 What are the statuses of RabbitMQ instances?,RabbitM,
 How to set the specifications of a RabbitMQ instance?,Creating RabbitM,
 What are the steps required to create an instance in RabbitMQ?,"1. Click the `New Instance` button in the upper right corner. \n2. Configure basic information, such as name, region, and availability zone. \n3. Configuration specifications, including version, number of copies, resource quotas, storage volumes, etc. \n4. Configure service settings, such as access method and connection to RabbitM",
What prompt will appear after the Elasticsearch instance is successfully created?,The screen will prompt `Instance created successfully`.,
How to set the access type and username and password?,"In the Create Elasticsearch instance page, after configuring the instance specifications, you can set the access type (ClusterIP or NodePort) and username and password.",
How to configure the specifications of the Elasticsearch instance?,"On the Create Elasticsearch Instance page, after selecting a version, you can configure the instance specifications. You can optionally enable/disable data nodes, Kibana nodes, dedicated master nodes, and cold data nodes. Among them, hot data nodes are enabled by default (to store daily active data of the Elasticsearch search service, 3 copies by default), Kibana nodes are enabled by default (nodes that store Elasticsearch visual data, 1 Kibana node by default), optional dedicated master nodes and Optional cold data nodes (nodes that store some Elasticsearch historical data, 3 cold data nodes by default).",
What does it take to create an Elasticsearch instance?,"1. Click `New Instance` in the upper right corner. \n2. On the Create Elasticsearch Instance page, enter the basic information of the instance and click `Next`. \n3. Select a version, configure the instance specifications, and click `Next`. You can optionally enable/disable data nodes, Kibana nodes, dedicated master nodes, and cold data nodes. \n4. After setting the access type (ClusterIP or NodePort), username and password, click `Next`. \n5. After confirming that the above basic information, specification configuration and service settings are correct, click `Confirm`.",
What conditions need to be met for the instance status to change to Running?,All relevant containers start successfully.,
How to set instance access mode?,"You can set three access modes: intra-cluster access, node port, and load balancer.",
Can the storage capacity and number of disks per copy of an instance be modified after the instance is created?,Cannot be lowered.,
What information should be filled in during the instance creation process?,Fill in the requirements below the input box.,
What basic information is required to create a MinIO instance?,"Instance name, cluster/namespace.",
What does MinIO’s multi-cloud gateway do?,"MinIO's multi-cloud gateway makes your existing storage infrastructure compatible with Amazon S3, allowing organizations to truly unify their data infrastructure from files to blocks, all exposed as objects accessible through the Amazon S3 API without the need for migration.",
How does MinIO achieve global consistency?,"MinIO allows multiple instances to be combined together to form a unified global namespace. Up to 32 MinIO servers can be combined into a distributed mode set, and multiple distributed mode sets can be combined into a MinIO server federation to achieve Global consistency.",
What does MinIO's continuous replication do?,"MinIO's continuous replication ensures that even with highly dynamic data sets, data loss in the event of a failure is kept to a minimum.",
How does MinIO implement a multi-cloud gateway?,"MinIO runs on bare metal, network attached storage, and every public cloud, and supports the Amazon S3 API, ensuring you have exactly the same view of your data from both an application and management perspective. Organizations can truly unify their data infrastructure from files to blocks, all exposed as objects accessible through the Amazon S3 API, without the need for migration.",
How does MinIO ensure global consistency?,"MinIO allows various instances to be grouped together to form a unified global namespace, and provides unified administrators and namespaces. Up to 32 MinIO servers can be combined into a distributed mode set, and multiple distributed mode sets can be combined into a MinIO server federation.",
How does MinIO implement identity authentication and management?,"MinIO supports the most advanced standards in identity management and integrates with OpenID connect compliant providers as well as major external IDP vendors. Access policies are fine-grained and highly configurable, supporting multi-tenant and multi-instance deployments.",
How does MinIO support encryption?,"MinIO supports multiple sophisticated server-side encryption schemes to protect stored data everywhere. MinIO's approach ensures confidentiality, integrity, and authenticity with negligible performance overhead. Server-side and client-side encryption is supported using AES-256-GCM, ChaCha20-Poly1305, and AES-CBC.",
What is the role of erasure code in MinIO?,"MinIO uses erasure codes to protect objects. MinIO uses the Reed-Solomon code to partition objects into n/2 data and n/2 parity blocks, allowing data to be reliably reconstructed from the remaining drives even if up to 5 drives are lost.",
How does MinIO protect data from Bitrot?,MinIO's optimized implementation of high-speed hashing algorithms ensures that it never reads corrupted data and captures and repairs corrupted objects in real time. This implementation is designed for speed and can achieve hashing speeds in excess of 10 GB per second on a single core of an Intel CPU.,
What is the role of Set in MinIO?,"Set in MinIO is a collection of drives. Distributed deployment automatically divides one or more sets according to the cluster size. An Object is stored in a Set, and a cluster is divided into multiple Sets.",
What does Drive mean in MinIO?,"Drive in MinIO is the disk that stores data, and all object data is stored in Drive.",
What is a Bucket in MinIO?,Bucket in MinIO is a logical space used to store Objects. They are isolated from each other and are similar to the top-level folders in the system.,
What does Object in MinIO mean?,"Object in MinIO refers to the basic objects stored, such as files, pictures, etc.",
What type of parameter configuration interface does MinIO have built in?,MinIO has a built-in UI-based parameter configuration interface.,
"After modifying the MinIO configuration parameters, what operations need to be performed to make the modifications effective?",Click the "Save" button for the changes to take effect.,
What operations can be performed in the MinIO parameter configuration UI interface?,"You can add, delete and modify various parameters of MinIO.",
How to open the MinIO parameter configuration UI interface?,"In the MinIO instance list, find the MinIO instance for which you want to configure parameters and click the instance name. Then select ""Configuration Parameters"" in the left navigation bar to open MinIO's parameter configuration UI interface.",
 Is it possible to turn on and off certain functions in the Kafka parameter configuration interface?,"Yes, you can turn on or off various functions by modifying the corresponding slider switch.",
 How to save modified parameter configuration?,Just click `Save` and the modified parameters will take effect immediately.,
 What can the Kafka parameter configuration interface do?,You can easily configure various parameters of Kafka and Zookeeper.,
 How to open the Kafka parameter configuration interface?,"In the message queue page, click a name, and then click `Configuration Parameters` in the left navigation bar to open it.",
 Which parameter configuration interface does Kafka have built in?,Kafka has a built-in parameter configuration UI interface.,
What are the steps to create a MySQL instance?,"1. In the instance list, click `New Instance` in the upper right corner. \n2. Create MyS in `",
How is the compatibility between the data service middleware and the Kubernetes version?,Please refer to the table below: \n| Middleware | Version | Features | 1.26 | 1.25 | 1.24 | 1.23 | 1.22 |\n| ------------- | ------ | - ------- | -------- | -------- | -------- | -------- | ------ -- |\n| RabbitMq | 0.6.2-24-gb25cc385 | operator | ✅ | ✅ | ✅ | ✅ | ✅ |\n| | | Create instance - | ✅ | ✅ | ✅ | ✅ | ✅ |\n| | - | Edit instance- | ✅ | ✅ | ✅ | ✅ | ✅ |\n| | - | Query instance- | ✅ | ✅ | ✅ | ✅ |\n- |- |- Instance connection- |- |- | - |- |\n- |- |- Delete instance - |- |- |- |- |\n|RabbitMq \n|RabbitMq \n|RabbitMq \n|RabbitMq \n|RabbitMq \n| ElasticSearch | 0.3. 4-16-g90e5ad9 | operator | ✅ | ✅ | ✅ | ✅ | ✅ |\n| | | Create instance | ✅ | ✅ | ✅ | ✅ | ✅ |\n| | | Edit instance | ✅ | ✅ | ✅ | ✅ | ✅ |\n| | | Query instance| ✅ |✅ |✅ |✅ |✅ |\n| |- |- Instance connection- |- |- |- |- |- |\n|- |- | - Delete instance- |- |- |- |- |- |\n| MyS,
What are the data service middlewares?,Data Services has officially released RabbitM,
How to set the number of Elasticsearch shards?,"The number of shards can only be specified when creating the index and cannot be modified. When creating an index, you can specify the number of Shards based on the actual situation.",
What are Elasticsearch replicas?,"The Elasticsearch replica is a backup of the index, and the data is automatically synchronized to the replica after the write operation is completed. Replicas can improve system high availability and concurrency performance during searches.",
What is Elasticsearch sharding?,"Elasticsearch sharding is splitting an index into multiple parts, each part is called a shard. Each shard is hosted on any node in the cluster, and each shard is an independent, fully functional ""index"" in its own right.",
What is an Elasticsearch index?,An Elasticsearch index is used to store data and is a logical space where one or more shards are grouped together.,
What is an Elasticsearch node?,"The Elasticsearch node is a member of the cluster and can be used to store index data or serve as a coordination node to share the CPU overhead of data nodes to improve processing performance and service stability. Specifically, it includes data nodes, dedicated master nodes and coordination nodes.",
What is a Broker?,"The agent is the service node of the message middleware, that is, the message queue server entity.",
Is there a direct relationship between producers and consumers?,Producers publish messages and consumers consume messages. Producers and consumers have no direct relationship with each other.,
What is a delayed message?,Delayed message means that the producer publishes the message to RabbitM,
What is a queue? what's the effect?,"Queues are used to store messages, producers send messages to the queue, and consumers get messages from the queue. Multiple consumers can subscribe to the same queue at the same time, and messages in the queue are assigned to different consumers. Each message is put into one or more queues.",
What optional attributes does the message header consist of?,"The message header consists of a series of optional attributes, including routing-key (routing key), priority (priority relative to other messages), delivery-mode (indicating that the message may require persistent storage), etc.",
What are the two parts of a message? What are the differences?,"Messages are generally divided into two parts: message body and label. Labels are also called message headers and are mainly used to describe this message. The message body is the content of the message, which is a json body or data, etc.",
 What is a Tag?,Tags are keywords associated with resources that help manage resource ownership and organize finding and operating on resources.,
What is Standby Node?,A standby node is a node that is set to be idle in hot standby mode.,
What is an SSL Certificate?,An SSL certificate is a digital certificate that describes the identity of a website.,
What is SQL Mode?,S,
 What is Read-Only Node?,Read-only nodes are replicas of the cluster master node.,
What is a Port?,port is the communication endpoint of a network connection. Use the port number used by each transport protocol to identify a port.,
 What is Point-In-Time-Recovery?,Recovery point-in-time ensures automated backups to restore data created in the previous state of the server.,
What is Operations Throughput Metric?,"The operation throughput metric is a measure of the throughput of get, insert, update, and delete operations for all databases on the server.",
What is Node Plan?,"A node plan, database, or cluster configuration is a hardware plan for node specifications.",
What is Machine Type?,A machine type is a set of virtualized hardware resources for a virtual machine (VM) instance.,
What is LUKS Disk Encryption (LUKS Disk Encryption)?,LUKS disk encryption is the Linux unified key set disk encryption specification.,
What is Index vs. Sequential Reads Metric?,The Index vs. Sequential Reads metric graph shows reads using indexes as a percentage of the total reads for all databases on the master server.,
What is Hot Standby?,Hot standby is the act of listening for when a primary node fails so that a backup node can take its place.,
What is High Availability?,High availability (HA) is an infrastructure design approach that focuses on reducing downtime and eliminating single points of failure.,
What is Failover?,Failover is a high-availability mechanism that monitors servers for failures and reroutes traffic or operations to a backup server if the primary server fails.,
What is E2EE?,"E2EE is end-to-end encryption, a communications system that encrypts messages and message services for everyone except the user who receives the message and the user who sends the message.",
What is DBaaS (Database as a Service)?,DBaaS is a cloud service that allows users to access cloud database systems on a subscription basis without owning a personal cloud data system.,
What is Connection Status Metric?,"The connection status metric is a measure of the number of threads created, connected, and running relative to database connection limits.",
What is an ACL?,"ACL is an access control list, a list of user permissions used to control access to system resources.",
 What is ACID compliance?,"ACID compliance is a set of database characteristics consisting of atomicity, consistency, isolation, and durability that ensure efficient completion of database transactions.",
What are Brokers and Clusters? What is the role of the cluster controller?,"An independent Kafka server is called a Broker. The Broker provides services to consumers, responding to requests to read partitions and returning messages that have been committed to disk. Broker is an integral part of the cluster. The cluster controller is responsible for management tasks, including assigning partitions to Brokers and monitoring Brokers.",
How to ensure that the read status will not be lost in Kafka?,"The consumer saves the last read offset of each partition in Zookeeper or Kafka. If the consumer shuts down or restarts, it can also reacquire the offset to ensure that the read status is not lost.",
What are consumers and producers? What is their function?,"Consumers are responsible for consuming messages, and producers are responsible for creating messages. Consumers can subscribe to one or more topics and read messages in the order they are generated; producers distribute messages evenly across all partitions of the topic.",
How to classify messages in Kafka? What parts can a topic be divided into?,"Messages are classified in Kafka through Topic. A topic can be divided into several Partitions, and a partition is a commit log.",
What is the basic data unit of Kafka? Why are multiple messages put into the same batch and then written?,The basic data unit of Kafka is message. Multiple messages are put into the same batch before being written in order to reduce network overhead and improve efficiency.,
How to solve the problem of failure to create a database using CR?,"When an error occurs when using CR to create a database, it may be because MyS",
When can I use pt-table-sync to synchronize master-slave instance data?,"When the master-slave instance data is inconsistent, you can use pt-table-sync to complete master-slave consistency synchronization. In the example, mysql-0=> mysql-1 supplementary data. This scenario is suitable for master-slave switching. It is found that the new slave library has redundant executed gtids and supplements the data before redoing. Note that this kind of supplementary data can only ensure that the data is not lost. If the data that has been deleted in the new main database will be replenished, it is a potential risk. If the new main database has data, it will be replaced with old data, which is also a risk. risk.",
How to solve the problem of CR failing to create a database and reporting an error?,"If an error occurs when using CR to create a database, and MyS",
How to solve the problem of inconsistency between primary and secondary data?,"When the master-slave instance data is inconsistent, you can use the pt-table-sync command to complete master-slave consistency synchronization. The specific commands are as follows: pt-table-sync --execute --charset=utf8 --ignore-databases=mysql,sys,percona --databases=database1,database2,...dsn=u=root,p=password,h =master-pod-name.mysql.mcamel-system,P=3306 dsn=u=root,p=password,h=slave-pod-name.mysql.mcamel-system,P=3306 --print. Among them, database1, database2,... are the names of the databases that need to be synchronized, and the dsn parameter is used to specify the connection information of the main database and the slave database.",
How to solve the problem that both the active and standby Pods have the role of replica?,1. Found two MyS,
"If the slave status shows `IO_Running: No`, how to fix it?","1. Confirm whether the network connectivity of the master database and slave database is normal. \n2. If the network connectivity is normal, confirm the status of the main library and confirm whether the current GTID of the slave library exists in the main library. You can view the GTID with the following command. \n```bash\nkubectl exec -it mcamel-common-mysql-cluster-mysql-1 -n mcamel-system -c mysql -- mysql --defaults-file=/etc/mysql/client.conf\n` ``\n```sql\nmysql> show variables like ""server_id"";\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| server_id | 2 |\n+-------------- -+-------+\nmysql> show master status\G;\nFile: mysql-bin.000002\nPosition: 424\nBinlog_Do_DB:\nBinlog_Ignore_DB:\nExecuted_Gtid_Set: 4bc2107c-819a-11ed-bf23-22be07e4eaff: 1-342297,\n7cc717ea-7c1b-11ed-b59d-c2ba3f807d12:1-619197,\na5ab763a-7c1b-11ed-b5ca-522707642ace:1-178069,\na6045297-8743-11ed-8712-8e 52c3ace534:1-4073131,\ na95cf9df-84d7-11ed-8362-5e8a1c335253:1-\nmysql> show slave status\G;\nSlave_IO_State:\nMaster_Host: mysql-docker_k8s_4_0\nMaster_User: slave\nMaster_Port: 3306\nConnect_Retry: 60\nMaster_Log_File: mys ql-bin.000002 \nRead_Master_Log_Pos: 424\nRelay_Log_File: mysqld-relay-bin.000002\nRelay_Log_Pos: 636\n```\n3. If the GTID exists, check whether the network or disk IO of the slave library is too high, and whether there are other errors; if normal , you can try restarting the slave Pod. \n4. If the problem persists, you can adjust the cache size of `slave` through the following command. \n```bash\nkubectl exec mcamel-common-mysql-cluster-mysql-1 -n mcamel-system -c mysql -- mysql --defaults-file=/etc/mysql/client.conf -NB -e "" stop slave;set global slave_net_timeout=60;set global slave_max_allowed_packet=1024*1024*64;set global slave_sql_verify_checksum=0;start slave;""\n```",
How to deal with purged binlog errors?,"If a slave replication error occurs when viewing the slave Pod log, and the keyword `purged binlog` appears, it is usually necessary to rebuild the slave library.",
How to check MySQL status?,Execute the following command to confirm MyS,
How to manually fix MySQL master-slave synchronization issues?,"If there is no error `ERROR` information in the Pod log of the slave library, it means that `False` is just because the delay in master-slave synchronization is too large. You can execute the following command on the slave library to further troubleshoot:\n1. Find the Pod of the slave node\n` ``bash\nkubectl get pod -n mcamel-system -Lhealthy,role | grep cluster-mysql | grep replica | awk ""{print $1}""\n```\n2. Set `binlog` parameters\n``` bash\nkubectl exec mcamel-common-mysql-cluster-mysql-1 -n mcamel-system -c mysql -- mysql --defaults-file=/etc/mysql/client.conf -NB -e ""set global sync_binlog=10086 ;""\n```\n3. Enter MyS",
How to expand PVC?,"If a PVC is found to be full, the PVC needs to be expanded. \n```bash\nkubectl edit pvc data-mcamel-common-mysql-cluster-mysql-0 -n mcamel-system # Just modify the request size\n```",
How to check disk usage of MySQL instance?,Disk usage can be checked by running the following command on the `master` node. \n```bash\nkubectl get pod -n mcamel-system | grep cluster-mysql | awk "{print $1}" | xargs -I {} kubectl exec {} -n mcamel-system -c sidecar -- df - h | grep /var/lib/mysql\n```,
How to avoid allocating too many shards?,"Make sure to keep the number of shards below 20 for every GB configured on the node, and try to keep the map size of a single index under control. Also monitor heap memory usage on the master node and ensure it is appropriately sized.",
Within what range should the shard size be controlled?,The shard size should be controlled at least a few GB to dozens of GB. The shard size for sequential data use cases is usually between 20GB and 40GB.,
How to manage sharding?,"Indexes can be managed automatically using Index Lifecycle Management (ILM), with automatic rollover based on index size, creation time, or number of documents. The frequency of checks can be controlled by modifying the `indices.lifecycle.poll_interval` parameter.",
How to calculate the number of shards?,"The formula for calculating the number of shards is: (metadata + headroom for growth) * (1 + index overhead) / required shard size = approximate number of primary shards. Among them, metadata refers to the metadata size of each document, growth space refers to the additional space that may be needed for index growth, and index overhead refers to the index overhead of each document.",
What is sharding?,"Sharding is the basic unit of data storage in Elasticsearch, which evenly distributes index data to multiple nodes for storage and processing.",
How many primary and replica shards does Elasticsearch have per index by default in version 7.x?,"In Elasticsearch version 7.x, each index defaults to 1 primary shard and 1 replica shard.",
Within what range should the individual shard size be kept? Why?,"The size of a single shard should be kept between 10-50G for the best experience, and it is generally recommended to be around 30G. Because sharding that is too large may slow down Elasticsearch's fault recovery, while sharding that is too small may result in too many shards, leading to problems with read and write performance and insufficient memory.",
How to quickly calculate Elasticsearch storage capacity requirements?,"A simplified version of the formula can be used: source data * (1 + number of replicas) * 1.45 = minimum storage requirement. For example, if you have 500G of data storage and need a replica, the minimum storage requirement is closer to 500 * 2 * 1.1 / 0.95 / 0.8 = 1.5T.",
What is the overhead in Elasticsearch besides data?,"In addition to data, Elasticsearch also has overhead such as index overhead, operating system reserved space, Elasticsearch internal overhead, number of copies, and security thresholds.",
Which two plans need to be confirmed in advance when starting storage capacity planning?,"At the beginning of storage capacity planning, usage scenarios and data requirement planning need to be confirmed in advance.",
How to solve the problem of 429 error?,"Method 1: Reduce write concurrency and control the amount of writes. \n- Method 2: Increase the queue size appropriately, if resources permit. Method 1 and method 2 can be used together.",
What is the cause of 429 error?,The reason for the 429 error is that the write concurrency of `Elasticsearch` is too large and `Elasticsearch` has no time to process it.,
"When writing data to Elasticsearch, an error message status:429, es_rejected_execution_exception is reported. What is the reason?","The reason for the 429 error is that the write concurrency of Elasticsearch is too large and Elasticsearch has no time to process it. You can appropriately reduce the write concurrency and control the amount of writes. Or if resources permit, increase the queue size appropriately.",
Why does the error Operation not permitted appear when installing Elasticsearch in an OCP environment?,It may be caused by SELinux. This problem can be solved by setting the status of SELinux. Execute the command setenforce 0.,
What precautions should be taken with this script when changing the password of an Elastic user?,"This script has a certain probability of failure, depending on the data writing speed. In real situations, it is necessary to stop writing to the data source before executing the above method.",
What is the reason for the error `Terminating due to java.lang.OutOfMemoryError: Java heap space`?,Elasticsearch memory overflow.,
What is the error message when Elasticsearch PVC disk capacity is full?,"The error message is as follows:\n```info\n{""type"": ""server"", ""timestamp"": ""2022-12-18T10:47:08,573Z"", ""level"": ""ERROR"", ""component"": ""oemfFsHealthService"", ""cluster.name"": ""mcamel-common-es-cluster-masters"", ""node.name"": ""mcamel-common-es-cluster-masters-es-masters-0"", ""message"": ""health check of [/usr/share/elasticsearch/data/nodes/0] failed"", ""cluster.uuid"": ""afIglgTVTXmYO2qPFNvsuA"", ""node.id"": ""nZRiBCUZ",
Please give an example of adding a custom plugin.,You can refer to the following example:\n```yaml\napiVersion: rabbitmq.com/v1beta1\nkind: RabbitmqCluster\nmetadata:\n...\nspec:\n...\ntemplate:\nspec:\nvolumes:\n - name: community-plugins\nemptyDir: {}\ninitContainers:\n- command:\n- sh\n- -c\n- curl -L -v https://github.com/rabbitmq/rabbitmq-message- timestamp/releases/download/v3.8.0/rabbitmq_message_timestamp-3.8.0.ez --output rabbitmq_message_timestamp-3.8.0.ez\nimage: docker.m.daocloud.io/curlimages/curl:7.70.0\nimagePullPolicy: IfNotPresent\ nname: copy-community-plugins\nresources:\nlimits:\ncpu: 100m\nmemory: 500Mi\nrequests:\ncpu: 100m # Resource limits can be adjusted according to actual conditions\nmemory: 500Mi \nterminationMessagePolicy: FallbackToLogsOnError\nvolumeMounts:\n- mountPath: /community-plugins/\nname: community-plugins\ncontainers:\n- name: rabbitmq\n...\nvolumeMounts:\n- mountPath: /opt/rabbitmq/community-plugins\nname: community-plugins\ nenvConfig: |\nPLUGINS_DIR=/opt/rabbitmq/plugins:/opt/rabbitmq/community-plugins # Add the plug-in directory to the environment variable\nadditionalConfig: |\n...\nadditionalPlugins:\n- rabbitmq_message_timestamp # Add the plug-in enablement \n...\n```,
How to add custom plugins to RabbitMQ?,RabbitM can be created in,
Why do I need to add a custom plugin for RabbitMQ?,RabbitM,
 How to monitor the status of Elasticsearch cluster in Kubernetes?,"You can use monitoring tools such as Kubernetes Dashboard or Prometheus to monitor the status of the Elasticsearch cluster, such as viewing node CPU and memory usage, cluster throughput and other indicators.",
 How to view the list of all Pods in the current cluster?,You can use the `kubectl get pods -n <namespace>` command to view a list of all Pods in the current namespace.,
 How to configure Elasticsearch resources and performance?,"Resources and performance can be configured by modifying parameters in the Elasticsearch configuration file, for example: `ES_JAVA_OPTS` controls JVM memory usage, `discovery.zen.minimum_master_nodes` controls the minimum number of nodes required to elect master nodes, etc.",
 How to set access permissions for Elasticsearch?,"Access permissions can be set by modifying the Elasticsearch configuration file, configuring `xpack.security.enabled` to true, and setting the user name and password, or by performing user and role management through the Kibana management interface.",
 What does the basic information about an Elasticsearch cluster include?,"The basic information of the Elasticsearch cluster includes instance running status, deployment location, creation time, version and other information.",
What programming languages does Elasticsearch support?,"Elasticsearch supports programming languages such as Java, JavaScript (Node.js), Go, .NET (C#), PHP, Perl, Python, and Ruby.",
"How does the Elastic Stack help with the data collection, visualization, and reporting process?","The Elastic Stack integrates with Beats and Logstash to easily process data before indexing it into Elasticsearch. At the same time, Kibana not only provides real-time visualization of Elasticsearch data, but also provides a UI for users to quickly access data such as application performance monitoring (APM), logs, and infrastructure indicators.",
What built-in features does Elasticsearch have to make it easier for users to store and search data more efficiently?,"Elasticsearch has a number of powerful built-in features, such as data aggregation and index lifecycle management, that make it easier for users to store and search data more efficiently.",
What are the distributed features of Elasticsearch?,Documents stored in Elasticsearch are distributed in different containers called shards. Shards can be replicated to provide redundant copies of data in the event of hardware failure. The distributed nature of Elasticsearch allows it to scale to hundreds (or even thousands) of servers and process petabytes of data.,
How does Elasticsearch ensure that search speeds are very fast?,"Elasticsearch is built on Lucene and has excellent performance in full-text search. At the same time, Elasticsearch is a near-real-time search platform. The delay between document indexing operations and the document becoming searchable is very short, generally only one second.",
What are the compatibility and openness benefits of RabbitMQ?,RabbitM,
How does RabbitMQ support containerized deployment?,RabbitM,
What are the characteristics of RabbitMQ's graphical interface operation?,RabbitM,
What are the characteristics of RabbitMQ's life cycle management?,RabbitM,
Why is RabbitMQ an industry leader?,RabbitM,
What are the product advantages of RabbitMQ?,#NAME?,
How does MySQL provide real-time status indicator data?,MyS,
Does MySQL have an open architecture?,"Yes,MyS",
Does MySQL support containerized deployment?,"Yes,MyS",
What technical route does MySQL adopt to ensure the stability and reliability of the platform?,MyS,
What programming languages does MySQL support?,MyS,
What other optional settings are available within HTTP path rules?,"HTTP path rules also provide 6 optional settings, including rewrite, timeout, retry, fault injection, etc.",
What two mutually exclusive features are supported in HTTP routing?,"HTTP routing supports two mutually exclusive functions, `route target` and `redirect`. Only one of these functions can be selected in an `HTTP path rule`.",
"In HTTP routing, how is the execution order of different routing rules determined?","In HTTP routing, the execution order of different routing rules is from top to bottom, and the routing rules with the highest ranking take effect first.",
What are the routing matching rules in HTTP routing?,"Routing matching rules in HTTP routing include: matching URI, matching port and matching header.",
What are the routing configuration parts of the service mesh in DCE5.0?,"The routing configuration of the service grid in DCE5.0 is mainly divided into three parts: HTTP routing, TLS routing and TCP routing.",
How to create a service gateway?,"In DCE5.0, a service gateway can be created through the gateway module. For specific operations, please refer to the official documentation.",
What cluster management functions does DCE5.0 provide?,"DCE5.0 provides the cluster module for joining and managing clusters. For specific operations, please refer to the official documentation.",
How to implement traffic monitoring and traffic topology?,"In DCE5.0, traffic monitoring and traffic topology can be implemented through the watch and watch2 modules. For specific operations, please refer to the official documentation.",
What is sidecar management? What sidecar management modules are provided in DCE5.0?,"Sidecar management is one of the important functions of the service grid in DCE5.0, which is used to manage and control communication between services. DCE5.0 provides sidecar management modules such as sidecar, sidecarm, and watch.",
What is peer authentication? How to implement peer authentication?,"Peer identity authentication is one of the functional modules of the service grid in DCE5.0 and is used to implement secure authentication across applications. Peer identity authentication can be implemented through the security module. For specific operations, please refer to the official documentation.",
What is a virtual service? How to configure virtual services?,"Virtual service is one of the functional modules of the service grid in DCE5.0 and is used to define business rules and traffic control policies. Virtual services can be configured through the traffic module. For specific operations, please refer to the official documentation.",
How do I manage service entries?,"In DCE5.0, service entries can be managed through the service module. For specific operations, please refer to the official documentation.",
How to upgrade version?,"In DCE5.0, the version can be upgraded through the upgrade module. For specific operations, please refer to the official documentation.",
What is a grid gateway?,The grid gateway is one of the functional modules of the service grid in DCE5.0 and is used to manage and control traffic communication between services.,
What functional modules does the service grid provide in DCE5.0?,"The service grid in DCE5.0 provides functional modules such as inject, gateway, upgrade, service, traffic, security, sidecar, and watch.",
What traffic management capabilities can the DCE5.0 service grid provide?,"The service grid of DCE5.0 can provide traffic management capabilities such as weight, content, TCP/IP and other routing rules, HTTP session retention, current limiting, outlier detection and other traffic management capabilities.",
"In the service grid of DCE5.0, which traditional microservice SDKs can be quickly migrated to run on the cloud-native container operating environment?",The service grid of DCE 5.0 provides integrated solutions for traditional microservice SDKs such as Spring Cloud and Dubbo. Businesses developed with traditional microservice SDKs can be quickly migrated to run on cloud-native container operating environments without extensive code modification.,
What infrastructure can the service grid of DCE 5.0 perform unified service discovery and management on?,The service grid of DCE 5.0 enables unified service discovery and management of multiple infrastructures such as containers and virtual machines (VMs).,
What is the role of service mesh?,"As a fully managed microservice management solution, service grid can uniformly manage complex multi-cloud and multi-cluster environments and provide users with functions such as service traffic management, security management, and service traffic monitoring.",
What are the advantages of the service mesh of DCE 5.0?,"The service grid of DCE 5.0 has the advantages of simplicity and ease of use, intelligent routing and elastic traffic management, graphical application panoramic topology, enhanced performance, enhanced reliability, multi-cloud, multi-cluster, and multi-infrastructure.",
How to set resource usage limits for a certain workload in DCE5.0?,"Select one (or more) workloads that have sidecar injection enabled, click the `Sidecar Resource Limit` button, set the request value and limit value of CPU/memory respectively in the pop-up dialog box, select `Restart now`, and click ` OK`. In the workload sidecar management list, you can see that the data in the `CPU request value/limit value` and `memory request value/limit value` of the specified workload have been updated.",
How to disable the automatic sidecar injection function of a certain workload in DCE5.0?,"Select one or more workloads that have sidecar injection enabled, click the `Injection Disable` button on the right, confirm in the pop-up dialog box whether the number of selected workloads is correct, check `Restart now`, and click `OK`. For related uninstallation progress, you can view the `Injected POD` column.",
How to enable the automatic sidecar injection function of a certain workload in DCE5.0?,"Select one or more workloads that do not enable sidecar injection, click the Enable Injection button on the right, confirm in the pop-up dialog box whether the number of workloads selected is correct, check Restart Now, and click OK. After completing the restart of the workload, sidecar injection will be completed.",
"In DCE5.0, how to view all workloads under a cluster and their sidecar injection status, namespaces, resource limits and other information?","In the left navigation bar, click `Sidecar Management` -> `Workload Sidecar Management`. After selecting a cluster, you can view all workloads under the cluster and their sidecar injection status, namespaces, resource restrictions, etc. information.",
What are the operations of workload sidecar management in the service mesh module?,"View sidecar injection information, view sidecar running status, enable sidecar injection, disable sidecar injection, and set sidecar resource limits.",
What is the name of the service mesh module in DCE5.0?,Service mesh module.,
How to view the request aggregation report in DCE5.0?,"In DCE5.0, you can use JMeter to initiate requests and set filtering conditions through assertions. Click `Report` -> `Aggregation Report` in the left navigation bar to view the request aggregation report.",
How to configure different traffic flows for workloads in DCE5.0?,"In DCE5.0, request traffic can be weighted and distributed based on clusters, workloads can be individually labeled in multiple clusters, and traffic can be distributed to different clusters based on weight ratios in virtual services.",
How to enable multi-cloud network interconnection in DCE5.0?,"In DCE5.0, you can click `Grid Configuration` -> `Multi-cloud Network Interconnection` in the left navigation bar to enable multi-cloud network interconnection.",
What functions does the service mesh module in DCE5.0 support?,"The service grid module in DCE5.0 supports functions such as communication management, request routing, flow control, and fault recovery between microservices.",
What is the service mesh module in DCE5.0?,"The service mesh module in DCE5.0 is an open source solution for communication management, request routing, flow control and failure recovery between microservices.",
How to bind virtual services to gateway rules?,"These hosts can be exposed outside the grid by binding a VirtualService to a gateway rule for the same host. Gateway rules need to be set in the gateways field (there can be multiple) before they can be applied to the declared gateway rules. If you want to take effect on gateway rules and all services at the same time, you need to explicitly add mesh to the gateways list.",
What is the hosts field in a virtual service?,"hosts is the destination host for the traffic. It can come from service registration information, service entry (ServiceEntry) or user-defined service domain name. A hostname can only be defined in one VirtualService. The same VirtualService can be used to control the traffic attributes of multiple HTTP and TCP ports.",
How to create a virtual service through YAML?,"To create a virtual service through YAML, you can click the `YAML Create` button to enter the creation page and write YAML directly. You can also use the template provided on the page to simplify the editing operation. The editing window provides basic syntax checking.",
How to create a virtual service via graphical wizard?,"To create a virtual service with the graphical wizard, you need to follow the following steps:\n1. Click `Traffic Management` -> `Virtual Service` in the left navigation bar, and click the `Create` button in the upper right corner. \n2. In the `Create Virtual Service` interface, first confirm and select the namespace, service and application scope to which the virtual service needs to be created, and then click `Next`. \n3. After following the on-screen prompts to configure HTTP routing, TLS routing and TCP routing respectively, click `OK`.",
What are the rule tags in virtual services?,"Rule tags include: HTTP Routing, TCP Routing, TLS Routing, Rewrite, Redirect, Retry, Timeout, Fault Injection, Proxy Service, and Traffic Mirroring.",
What are virtual services in a service mesh? What are the ways to create it?,"Virtual services refer to routing and forwarding requests from different regions and users through multiple matching methods (port, host, header, etc.), distributing them to specific service versions, and dividing the load according to weight ratios. It provides routing support for three protocols: HTTP, TCP, and TLS. \nVirtual services provide two creation methods: graphical wizard creation and YAML creation.",
What is the service mesh module in DCE5.0?,"The service grid module in DCE5.0 is a microservice governance solution based on Istio, which provides functions such as flow control, fault self-healing, and security protection for microservices.",
How to view all services that enable TLS protocol in the current namespace?,"In TLS routing, you can use the drop-down list to view all services that enable the TLS protocol under the current namespace.",
"In TCP routing, how to specify the port number for matching? What parameters can be set for routing targets?","In TCP routing, you need to add a matching rule to specify the port number for matching. In the routing target, you can set parameters such as target service name, port, version, and traffic distribution weight.",
How to add multiple routing rules in TLS routing? How is the execution order determined?,"In TLS routing, multiple matching rules and routing targets can be added, and the execution order is from top to bottom, with the routing rules with the highest priority taking effect.",
What are the routing rules in the service mesh module?,"Routing rules in the service mesh module specify how requests are routed to different service instances. Including TLS routing and TCP routing two types. TLS routing matches requests based on port and SNI name, and TCP routing matches based on port number.",
What is the service mesh module in DCE5.0?,"The service mesh module in DCE5.0 is an application architecture that splits applications into multiple tiny, autonomous services that communicate with each other over the network and can be dynamically adjusted and updated.",
What is TCP routing used for?,"TCP routing is used to route TCP traffic to different target instances, matching based on port numbers.",
What is TLS routing used for?,"TLS routing is used to route encrypted TLS traffic to different target instances, selecting which target instance the traffic is routed to based on the value of the SNI header.",
What are the configuration items of traffic mirroring rules?,Traffic mirroring rules include configuration items such as traffic mirroring percentage (mirrorPercentage) and mirroring to service (host). Traffic can be copied to specified target services for sampling and analysis.,
What are the considerations for proxy virtual service rules?,Proxy virtual service rules do not support nesting. Only the main virtual service can enable the proxy function. Proxies cannot be configured when the virtual service has a "route target/redirect" configuration.,
What are the configuration items for fault injection rules?,"Fault injection rules include configuration items such as delay duration (fixedDelay), fault injection percentage (percentage), termination response code (httpStatus), and termination percentage (percentage). Errors or delays can be simulated to help test and verify the robustness of the system.",
What are the configuration items for retry rules?,"Retry rules include configuration items such as the number of retries (attempts), retry timeout (perTryTimeout), and retry conditions (retryOn). You can define the number and interval of attempts to initiate a request again when the request feedback is abnormal, as well as the error type to be retried.",
What are the configuration items for timeout rules?,"Timeout rules include a timeout configuration item, which is used to define the tolerable timeout period after a request is made to the target service.",
What are traffic management rules?,"Traffic management rules are used to specify which target instance to route the request to, and routing configuration can be performed based on request header information.",
What functions does the service grid module provide in DCE5.0?,"The service grid module in DCE5.0 provides functions such as traffic management, timeout, retry, fault injection, proxy virtual service, traffic mirroring, TLS routing and TCP routing.",
What is the service grid module in DCE5.0?,The service grid module in DCE5.0 is a tool used to manage communication and flow control between services.,
Please list some field parameter descriptions.,"Hosts: Need to match the hosts field in the traffic management policy (virtual service, target rules, etc.). \n- Addresses: Virtual IP addresses and CIDR prefixes associated with the service. \n- Ports: Service port. \n- Location: Service address, used to indicate whether the service is inside the grid. \n- Resolution: Provides multiple resolution methods for service addresses. \n- Endpoint: Service-related endpoint information, including IP address, port, service port name, etc. \n- WorkloadSelector: Used to select workloads for services within the grid.",
How to create a service entry using YAML?,"After entering the selected grid, click `Traffic Management` -> `Service Entries` in the left navigation bar, and click the `YAML Create` button in the upper right corner. Select the namespace, select the template, modify each field parameter or directly import the existing YAML file. After confirming the parameters are correct, click OK.",
How do I create a service entry using the graphical wizard?,"After entering the selected grid, click `Traffic Management` -> `Service Entry` in the left navigation bar, click the `Create` button in the upper right corner, on the `Create Service Entry` page, configure various parameters and click `OK `That’s it. The meaning of specific parameters can be found in this document.",
How are service entries created?,"Service mesh provides two creation methods, one is graphical wizard creation and the other is YAML creation.",
What is a service entry?,"A service entry can add an external service, Web API, or virtual machine to the service mesh's internal service registry, and traffic governance is applied to that external service through virtual services and targeting rules, as if the service entry were a normal service in the mesh. Same.",
How to restart and upgrade the sidecar version?,"The restart upgrade of the sidecar version is also divided into three steps: environment detection, selecting the target version, and executing the upgrade. If the hot upgrade requirements are not met during the environment detection phase, the next two steps will enter the restart upgrade process. When selecting the target version, you cannot select the version and only support upgrading to the latest version. Afterwards, clicking ""One-click upgrade"" will immediately restart the Pod, so please operate with caution.",
How to hot upgrade the sidecar version?,"The hot upgrade of the sidecar version is divided into three steps: environment detection, selecting the target version, and executing the upgrade. During the environment detection phase, it is necessary to detect whether the cluster environment meets the hot upgrade requirements, including whether the Istio version is a customized version (version suffix: -mspider), whether the K8s version meets the hot upgrade requirements, and whether the EphemeralContainer is enabled. After meeting the requirements, you can select the sidecar version you want to upgrade when selecting the target version. The default is the latest version. Then click ""One-click upgrade"" to start the sidecar hot update.",
What is a sidecar version upgrade?,"Sidecar version upgrade is a function triggered after Istio version upgrade, and can be divided into two methods: hot upgrade and restart upgrade. Among them, hot upgrade can complete the sidecar upgrade without restarting the user Pod, achieving uninterrupted business; restart upgrade requires restarting the user Pod.",
What functions does the service grid module in DCE5.0 provide?,"The service grid module in DCE5.0 provides functions such as traffic management, service security, monitoring and tracking. These include functions such as traffic routing, fault injection, circuit breaker, traffic restriction, security authentication, encrypted transmission, indicator monitoring and distributed tracking.",
How to use TLS keys in DCE5.0?,"TLS keys can be used in target rules and gateway rules to enable client or server TLS mode. You need to select simple or bidirectional mode or Istio bidirectional mode, and configure the corresponding key into the corresponding rule.",
What are TLS keys?,"TLS keys are a technology used to encrypt and verify the security of network communications. In DCE5.0, TLS keys can be used to enable client TLS mode and server TLS mode.",
What is the service grid module in DCE5.0?,"The service grid module in DCE5.0 is a microservice architecture based on Istio. By integrating Istio functions, it can provide functions such as traffic management, service security, monitoring and tracking.",
How to create TLS key?,"After entering a grid, click `Grid Configuration` -> `TLS Key Management` in the left navigation bar, click the `Create` button, fill in the name, select the namespace, fill in the credentials and private key and add a label. Can.",
How to create credentials and private key?,It can be generated using the openssl command. The specific command is:\n```\nopenssl req -out example_certs1/helloworld.example.com.csr -newkey rsa:2048 -nodes -keyout example_certs1/helloworld.example.com.key -subj " /CN=helloworld.example.com/O=helloworld organization"\noopenssl x509 -req -sha256 -days 365 -CA example_certs1/example.com.crt -CAkey example_certs1/example.com.key -set_serial 1 -in example_certs1/helloworld .example.com.csr -out example_certs1/helloworld.example.com.crt\n````,
What is the role of TLS key management?,TLS key management is used for link TLS encrypted communication between services and can manage keys in the service mesh.,
"What are the new additions, optimizations and fixes in v0.11.1?","Added key management-related APIs, governance policy tags and related filtering capabilities, cluster health status checking capabilities in the grid, and other functions; optimized pages such as virtual services, target rules, gateway rules lists, etc.; fixed multiple issues, including cancellation Issues such as the issue that sidecar related information is still displayed after sidecar injection, the managed cluster cannot be used as a working cluster, etc.",
"What are the new additions, optimizations and fixes in v0.12.0?","Added interface implementation for traffic transparent transmission function and support for istio 1.15.4 and 1.16.1; optimized the namespace sidecar management page, release pipeline parameters, sidecar injection, and sidecar resource restriction logic; repaired multiple Problems include some components not being updated after the grid is upgraded, some resources not being cleared after the grid is removed, etc.",
"What are the new additions, optimizations and fixes in v0.13.1?","Added new functions such as multi-cloud network interconnection management and related interfaces, and enhanced grid global configuration capabilities; optimized the Namespace controller plus Cache and reduced the log output of ckube in normal mode; fixed multiple issues, including migration from the container management platform Except for grid-related logic errors caused by the cluster, failure to synchronize the Cluster status information of the container management platform, etc.",
What are the upgrades and fixes in v0.14.3?,"The front-end version has been upgraded to v0.12.2, which fixes the problem that Istio resources with `.` cannot be updated, the problem that istio-proxy cannot start normally in version 1.17.1, and the problem that the ingress gateway is missing a name that causes the merge to fail and cannot be deployed.",
"In Service Mesh Release Notes, what are the new and fixed contents in v0.15.0?","A number of new features have been added, including the introduction of d2 as a drawing tool, a new wasm plug-in, a new LoadBalancer Annotations implementation of grid configuration hosting Istiod, etc. Fixed multiple issues, including incompatibility with grpcgateway-accept-language, inability to update Istio resources with `.`, etc.",
What if I can't find the cluster I want to remove?,"You can check whether the cluster has been deleted from the grid in `Grid Management` in the left navigation bar. If you still can't find it, you can contact technical support for help.",
How to confirm that the removal information is correct?,"In the pop-up window, carefully check whether the cluster information to be removed is correct and click OK after confirming it is correct.",
What is the impact of removing a cluster?,"After a cluster is removed, the cluster cannot be centrally managed through the grid, and related information about the cluster (such as logs, etc.) may be lost. So it needs to be done with caution.",
What prerequisite operations are required to remove a cluster?,"Removing a cluster requires completing some pre-operations, such as uninstalling injected sidecars, clearing cluster-related grid gateways, etc. For other pre-operations, please follow the on-screen prompts.",
How to remove a cluster?,"Click `Cluster Management` in the left navigation bar, and click the `Remove` button on the right side of the cluster list. Or in the grid list, click the `...` button on the far right and select `Remove` in the pop-up menu. After confirming that the information is correct, click OK.",
How do I create a request authentication through the wizard?,"1. Click `Security Management` -> `Request Identity Authentication` in the left navigation bar, and click the `Create` button in the upper right corner. \n2. Follow the on-screen prompts for basic configuration and click `Next`. \n3. After following the on-screen prompts for authentication settings, click `OK` and the system will verify the configured information. Refer to [Parameter configuration of authentication settings](./params.md#_7). \n4. After the verification is passed, the screen prompts that the creation is successful. \n5. On the right side of the list, click `⋮` in the operation column to perform more operations through the pop-up menu.",
How to configure the workload labeled `app: httpbin` to use JWT authentication?,This can be achieved through the following YAML configuration:\n````yaml\napiVersion: security.istio.io/v1beta1\nkind: RequestAuthentication\nmetadata:\nname: httpbin\nnamespace: foo\nspec:\nselector:\nmatchLabels:\napp: httpbin\njwtRules:\n- issuer: "issuer-foo"\njwksUri: https://example.com/.well-known/jwks.json\n```,
What is request authentication?,Request authentication is an authentication mode in the service mesh that uses JSON Web Token (JWT) to implement request encryption.,
How does the service grid provide rich performance monitoring capabilities?,"By generating cross-cluster service access data, the service grid integrates various performance monitoring services, including cross-cluster service call chains, service access topology, and service running health status. At the same time, through the global view across the cluster, the access status between services can be correlated.",
Why is non-intrusive surveillance data collection needed?,"In complex application scenarios, monitoring data collection is very important for anomaly troubleshooting. Non-intrusive monitoring data collection can provide monitoring data collection functions in an application-non-intrusive manner without requiring additional attention to the generation of monitoring data.",
What is fine-grained authorization?,"Fine-grained authorization refers to inter-service access authorization management based on authentication. It can control a service or a specific interface for authorization management, and supports source and target services to be accessed between different clusters or even different instances.",
How does the service mesh ensure security?,"The service grid distributes key and certificate pairs to service instances on the data plane by sharing a set of root certificates, and regularly replaces key certificates. Based on authentication, it provides two-way authentication and channel encryption to ensure the security of service access.",
What is a service mesh?,"A service mesh is an infrastructure layer used to manage communication between multiple microservices, providing global service access security and fine-grained authorization management.",
What are the advantages of service operation monitoring?,"By using service grid technology for monitoring, it can provide inter-service access topology, call chain, monitoring and other data collection in complex application scenarios, and provide operators with support for service operation health status assessment and optimization.",
How to achieve non-intrusive monitoring data collection?,Service grid technology can provide monitoring data collection in a non-invasive way. Users only need to focus on their own business development and do not need to pay additional attention to the generation of monitoring data.,
What solutions does the service mesh module have for security?,"The service mesh module provides comprehensive security solutions, including authentication policies, transparent TLS encryption, and authorization and audit tools to ensure access security for microservice applications in multi-cluster scenarios.",
What traffic management capabilities can the service mesh module provide?,"The service grid module can provide traffic management capabilities such as dynamically modifying access load balancing policies, limiting the number of concurrent connections, limiting the number of requests, and isolating faulty service instances.",
What is a service mesh module?,"The service grid module is a non-intrusive microservice infrastructure that provides traffic management, security, monitoring and other functions. It can manage microservice applications without modifying business code.",
How to create a hosted/proprietary grid? Please briefly describe the steps.,"The steps to create a hosted/private grid are as follows:\n1. In the upper right corner of the service grid list page, click ""Create Grid"";\n2. Select the ""hosted/private"" type and fill in the basic information;\n3 . System settings: Configure whether to enable observability, set the grid size and click Next;\n4. Governance settings: Set parameters such as outbound traffic policy, location-aware load balancing, request retry, etc.;\n5. Sidecar settings: Set global sidecar, resource limits, logs and other parameters;\n6. Click ""OK"".",
What three grid types does DCE5.0 service grid support? Please briefly introduce the characteristics of these three types.,"The DCE5.0 service grid supports three types: managed grid, dedicated grid, and external grid. The managed grid is fully hosted within the DCE5.0 service grid, and the control plane can be deployed in an independent cluster, enabling unified management of multi-cluster services in the same grid; the proprietary grid adopts the Istio traditional structure and only supports A cluster has a dedicated control plane; the external grid can connect the enterprise's existing grid to the DCE5.0 service grid for unified management.",
In which service grid modes is the cluster management function effective?,"The cluster management function is only valid in the managed grid, and is not enabled in the dedicated grid and external grid modes.",
What is cluster management?,"Cluster management is a page in the service grid that provides cluster access and removal functions. Cluster access refers to the first step for a cluster to join the grid management boundary. After the access is completed, the cluster will appear in the list of sidecar management. Users can then enable sidecar injection for the namespace and workload of the cluster to achieve aggregated governance and control of the cluster.",
What two forms of creation/editing are provided by the service mesh?,"The service mesh provides two creation/editing forms: wizard and YAML. The wizard creation method provides users with a simple and intuitive interactive method, which reduces the user's learning cost to a certain extent; the YAML creation form is more suitable for experienced users. Users can directly write YAML files to create governance policies, and the creation window also provides users with The more commonly used governance policy templates improve user writing speed.",
"In actual application, what three types of strategies are needed to be used together?","Virtual services are needed to define routing rules and describe where requests that meet the conditions go; target rules define subsets and policies, describing how to handle requests arriving at the target; if there are external service communication requirements, you need to configure port mapping in gateway rules, etc. details to fulfill the requirements.",
What are virtual services and target rules used for?,"Virtual services are mainly used to customize routing rules for request traffic, and can handle data flows such as diversion, redirection, and timeout return. Target rules focus more on the management of the traffic itself, providing more powerful load balancing and Connection survival exploration, outlier detection and other functions.",
What is traffic management and what functions can it achieve?,"Traffic management provides users with multiple traffic management functions, including routing, redirection, outlier detection, diversion, etc. Corresponding functions can be achieved by configuring virtual services, target rules, and gateway rules.",
What filtering methods does workload monitoring provide?,"Workload monitoring provides filtering methods from multiple dimensions such as namespace, service, request source, service version, etc.",
What filtering methods does service monitoring provide?,"Service monitoring provides filtering methods from multiple dimensions such as namespace, request source, service version, etc.",
What does grid performance monitoring include?,"Grid performance monitoring includes performance display of grid control plane and data plane components, and has performance statistics based on component versions.",
What does grid global monitoring include?,Global grid monitoring includes statistical information on various grid resources.,
What chart sections are used for traffic monitoring? What information does each contain?,"Traffic monitoring uses 4 chart sections, including grid global monitoring (statistics of various grid resources), grid performance monitoring (grid control plane and data plane component performance display, and performance statistics based on component versions), Service monitoring (performance statistics of all services injected into the sidecar in the grid, and multiple filtering methods are provided. Users can filter and display content from multiple dimensions such as namespace, request source, service version, etc.), workload monitoring (in the grid All workload performance statistics injected into the sidecar are provided, and multiple filtering methods are provided. Users can filter and display content from multiple dimensions such as namespace, service, request source, service version, etc.).",
What are sidecar log levels? What sidecar logging levels are available?,"Sidecar log level refers to the log level recorded in the Envoy component. Sidecar log levels that can be selected include: critical, debug, error, info, off, and trace.",
What retry policies for gRPC can be set in a service mesh?,"The retry strategies of gRPC that can be set in the service mesh include: canceled, deadline-exceeded, internal, resource-exhausted and unavailable.",
What retry policies for HTTP can be set in a service mesh?,"HTTP retry strategies that can be set in the service mesh include: 5xx, connect-failure, envoy-ratelimited, gateway-error, http3-post-connect-failure, refused-stream, reset, retriable-4xx, retriable-headers and retriable-status-codes.",
What Envoy-related parameters can be set when creating a service mesh?,"When creating a service mesh, you can set Envoy-related parameters such as the maximum number of retries and the sidecar log level.",
What are Envoy components in a service mesh?,The Envoy component is embedded in the service mesh to manage communication between services.,
How are routing rules defined in DCE 5.0?,"In DCE 5.0, routing rules are declaratively described through VirtualService and DestinationRule, and are delivered to Envoy in real time through Istio's Pilot component to implement dynamic routing.",
What does platform management mean?,"The platform management plane refers to a component that provides cluster resource management and service governance capabilities through a graphical interface or API interface, and is used to configure, monitor traffic, and diagnose errors.",
What does control surface mean?,"The control plane refers to a collection of Istio components that provide core functions such as traffic management, access control, service discovery, load balancing, and security policies.",
What does the data plane mean?,The data plane refers to the backend applications that actually handle requests and the network architecture supported by them.,
What are the components of the DCE 5.0 service grid?,"The DCE 5.0 service grid consists of three modules: data plane, control plane and platform management plane.",
Is there any special handling required after mTLS policy is enabled?,"Yes. The corresponding target rule also needs to enable mTLS mode, otherwise it will not be accessible normally.",
How to configure mTLS mode parameters?,Please refer to [Peer Identity Authentication Parameter Configuration](./params.md#_2) for specific parameter configuration.,
How do I create a peer authentication through the wizard?,"Create it by clicking `Security Governance` -> `Peer Identity Authentication` in the left navigation bar, then perform basic configuration and follow the on-screen prompts to set the authentication settings. Finally, further operations are available in the list via pop-up menus.",
How to enable peer authentication?,"After enabling peer authentication, mTLS mode needs to be turned on. Validation can be done via YAML examples.",
What is peer authentication?,"Peer-to-peer identity authentication refers to providing two-way security authentication between services without making intrusive modifications to the application source code. At the same time, the creation, distribution, and rotation of keys and certificates are automatically completed by the system and are transparent to users, thus Greatly reduces the complexity of security configuration management.",
"In creating an `AuthorizationPolicy`, how can I restrict traffic to only GET requests and not from the my-namespace namespace?",Just add the following rules in `rules`:\n```\n- from:\n- source:\nnotNamespaces: ["my-namespace"]\nto:\n- operation:\nmethods: ["GET "]\n```,
What is peer authentication? How to use mutual TLS authentication for peer authentication?,"Peer authentication is used to verify the identity of the service itself, and you can use mutual TLS authentication for peer authentication. You need to specify `mode` as `STRICT` in the `PeerAuthentication` resource, requiring both the client and server to provide valid certificates.",
What is request authentication? How can I use it to authenticate the client?,"Request authentication is used to verify the identity of a client making a request to a service. Users can authenticate using JSON Web Tokens, specifying the issuer and the location of the public key used to authenticate the token.",
What are the ways to create and edit resource files?,Users can create and edit resource files through wizards and YAML writing.,
What is the authorization mechanism provided by the service mesh?,"The authorization mechanism provided by the service mesh allows users to control access between services by specifying rules, including factors such as the source and destination of the traffic, the protocol used, and the identity of the client making the request.",
"In addition to jumping to traffic management and security management, what other functions does the menu item have?",The article does not describe it.,
How to handle the situation where abnormal status is displayed in the diagnostic configuration column?,"When an abnormal status is displayed in the diagnostic configuration column, hovering the cursor over ⓘ will display the cause of the exception. Abnormal status needs to be handled, otherwise it will affect grid-related capabilities in the next stage such as traffic management.",
How can I view detailed information about a service?,"Click on a service name to enter the details page to view the address, port and other specific information of the service in each cluster. At the same time, you can modify the communication protocol in the address information tab.",
Why are services with the same name in the same namespace aggregated into one service?,"Services with the same name in the same namespace are aggregated into one service, which facilitates unified traffic management of cross-cluster collaborative services.",
What is service management?,"Service management refers to a module in DCE5.0, which is used to list all services that have been injected into the sidecar under the current grid, and can filter services based on namespace, which is conducive to unified traffic management of cross-cluster collaborative services.",
What node information is given in the example output?,"The sample output gives each node's HostName, IP, Kubernetes version, Crictl version, OS kernel version, and OS Release.",
How to connect to the OpenShift cluster and start testing?,"After obtaining the kubeconfig of the OpenShift cluster, connect to the cluster in the container management of the demo-dev environment and start the test.",
"In the test environment, how many OpenShift nodes are prepared?","In the test environment, 3 OpenShift nodes were prepared.",
Which version of OpenShift was tested?,The OpenShift version tested was 4.11.,
What did this report test?,"This report tests the creation and access of the service mesh on OpenShift 4.11, using the environment demo-dev.daocloud.io.",
How to expose the internal services of the service grid module to the outside world in DCE5.0?,"Internal services can be exposed to the outside through configuration, which can be divided into the following steps:\n1. Through URI matching, external applications can access the specified page routing. \n2. Create istio gateway rules to expose services and ports to the outside world. \n3. Create routing rules to route to the specified page based on the URI in the request.",
What are the functions of the service grid module in DCE5.0?,"The service grid module in DCE5.0 has functions such as load balancing, traffic management, and fault recovery.",
What is the service grid module in DCE5.0?,"The service grid module in DCE5.0 is a system for managing microservices. It has functions such as load balancing, traffic management, and fault recovery. It can access services in the grid through configuration.",
"If JWT authentication is required, which field should be used in Condition?","If JWT authentication is required, the request.auth.claims field should be used in Condition.",
"In the Key field of Condition, which ones apply to HTTP and TCP protocols?","In the Key field of Condition, source.ip, remote.ip, source.namespace, source.principal, destination.ip and destination.port are available for HTTP and TCP protocols.",
"In Condition, what Key fields are supported?","In Condition, the following Key fields are supported: request.headers, source.ip, remote.ip, source.namespace, source.principal, request.auth.principal, request.auth.audiences, request.auth.presenter, request.auth .claims, destination.ip, destination.port and connection.sni.",
What is a policy condition?,"Policy condition Condition is one of the attributes that need to be set when adding policy restrictions in the service mesh. It is used to specify other required attributes, such as request headers, source IP address, destination IP address, etc.",
What is the service mesh module in DCE5.0?,"The service grid module in DCE5.0 is a service management tool that can automatically manage traffic control, security authentication, traffic management and other issues between different services, improving service reliability and scalability under the microservice architecture.",
What does the policy condition (Condition) refer to?,"Policy conditions (Condition) include HTTP request headers (request.headers), source IP address (source.ip), original client IP address (remote.ip), source load instance namespace (source.namespace), and source payload identifier. (source.principal), the authenticated principal request (request.auth.principal), the target subject of this authentication information (request.auth.audiences), the certificate issuer (request.auth.presenter), Claims come from JWT (request.auth.claims), destination IP address (destination.ip), port on the destination IP address (destination.port), server name indication (connection.sni), experimental metadata matching for filters (experimental .envoy.filters.*).",
What does the request operation (Operation) refer to?,"Request operation (Operation) includes hosts, ports, methods and paths.",
What does the request source (Source) refer to?,"The request source (Source) includes principals, namespaces, IP address (ipBlocks) and remote IP address (remoteIpBlocks).",
Can the service grid module in DCE5.0 add request sources and request operations?,"Yes, the service grid module in DCE5.0 can add request sources and request operations.",
What is the service grid module in DCE5.0?,The service grid module in DCE5.0 is a solution for managing inter-service communication in a microservice architecture.,
"In the request source field, what are principals?","In the Source field of the request, principals is a list of peer identities derived from the peer certificate.",
Are policy conditions optional? Can I add multiple policy conditions?,"Yes, policy conditions are optional settings and multiple can be added.",
"When the strategy action is selected as custom, which input item needs to be added?","When the policy action is selected as custom, the Provider input item needs to be added.",
What are the optional policy actions in policy settings?,"Policy actions include allow, deny, audit, and custom.",
What are the parameters for the basic configuration of the authorization policy?,"Basic configuration includes name, namespace, and workload label. Workload label includes label name and label value.",
"When adding a JWT rule, is the Issuer parameter required?",yes. The Issuer parameter is the issuer information in the JWT key information and must be filled in.,
What are the parameters for the basic configuration of requesting authentication?,"Basic configuration includes name, namespace, and workload label. Workload label includes label name and label value.",
What are the options for mTLS mode in authentication settings?,"The mTLS mode setting namespace options include UNSET, PERMISSIVE, STRICT and DISABLE, where UNSET means inheriting the parent option.",
What are the basic configuration parameters for peer authentication?,"Basic configuration includes name, namespace, and workload label. Workload label includes label name and label value.",
What are the modules of the service grid in DCE5.0?,"The service grid module in DCE5.0 includes peer identity authentication, request identity authentication, authorization policy, etc.",
How to confirm hosting grid control plane service?,"After ensuring that the grid status is normal, observe whether the Services under `istio-system` of the control plane cluster `mdemo-cluster1` are successfully bound to the LoadBalancer IP.",
What parameters need to be filled in when creating a grid?,"When creating a grid, you need to fill in parameters such as the hosting grid, a unique grid name, a grid version that meets the requirements, the cluster where the hosting control plane is located, the load balancing IP, and the mirror warehouse. \n### Expose the grid hosting control plane Hosted Istiod",
What components do I need to install if I want to have grid observation capabilities?,"If you want to have grid observation capabilities, you need to install the Insight Agent component. \n## Grid Deployment\n### Create Grid",
What remote key is required to join a cluster in the grid?,A cluster joining the grid needs to provide a remote key with sufficient permissions to allow Mspider to install components on the cluster and the Istio control plane to access the API servers of other clusters. \n### Multi-cluster regional planning,
What must the control plane cluster provide to other data plane clusters to access the control plane?,The control plane cluster must provide a reliable IP for other data plane clusters to access the control plane.,
What are the cluster type and version requirements?,The type and version of the current cluster must be clarified to ensure that the subsequently installed service grid can run normally in the cluster.,
What is the sidecar injection volume?,"Sidecar injection volume is a column of data showing the progress of sidecar injection, indicating how many processes have been successfully injected by the sidecar agent for a workload.",
How to clear the sidecar injection policy under a certain namespace?,"Users can select one or more namespaces that have the automatic injection function turned on, click the `Clear Policy' button in the namespace list, and confirm in the pop-up box to complete the operation.",
How to disable or turn off the automatic injection function under a certain namespace?,"Users can select one or more namespaces that have the automatic injection function turned on, click the `Injection Disable` button in the namespace list, and confirm in the pop-up box to complete the operation.",
How to enable the sidecar automatic injection function under a certain namespace?,"Users can select one or more namespaces that do not have the automatic injection function turned on, click the `Injection Enable` button in the namespace list, and confirm in the pop-up box to complete the operation.",
How to view sidecar injection information under namespace?,"In the left navigation bar, click `Sidecar Management` -> `Namespace Sidecar Management`. After clicking the corresponding cluster, you can view the sidecar status of all namespaces under the cluster.",
What is namespace sidecar management?,"Namespace sidecar management is part of the DCE5.0 service grid module. Users can enable, disable, or clear sidecar injection policies from the namespace level to control the sidecar status of workloads in the namespace.",
What functions does the service grid module of DCE5.0 have?,"The service grid module of DCE5.0 includes: namespace sidecar management, workload sidecar management, sidecar agent automatic upgrade, traffic management, black and white list management, and traffic mirroring.",
Under what circumstances do I need to install the istio CNI plugin?,"OpenShift 4.1 and above no longer uses iptables, but instead uses nftables, so the istio CNI plug-in needs to be installed.",
How to eliminate backend errors?,You need to activate iptables and perform the following operations:\n1. Modify the YAML file\n2. Add parameters\n3. Deploy istio-cni,
What problems occurred during the process of connecting to the backend?,"The following error occurred in the backend: \n```none\nCOMMIT\n2022-10-27T07:06:50.610621Z info Running command: iptables-restore --noflush /tmp/iptables-rules-1666854410610268141.txt1105821213\n2022-10 -27T07:06:50.616716Z error Command error output: xtables parameter problem:\niptables-restore: unable to initialize table ""nat""\nError occurred at line: 1\nTry `iptables-restore -h"" or ""iptables-restore - -help"" for more information.\n2022-10-27T07:06:50.616746Z error Failed to execute:\niptables-restore --noflush /tmp/iptables-rules-1666854410610268141.txt1105821213, exit status 2\n````",
How to connect to the Openshift cluster?,"First, you need to create a grid and then connect it to the Openshift cluster. Return to the grid list and you will find that you have successfully connected.",
How to add privileged user permissions to the namespace of the service grid?,"In the Openshift cluster, use the following command to add the privileged user permissions of the namespace to the service grid:\n```shell\noc adm policy add-scc-to-user privileged system:serviceaccount:istio-operator:istio-operator\ noc adm policy add-scc-to-user privileged system:serviceaccount:istio-system:istio-system\n````",
What is OCP?,OCP is a container platform launched by Red Hat.,
How to deploy the bookinfo application for testing?,"You need to create the bookinfo namespace and add SCC, enable namespace sidecar injection, and deploy the bookinfo application. You can view the bookinfo namespace labels and pod status through terminal commands.",
How to solve the resource parameter format error problem when connecting to the DCE5.0 service grid?,"This problem needs to be solved by modifying the yaml file, changing the memory resource parameter format to the correct format, and re-injecting it into the sidecar.",
How to solve the permission restriction problem when connecting to the DCE5.0 service grid?,You can solve the permission restriction problem by increasing the SCC user namespace permission "privileged" and restart istio-operator rs.,
What problems may you encounter when connecting to the DCE5.0 service grid?,"When connecting to the DCE5.0 service grid, you may encounter problems such as permission restrictions, resource parameter format errors, and sidecar injection failure.",
How to check whether each component is healthy?,"You can check whether each component is healthy through the graphical interface, or you can check the component status through the command line tool kubectl.",
What is the service grid module in DCE5.0?,"The service grid module in DCE5.0 is based on Istio's service grid implementation and implements functions such as traffic management, observability, and security control.",
How to resolve "istio-operator RS CreateFailed" error when creating a proprietary grid?,You can increase the SCC (security context constraint) user namespace permission "privileged" and restart istio-operator rs to solve the problem. The command is as follows:\n```sh\noc adm policy add-scc-to-user privileged -z istio-operator\noc adm policy add-scc-to-user privileged -z istio-system\noc adm policy add-scc -to-group privileged system:authenticated\n```,
What is Istio?,"Istio is an open source, cloud-native application management tool that provides traffic management, security, and monitoring solutions by injecting sidecars between applications. Istio supports cross-Kubernetes environments and other deployment environments.",
What are the advantages of the service grid module in DCE5.0?,The advantages of the service grid module in DCE5.0 include:\n- Simplify traffic management\n- Provide security\n- Improve monitoring efficiency\n- Support multiple applications and programming languages\n- Flexibly expand,
What is a sidecar in a service mesh?,"Sidecars in service mesh refer to proxies injected between applications, which can provide rich functions such as load balancing, routing, failure recovery, authentication and authorization, etc.",
What is the service grid module of DCE5.0?,"The service mesh module of DCE5.0 is a solution for managing microservice architecture. It can achieve traffic management, security, monitoring and other functions by injecting proxies (sidecars) between applications.",
How to query the global service cluster?,"Through the cluster list interface of container management, you can query the global service cluster by searching for `cluster role: global service cluster`.",
What is Metallb and how to install and configure it?,"Metallb is a Kubernetes package that implements load balancing. It can be installed through Helm or manually. After installing Metallb, you also need to configure the IP pool and add a designated IP to the corresponding service before it can be used.",
How to create a cluster through kind?,cluster can be created through the following command:\n```bash\ncat <<EOF > kind-cluster1.yaml\n...\nEOF\nkind create cluster --config kind-cluster1.yaml --name mdemo-kcluster1\ ncat <<EOF > kind-cluster2.yaml\n...\nEOF\nkind create cluster --config kind-cluster2.yaml --name mdemo-kcluster2\n```,
How to create a cluster through kubeadm?,The cluster can be created with the following command:\n```bash\nkubeadm init --image-repository registry.aliyuncs.com/google_containers \\n--apiserver-advertise-address=10.64.30.131 \\n--service-cidr =10.111.0.0/16 \\n--pod-network-cidr=10.100.0.0/16 \\n--cri-socket /var/run/cri-dockerd.sock\n````,
Which component does the service grid module in DCE5.0 need to rely on to provide observability capabilities?,The service grid module needs to rely on Insight-agent to provide observability capabilities.,
What is the service mesh module in DCE5.0?,The service mesh module is a module in DCE5.0 that is used to build microservice applications and manage communication between microservice applications.,
How to verify the network interoperability of the demo cluster?,You can arbitrarily select a cluster and execute the corresponding command line command to verify the network interoperability of the demo cluster.,
How to deploy helloworld and sleep applications?,"Helloworld and sleep applications can be deployed through the container management platform or using the command line. For details, please refer to the relevant deployment documents given in the article.",
How to add parameters to achieve cross-cluster traffic load balancing?,Cross-cluster traffic load balancing can be achieved by adding the `istio.custom_params.values.global.meshNetworks` parameter in YAML.,
What is GlobalMesh?,GlobalMesh is a custom Kubernetes resource type. The service grid module in DCE describes multi-cluster network interoperability by defining GlobalMesh resources.,
What is the service grid module in DCE5.0?,"The service grid module in DCE5.0 is a developer tool provided by DaoCloud, which can realize multi-cluster network interoperability in the corresponding container management platform (such as Kubernetes), also called multi-cluster service grid.",
How does Istio distinguish which network a service is on? How to configure network ID for worker cluster?,"Istio requires users to explicitly define a `network ID` for each working cluster when installing Istio, in order to distinguish the network where the service is located. To configure the network ID for the working cluster, you need to manually enter the global control plane cluster, search for the resource `MeshCluster`, find the working cluster that joins the grid, and add the parameter `istio.custom_params.values.global.network` to it, and its value is according to the original Configure the network ID in the planning form.",
What is the East-West Gateway? Why do I need to install an East-West gateway?,"The East-West Gateway is a solution provided by Istio to solve network problems that occur when services communicate across clusters. When the target service is on a different network, its traffic will be forwarded to the east-west gateway of the target cluster, which will parse the request and forward the request to the real target service.",
How to add a worker cluster?,"Add a cluster on the graphical interface of the service grid, select the working cluster to be added and wait for the cluster installation to complete.",
How to create a grid?,"First, on the grid management page, select Create Grid, then enter a unique grid name, select the preselected grid version and cluster that meet the requirements according to the prerequisite environment, and finally configure the load balancing IP and mirror warehouse.",
What are the key capabilities of the service grid module in DCE5.0?,"The key capabilities of the service grid module in DCE5.0 include observability, cross-cluster traffic management, security policy, service registration and discovery, traffic control, etc.",
What is mTLS authentication?,"mTLS stands for Mutual Transport Layer Security, which is two-way transport layer security authentication. It allows communicating parties to authenticate each other during the SSL/TLS handshake. The service grid of DCE5.0 establishes a service-to-service communication channel through client and server-side PEP, and uses a two-way TLS handshake for transmission authentication.",
How does a service mesh use request authentication?,"The service grid uses JSON Web Token (JWT) authentication to enable request-level authentication and supports multiple OpenID Connect authentication implementations, such as ORY Hydra, Keycloak, Auth0, Firebase Auth, and Google Auth.",
What is Istio Agent?,The Istio Agent is an agent that runs with each Envoy proxy and works with istiod to automate large-scale key and certificate rotation.,
How does the service mesh module provide identity and authentication?,The service mesh module uses X.509 certificates to provide strong identity for each workload and provides peer and request authentication.,
What is a service mesh module?,Service mesh modules are a way to separate applications and simplify the relationship between applications and the network by abstracting communication into interactions between different services.,
The control plane cluster needs to manage the grid policies of all clusters. What problems may exist?,"There may be namespaces that pollute the control plane cluster, or some grid resources may be affected by operations on the cluster itself (for example, a namespace is deleted).",
What are the advantages of service mesh technology selection solutions?,#NAME?,
What is the role of virtual clusters in service mesh architecture?,"The virtual cluster is connected to the grid control plane, and the purpose is to isolate the grid resources from the resources of the control plane cluster to avoid the generation of dirty resources and the problem of contaminating multiple control plane clusters.",
What are the advantages of the architecture selection scheme of the service grid module in DCE5.0?,"The architecture selection scheme of the service grid module in DCE5.0 includes a free network model and a single control plane for multiple clusters. Its advantages include simpler management, low-complexity configuration, better performance, stronger security, and support. Cross-cluster load and disaster recovery capabilities, etc.",
"In a multi-cluster scenario, how to achieve cross-cluster load capacity?","You can do this by not exchanging the API Server remote keys of the cluster, so that you can only perform service discovery within your own cluster; or by configuring the VirtualService and DestinationRule policies to disable the traffic load between multiple clusters.",
Why is service discovery more complicated in a multi-cluster scenario?,"In a multi-cluster scenario, due to the isolation of registration centers between clusters, service discovery needs to observe and record the services of the registration center in each cluster, aggregate services with the same name in the same namespace, and sort them into a complete grid. The list of service endpoints can only be provided to the sidecar.",
What are the core dimensions of the service mesh module?,"The core dimensions of the service mesh module include network model, control plane architecture and data plane service discovery.",
 What is the service mesh module in DCE5.0?,"The service grid module in DCE5.0 is a technical solution that can govern and manage services. It can implement functions such as flow control, fault recovery, and security policies. It also supports service discovery and cross-cluster traffic load in multi-cluster scenarios. and other abilities.",
What functionality does WasmPlugin provide?,`WasmPlugin can provide extended functions for the Istio proxy through WebAssembly filters. The filter capabilities it provides can form complex interactions with Istio internal filters. `,
What are Telementry resources?,"The `Telementry resource` defines how to generate telemetry for workloads in the grid, and provides Istio with the configuration of three observability tools: indicators, logs, and full-link tracking.",
What common types of security governance resources does Istio provide?,"Common security governance resource types provided by `Istio` are: `AuthorizationPolicy`, `PeerAuthentication` and `RequestAuthentication`.",
What common types of traffic governance resources does Istio provide?,"Common traffic management resource types provided by `Istio` are: `DestinationRule (destination rule)`, `EnvoyFilter`, `Gateway (gateway rule)`, `ProxyConfig`, `WorkloadEntry`, `ServiceEntry (service entry)`, `SideCar ` and `VirtualService (virtual service)`.",
What is the Istio resource management page?,"The `Istio resource management page` lists all Istio resources by resource type, providing users with the ability to display, create, edit, delete various resources, etc.",
How to check the Istio version number of a mesh?,"After the grid upgrade is completed, return to the grid list page and you can see that the Istio version of the grid has changed.",
Are there any other operations that can be performed during the upgrade process?,"The upgrade process cannot be terminated once started, and it is recommended not to perform any setup operations on the grid during the upgrade.",
How to deal with environment detection failure?,"If the environment detection fails, it may be that the cluster (k8s) version is too low or too high. You need to deal with the cluster (k8s) problem first. If the cluster (k8s) version is too low, you can upgrade the cluster (k8s) version on the container management platform first, and then click the `Recheck' button; if the cluster (k8s) version is too high, it is recommended to go back to ""Select target version"" and choose another Later versions of Istio.",
How to upgrade Istio version?,"When a newer Istio version exists in the system image repository, an exclamation mark icon will appear on the upgradeable grid card in the `Grid List`. Click the `Upgrade Now` button to enter the upgrade wizard. The upgrade wizard consists of three steps: `select target version`, `environment detection`, and `perform upgrade`. For specific steps, please refer to the text.",
What two types of Istio version upgrades does DaoCloud Grid support?,DaoCloud Grid supports two types of Istio version upgrades: native version and customized version.,
How to access the documentation of the DCE5.0 service grid module?,The document link is https://docs.daocloud.io/mspider,
What information should I pay attention to when adding a cluster?,The process of adding a cluster will last for a few seconds. Please pay attention to the cluster statistics information in the grid list to understand the changes in the number of normal clusters.,
Under what circumstances cannot a cluster be added to a DCE5.0 hosting grid?,Clusters cannot be added to a proprietary grid; only clusters with a status of 'Running' can be added to the grid. Clusters in other statuses cannot be selected or added.,
What state is the grid in after adding the cluster?,The grid is in `Not Ready` state after adding the cluster.,
How to add a cluster to a DCE5.0 hosting grid?,"Click `Cluster Management` in the left navigation bar, click the `Add Cluster` button, check the cluster to be added, and click the `OK` button to complete the cluster addition operation.",
What functions are supported for cluster management on the UI interface for managed grids and private grids?,Cluster management on the UI interface supports access and removal functions for hosted grids and private grids.,
What three types of meshes does the DCE service mesh support? What features are supported on each grid UI interface?,"DCE service grid supports three types of grids: managed grid, dedicated grid, and external grid. \n- On the UI interface, the supported functions of managed grid, dedicated grid and external grid are shown in the table below: \n| Supported functions | Hosted grid | Private grid | External grid | \n| :- -: | :------: | :------: | :------: |\n| Grid Management| √ | √ | √ |\n| Grid Overview| √ | √ | √ |\n| Cluster management (access/removal)| √/√| ×/×| ×/×|\n| Service management (service list/details/edit/entry)|√/ √/√/√|√/√/√/√|√/√/√|\n| Traffic governance (virtual service/target rule)| √ / √ / √ | √ / √ / √ | √ / √ / √ |\n| Security governance (peer identity authentication/request identity authentication/authorization policy) | √ / √ / √ ｜∏\ // Text layout problem, it should be three ✓, but there is a problem with the display\n｜ ✓ |\ n| ✓ |\n| ✓ |/✓ |/✓ |/✓ |\n| Traffic monitoring (topology/charts) | √ / √ | √ / √ | √ / √ |\n| Sidecar management (injection/ Offload/Resource Limitation) | √ / √ / √ | √ / √ / × |√ /√/× |\n| Grid Gateway (Create/Delete/Set) |√/√/√|√/√/√ | /× |/× |/×|\n| Istio resource management (create/edit/delete) | √ / √ / √ | √ / √ / √ |∏\ // Text layout problem, it should be three ✓, but There is a problem with the display\n｜ ✓ |\n｜ ✓ |\n｜ ✓ |/✓ |/✓ |/✓ |",
How to avoid disruption when updating authentication policies?,"PERMISSIVE mode can be used to transition changes to a peer authentication policy from DISABLE to STRICT and vice versa; when migrating a request authentication policy from one JWT to another, new JWT rules are added to the policy without being deleted Old rules.",
How does a service mesh apply the narrowest matching policy?,"The service mesh applies the narrowest matching policy to each workload in workload-specific, namespace-scoped, and mesh-scoped order.",
What is the `Request Authentication` policy in a service mesh? What validation rules are specified?,"The `Request Authentication` policy specifies the values required to authenticate a JSON Web Token (JWT), including the token's location in the request, the issuer of the request, and the public JSON Web Key Set (JWKS).",
What is the `Peer Authentication` strategy in a service mesh? What modes are supported?,"The `Peer Authentication` policy specifies the two-way TLS mode implemented by the service mesh on the target workload. The supported modes are: PERMISSIVE, STRICT, DISABLE, and UNSET.",
How to verify whether multi-cluster grayscale release is successful?,You can access the Ingress grid load balancing address through a browser and confirm whether the access ratio between its v1 and v2 versions is consistent with the ratio in the above policy.,
How to deploy grayscale application strategy?,"First, you need to create a DestinationRule, define the business versions of different clusters by defining SubSet, and turn on the Istio two-way TLS mode; then you need to expose the service through ingress, create gateway rules and virtual service rules, and configure the virtual service rules required to access the service.",
How to create a demo application and verify traffic?,"You need to enable sidecar injection in the namespace, select the corresponding cluster and namespace and configure basic workload information, including image version, access type, port configuration, etc., and add corresponding container group labels in the container management platform to distinguish different versions. workload.",
What preparatory work is required for multi-cluster grayscale release?,You need to refer to the grid multi-cloud deployment document to build the grid infrastructure.,
What is multi-cluster grayscale release?,Multi-cluster grayscale release implements traffic control and version updates by deploying different versions of business applications in different clusters and configuring corresponding policies on the service grid to adjust traffic between business versions.,
What are the functions of the service grid module in DCE5.0?,"The service grid module in DCE5.0 provides functions such as traffic management, security management, fault recovery, and grayscale publishing.",
What function is Istio Envoy responsible for in the service mesh?,"Istio Envoy is deployed as a Sidecar container in each application Pod in the service mesh. It is responsible for proxying all traffic between applications and can provide functions such as traffic monitoring, troubleshooting, and A/B testing.",
What function is Istio Mixer responsible for in the service mesh?,"Istio Mixer is responsible for functions such as policy control and telemetry data collection in the service mesh. It can control all traffic in and out of the cluster, and can send traffic data to monitoring systems such as Prometheus or Zipkin.",
What function is Istio Pilot responsible for in the service mesh?,"Istio Pilot is responsible for traffic management, security, routing and other functions in the service mesh. It collects and distributes load balancing policies for communication between services and converts them into configurations that Istio Envoy can understand.",
What are the core components of the service mesh module in DCE5.0?,"The service mesh module in DCE5.0 has the following three core components: Istio Pilot, Istio Mixer and Istio Envoy.",
What is the service mesh module in DCE5.0?,"The service grid module in DCE5.0 is an infrastructure used to manage application communication. It abstracts the communication relationships between different services into a grid and is responsible for traffic management, security, monitoring, troubleshooting and other functions.",
What are some common failure cases you might encounter when creating a service mesh?,"When creating a service grid, you may encounter the following common failure cases: the cluster cannot be found, the creation is always ""creating"" but ultimately fails, the created grid is abnormal but the grid cannot be deleted, the managed grid fails to manage the cluster , istio-ingressgateway exception occurs when the hosting grid manages the cluster.",
Can the global cluster of DCE 5.0 be used to create a hosted mode grid?,"Can't. The global cluster of DCE 5.0 contains Istio-related components that global management depends on, and it is not allowed to use this cluster to create a managed mode grid. However, external mesh creation can be used during demos.",
Where are the virtual clusters created when creating a hosted grid?,"When creating a managed grid, a managed control plane virtual cluster API Server is created in the managed control plane cluster to hold the Istio CRD resources.",
What are the deployment modes of the service grid module in DCE 5.0?,"There are three deployment modes for the service grid module in DCE 5.0: hosted mode, proprietary mode and external mode.",
What is the service mesh module in DCE 5.0?,"The service mesh module in DCE 5.0 is a network architecture used to manage, secure and interconnect microservices. It is an observable, controllable and scalable communication layer managed by Istio.",
How to verify after successful installation?,"You can view the Pod information under the namespace `mspider-system`. If the relevant Pod has been created and running, it means that the service grid is installed successfully.",
Which parameter needs to be replaced in the installation command?,You need to replace `0.0.0-xxx` with the service mesh version number you plan to install.,
How to install the service grid in DCE5.0?,"You can select the cluster where you want to install the service grid from the `Container Management` platform, and execute a series of commands in the cluster to install it.",
What is the service mesh in DCE5.0?,"The service mesh in DCE5.0 is a lightweight, distributed application network architecture that provides observability, reliability, and security between services and makes it easier for applications to to microservices.",
What telemetry can Telementry generate for workloads within the grid?,"Telementry can generate configurations of three observability tools: metrics, logs, and full-link tracking for workloads within the grid.",
What is WasmPlugin used for?,WasmPlugin provides extended functions for the Istio proxy through WebAssembly filters. The filter capabilities it provides can form complex interactions with Istio internal filters.,
What security authentication methods does PeerAuthentication provide?,"PeerAuthentication provides two-way secure authentication between services and supports operations such as automatic creation, distribution, and rotation of keys and certificates.",
What requests can AuthorizationPolicy be used for?,AuthorizationPolicy can be used for external requests and requests between services within the grid.,
What routing support does VirtualService provide?,"VirtualService provides routing support for HTTP, TCP, and TLS protocols, and can implement routing and forwarding of requests from different regions and users through a variety of matching methods (port, host, header, etc.).",
What is SideCar?,"A SideCar is a container that can run alongside an application container to handle the inbound and outbound traffic of the application and provide some additional features such as load balancing, failure recovery, etc.",
What functions does the service mesh module in DCE5.0 provide?,"The service grid module in DCE5.0 provides functions such as traffic management, security governance, proxy expansion, and system settings.",
How to implement flow control in Istio?,"In Istio, flow control can be achieved through Mixer. Users can implement rate limiting, circuit breaker and other operations on requests by defining the relevant configuration of Mixer.",
What is Istio's sidecar?,Istio's Sidecar refers to a lightweight Envoy Proxy instance that is automatically injected into the microservice container and is responsible for handling all communications between microservices.,
How to install the service grid module in DCE5.0?,"Through the ""Add Cluster"" function of the DCE console, check ""Enable Service Grid"" and select ""Enable Istio"" to install the service grid module in DCE5.0.",
What is Istio Telemetry?,"Istio Telemetry is a component used to collect, store and display microservice monitoring and tracking data, including sub-components such as Mixer and Prometheus.",
What is Istio Galley?,Istio Galley is a component that runs on the control plane and is used to validate and transform user-submitted configuration files and distribute them to various data planes.,
What is Istio Citadel?,"Istio Citadel is a component used for certificate management and security access control. It can configure automatic certificate signing, verification and revocation functions for Envoy in microservices.",
What is Istio Ingress Gateway?,"Istio Ingress Gateway is a component used to expose external traffic of microservices to the inside of the cluster. It supports HTTP, HTTPS, gRPC and TCP traffic.",
What is Istio Pilot?,"Istio Pilot is a core component of the data plane, used to manage and configure Envoy's routing and load balancing functions in microservices.",
What are the subcomponents of the service mesh module in DCE5.0?,The service mesh module in DCE5.0 contains the following subcomponents:\n- Istio Pilot\n- Istio Ingress Gateway\n- Istio Citadel\n- Istio Galley\n- Istio Telemetry,
What is the service grid in DCE5.0?,The service grid in DCE5.0 is a componentized solution built on Istio to solve microservice governance issues.,
How to force deletion of an abnormal service mesh created?,"If you want to force deletion of the abnormal service mesh, please perform the following operations:\n1. Disable sidecar injection of the managed cluster;\n2. Delete the created mesh gateway instance;\n3. Remove the cluster;\n4. Return to the service Grid, delete the grid instance.",
How to troubleshoot service mesh failure?,It is recommended to investigate the cause of the specific service mesh failure and resolve it.,
Under what circumstances will the created service mesh fail to be deleted?,"When the grid is in a failed state, when a cluster is managed, a grid gateway instance is created, or sidecar injection is enabled, deletion of the service grid will fail and the service grid cannot be deleted normally.",
What is the difference when traditional microservices join the service mesh?,"Traditional microservices joining the service grid only need to inject their corresponding workloads into the sidecar so that the grid can manage them. Based on different microservice frameworks and the non-intrusive nature of the grid, the governance strategies of the original framework may change, but there is no need for users to implement them themselves. When connected to the grid, traditional microservices can obtain all the capabilities of the grid, including traffic management, security, observability, etc.",
"If different namespace sidecar injection strategies and workload sidecar injection strategies are set, what is the actual execution effect?","Matching will be done in the following order of rules. If a valid rule is matched, subsequent rules will not be executed:\n- If one of the two is set to disabled, sidecar injection will be disabled\n- If one of the two is set to enabled, then it will be enabled Sidecar injection\n- If neither is set, the grid global sidecar injection strategy (values.sidecarInjectorWebhook.enableNamespacesByDefault) will be executed\nFor details, please refer to [Installing Sidecar](https://istio.io/latest /zh/docs/setup/additional-setup/sidecar-injection/)",
Is the 4th generation service mesh (DSM) upgradeable to the 5th generation service mesh?,"It cannot be upgraded, but it can be connected to the service grid in the form of an external grid. It is recommended to reinstall to obtain the best scalability and later maintenance.",
Can sidecars injected into the grid be upgraded?,"Yes, sidecar hot upgrade is also supported.",
Can Istio for service mesh be upgraded?,"Yes, users can upgrade independently.",
What are the Istio component versions in the service mesh?,There are multiple versions that the user can choose from when creating the grid.,
Can Istio in a service mesh be installed separately?,"Yes, but not recommended. Istio control plane components installed separately by users can be connected to the service grid through an external grid, but compared with other modes, they only provide basic Istio functions.",
What plug-ins are supported by the service mesh in DCE5.0?,"The service mesh in DCE5.0 supports Tracing, Prometheus, Kiali and Grafana plug-ins.",
Is the service grid module of DCE5.0 compatible with the universal service grid?,"Yes, the service grid module of DCE5.0 is fully compatible with the universal service grid.",
"In DCE5.0, which microservice frameworks can be used to connect to the service grid?","In DCE5.0, services developed by Spring Cloud and Dubbo SDK can be used to connect to the grid without intrusion and manage them uniformly.",
What functions can be achieved by the multi-cluster mode in DCE5.0?,The multi-cluster mode in DCE5.0 can achieve unified management and scalability of multi-cluster configurations.,
What observability features does the service mesh support in DCE5.0?,"The service grid in DCE5.0 supports traffic topology, service operation monitoring, access logs and call chains.",
What security features does the service mesh support in DCE5.0?,The service grid in DCE5.0 supports transparent two-way authentication and fine-grained access authorization.,
What traffic management functions does the service grid support in DCE5.0?,"The service grid in DCE5.0 supports seven-layer connection pool management, four-layer connection pool management, outlier detection, retry, timeout, load balancing, HTTP Header operation and fault injection.",
What TLS modes does Istio gateway rule support?,"Istio gateway rules support three TLS modes: ISTIO_MUTUAL, SIMPLE, and PASSTHROUGH.",
What information does the Servers field in the gateway rule include?,"The Servers field includes information related to externally exposed services, including hosts (service name), listening port, protocol type, etc.",
What is the role of the Selector field in gateway rules?,The Selector field is used to select the istio gateway for north-south traffic. You can use multiple istio gateways or share one with other rules.,
What are Istio gateway rules?,"Istio gateway rules (Gateway) are used to expose services outside the grid. Compared with Kubernetes' ingress object, istio-gateway adds more functions, including L4-L6 load balancing, external mTLS, SNI support, and Other internal networking features already implemented in Istio.",
What should I pay attention to when deleting in batches?,"The batch deletion operation will delete all selected items at the same time and cannot be restored, so please operate with caution.",
How to delete multiple grid gateways in batches?,"Click `Grid Gateway` in the left navigation bar, select multiple gateways to be deleted in the list, and click the trash can icon in the upper right corner to delete them in batches.",
What are the effects of deleting a grid gateway?,"After deleting a grid gateway, information related to the gateway will be lost, so you need to operate with caution.",
How to avoid accidental deletion?,"It is recommended to select the gateway to be deleted first, and then click the `...` button to delete it.",
How to delete a grid gateway?,"Click `Grid Gateway` in the left navigation bar, click the `...` button on the right side of the grid list, and select `Delete` in the pop-up menu. If you need to delete multiple gateways, you can check multiple gateways in the gateway list and click the trash can icon in the upper right corner.",
What prerequisite operations need to be completed to delete the grid?,"Depending on the grid type, different pre-operations need to be completed. To delete an external grid, you only need to confirm the deletion; to delete a dedicated and hosted grid, you need to remove the sidecar, clear the gateway, remove the cluster, etc.",
How to delete grid?,"Select the corresponding grid in the grid list, click `...` on the right, select `Delete` in the pop-up menu, complete the pre-operation according to the grid type, enter the confirmation code and click the `Delete` button to delete grid.",
How to create target rules? What are the ways to create it?,"Target rules can be created through graphical wizards and YAML creation. The graphical wizard requires the following operations: Click `Traffic Governance` -> `Target Rules` in the left navigation bar, click the `Create` button in the upper right corner, select the policy type after performing basic configuration in the interface, and configure the corresponding governance policy Then, return to the target rule list. The YAML creation method allows you to create YAML files directly with the help of built-in templates.",
What is outlier detection? What role does it play in the target rules?,"Outlier detection is a design pattern that reduces service anomalies and service delays. It mainly handles service anomalies insensitively and ensures that cascades or even avalanches will not occur. In the target rule, if the cumulative number of errors in the service within a certain period of time exceeds the predefined threshold, the service with the error will be removed from the load balancing pool, and the health status of the service will continue to be monitored. When the service returns to normal Afterwards, the service will be moved back to the load balancing pool.",
What load balancing models are supported by target rules?,"Target rules use the round-robin load balancing strategy by default. It also supports random, weighted, least request and other load balancing models.",
How do target rules perform set cutting of service endpoints?,The traffic of a service can be divided into N parts for the client to use in different scenarios. This service is called `subsets` and is defined based on one or more `labels` based on the `host` corresponding service. In Kubernetes it are key/value pairs attached to objects like Pods. These labels are applied to the Deployment of the Kubernetes service and serve as metadata information (Metadata) to identify different versions.,
What are target rules?,"Target rules are an important part of service governance. They divide request traffic through ports, service versions, etc., and customize envoy traffic policies for each request traffic. The policies applied to traffic include not only load balancing, but also minimum connections. numbers, outlier detection, etc.",
What basic information needs to be filled in the external grid?,"The following basic information needs to be filled in for the external grid:\n-Grid name: starts with a lowercase letter, consists of lowercase letters, numbers, and a dash (-), and cannot end with a dash (-). \n- Cluster: The cluster used to run the grid management plane. \n- Istio root namespace: Specifies the Istio root namespace where the grid is located. \n- Grid component warehouse: Enter the URL address of the mirror warehouse.",
How to create an external grid in DCE5.0?,"In the upper right corner of the service grid list page, click ""Create Grid"", select ""External Grid"", fill in the basic information of the grid, system settings, governance settings, sidecar settings and other options, and click OK to create an external network. grid.",
What is the service mesh in DCE5.0?,"The service mesh in DCE5.0 is a set of programmable network infrastructure used to simplify application communication, security and monitoring in microservices architecture.",
What is the data surface?,"The data plane refers to the data flow processing layer composed of Envoy Sidecar. In Istio, Envoy Sidecar is used to control the flow of microservices.",
What is a control surface?,"The control plane refers to the components in Istio used to manage all Envoy Sidecars, including Pilot, Mixer, Citadel, etc.",
What is Istio?,"Istio is an open source service mesh jointly developed by Google, IBM, Lyft and other companies.",
What is the service grid module of DCE5.0?,"The service grid module of DCE5.0 is a microservice application management system based on Istio, which provides functions such as traffic management, security control, fault recovery and observability.",
What are the environmental dependencies mentioned in the article?,Environmental dependency means that helm application Metrics Server has been installed in the cluster.,
How is the HPA auto-scaling policy calculated?,"The HPA auto-scaling policy is calculated based on the request values of all Pod resources/workloads under the workload. When the actual CPU usage is greater/less than the target value, the system automatically reduces/increases the number of Pod copies.",
How to create an auto scaling policy?,"Users can select the corresponding cluster in container management, enter the Workload -> Stateless Load page, find the object for which the policy is to be created, click the workload name to enter the elastic scaling tab, click the Edit button to configure the parameters and confirm to complete the creation. Among them, the target CPU utilization and target memory usage can be used as configuration parameters of the policy.",
What elastic scaling methods does the service grid module of DCE5.0 provide?,"The service grid module of DCE5.0 provides three elastic scaling methods: index scaling (HPA), scheduled scaling (CronHPA), and vertical scaling (VPA).",
What is the service mesh module in DCE5.0?,"The service grid module in DCE5.0 is a cluster component that provides microservice management, orchestration and governance functions.",
What indicators does the indicator data information of the HTTP service in the sidebar of the traffic topology module include?,"Indicators include: request rate (RPM), error rate (%), and average delay (ms).",
"When you click on any service in the traffic topology module, what information will pop up?",A sidebar will pop up to display service-related indicators based on protocol type.,
What are the health states in the traffic topology module? What colors do they correspond to?,"Divided into normal, warning, abnormal and unknown. Normal (grey), warning (orange), abnormal (red), unknown (dashed line).",
What methods can be selected to filter and display service nodes in the traffic topology module?,You can choose the view mode and namespace.,
Which module in DCE5.0 can display the topological relationship of all services under the grid?,Traffic topology module.,
"In a service mesh, which component is primarily responsible for sidecar lifecycle management?",istiod.,
What is the purpose of mspider-mcpc-mcpc-controller in the control plane cluster?,mspider-mcpc-mcpc-controller is used to aggregate grid multi-cluster related data plane information.,
What is istiod-{meshID}-hosted used in the control plane cluster?,istiod-{meshID}-hosted is used for policy management of hosted meshes.,
What are the control plane components of the service mesh?,"The control plane components of the service grid include mspider-ui, mspider-ckube, mspider-ckube-remote, mspider-gsc-controller, mspider-api-service, etc.",
What are service mesh control plane components?,"The service grid control plane component is a part of the service grid and is used to manage and control the entire service grid life cycle, including grid creation, configuration, and permission management.",
What is the difference between mesh gateway and sidecar?,"Grids run as individual instances, while sidecars are deployed and run inside each application instance.",
How to create a grid gateway?,"1. Click `Grid Gateway` in the left navigation bar to enter the gateway list and click the `Create` button in the upper right corner. \n2. Click `OK` after configuring various parameters. \n3. The status of the newly created grid gateway is `Creating`, and the status will change to `Running` in a few seconds.",
What is the role of grid gateway in DCE5.0?,"Grid gateways are divided into two categories: Ingress and Egress, which are used to define the traffic inlet and outlet of applications within the service grid to achieve precise traffic control.",
What are the specific steps for customizing?,"1. View the managed grid access cluster. \n2. Find istiod in the workload stateless load page in the container management module. \n3. Enter the container configuration basic information tab page. \n4. Modify the CPU and memory quotas, click Next and OK. \n5. Check the Pod resource information under the workload and confirm that the quota has been modified.",
Take istiod on the working cluster under which service grid as an example?,istiod on the worker cluster under the hosting grid.,
What are the prerequisites for customization?,"The cluster has been managed by the service grid, and the grid components have been installed normally; the login account has admin or editor permissions for the global management cluster and the namespace istio-system in the working cluster.",
What is this article about?,This article describes how to customize service mesh component resources.,
How about a reference YAML example for authorization policies?,The reference YAML example of the authorization policy is as follows:\n````yaml\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\nname: "ratings-viewer"\nnamespace: default\nspec:\nselector:\ nmatchLabels:\napp: ratings\naction: ALLOW\nrules:\n- from:\n- source:\nprincipals: ["cluster.local/ns/default/sa/bookinfo-reviews"]\nto:\n- operation :\nmethods: ["GET"]\n```,
What is an authorization policy?,"The authorization policy is similar to a four-layer to seven-layer ""firewall"". It analyzes and matches the data flow, and then performs corresponding actions. Authorization policies apply regardless of whether the request comes from internal or external sources.",
What are the advantages of the service mesh in DCE5.0?,"The service grid in DCE5.0 has the following advantages:\n1. Observability: It can realize real-time monitoring and analysis of traffic, performance, errors and other issues in the microservice architecture. \n2. Reliability: The reliability of the microservice architecture can be improved by introducing elastic mechanisms, timeout control, circuit breakers, etc. \n3. Security: Security mechanisms such as encryption, authentication, and authorization can be implemented to ensure communication between microservices. \n4. Maintainability: Able to effectively manage and maintain various components in the microservice architecture, improving development and operation and maintenance efficiency.",
What are the components of the service mesh in DCE5.0?,The service mesh in DCE5.0 consists of the following components:\n- mcpc-controller\n- reg-proxy\n- ckube-remote\n- hosted-apiserver.etcd\n- hosted-apiserver.apiserver\n- istiod \n- gateway,
What are the grid sizes in DCE5.0? How much component resources do each require?,"The grid sizes in DCE5.0 are divided into three types: small, medium and large. The component resource usage they require is as follows:\nSmall grid:\n- mcpc-controller: memory request 50Mi, memory limit 1600Mi, CPU request 100m, CPU limit 300m\n- reg-proxy: memory request 50Mi, memory limit 300Mi, CPU request 100m, CPU limit 300m\n- ckube-remote: Memory request 50Mi, memory limit 500Mi, CPU request 100m, CPU limit 500m\n- hosted-apiserver.etcd: Memory request 100Mi, memory limit 1000Mi, CPU request 100m, CPU limit 500m\n- hosted-apiserver.apiserver: memory request 100Mi, memory limit 1000Mi, CPU request 100m, CPU limit 1000m\n- istiod: memory request 200Mi, memory limit 1500Mi, CPU request 200m, CPU limit 1500m\ n- gateway: memory request 50Mi, memory limit 900Mi, CPU request 50m, CPU limit 1000m\nMedium grid:\n- mcpc-controller: memory request 200Mi, memory limit 1600Mi, CPU request 100m, CPU limit 600m\n- reg-proxy: memory request 100Mi, memory limit 600Mi, CPU request 100m, CPU limit 500m\n-ckube-remote: memory request 200Mi, memory limit 1000Mi, CPU request 200m, CPU limit 1000m\n- hosted-apiserver.etcd: Memory request 200Mi, memory limit 1000Mi, CPU request 100m, CPU limit 500m\n- hosted-apiserver.apiserver: Memory request 200Mi, memory limit 1000Mi, CPU request 100m, CPU limit 1000m\n- istiod: Memory request 500Mi, memory limit 2000Mi, CPU request 400m, CPU limit 2000m\n- gateway: memory request 150Mi, memory limit 1500Mi, CPU request 300m, CPU limit 2000m\n Large grid: \n- mcpc-controller: memory request 300Mi, memory limit 2000mi, CPU request 300mi, CPU limit 1000mi\n- reg-proxy: Memory request 200mi, memory limit 1000mi, CPU request 100mi, CPU limit 1000mi\n- ckube-remote: Memory request 400mi, memory limit 2000mi; CPU request 200mi; CPU limit 500mi;\n-hosted-api-server.etcd: kernel application 300M; kernel maximum usage 15000M; cpu application/maximum usage are: (1/10 core)\n-hosted-api-server.apiserver: kernel application 300M ; The maximum kernel usage is 15000M; CPU application/maximum usage are: (4/10 cores)\n- istiod: memory request 800Mi, memory limit 3000Mi, CPU request 600M, CPU limit 3000m\n- gateway: memory request 500Mi, memory Limit 2000Mi, CPU request 600m, CPU limit 3000m",
What is a service mesh?,"service mesh is an infrastructure layer used to manage microservices architecture, enabling better observability, reliability, and security by separating network logic from applications.",
How to turn off the multi-cloud network interconnection function?,"In the `Basic Settings' area at the top, click the slider in the `Enabled' state to confirm the prerequisites for closing and then enter the confirmation text to close it.",
How to delete a network group?,Click `Delete Network Group` in the drop-down box of the network group operation that needs to be deleted to confirm and delete it.,
How do I remove a network group interconnect?,"Select the network group that needs to be removed, click the Remove Internet button to confirm and remove it.",
How do I update the east-west gateway address for a network grouping?,"Check a group in the interconnection list, click `Update East-West Gateway Address`, enter the new East-West Gateway Address and click `OK` to update.",
How to add network group interconnection in DCE5.0?,"Click the `Join Internet` button, check a network group, select the available `East-West Gateway Address`, and click `OK` to join the network group interconnection.",
What is the introduction of the service grid module in DCE5.0?,"The service grid module in DCE5.0 provides network group interconnection functions, including operations such as joining interconnections, updating east-west gateway addresses, removing interconnections, deleting network groups, and closing multi-cloud network interconnections.",
How to turn off the multi-cloud network interconnection function?,"In the Basic Settings area at the top, click the Enabled status slider and follow the prompts.",
How to delete a network group?,Click `Delete Network Group` in the drop-down box of the network group operation that needs to be deleted. Follow the prompts.,
How to remove interconnection?,Select the network group that needs to be removed and click the `Remove Internet` button. Follow the prompts.,
How to update the east-west gateway address?,"Select a network group that has joined the Internet in the Internet list, click the `Update East-West Gateway Address' button, select the address to be updated and confirm.",
How to add a network group to the Internet?,Click the `Join Internet` button and check the network group that needs to be added to the Internet. Follow the prompts.,
How to create/delete east-west gateway?,"East-West gateways are used for communication between network groups. One or more gateways can be created on any cluster within a group. Click the `⋮` on the right side of a cluster and select `Edit East-West Gateway`. After filling in the relevant configuration, click OK. Select an East-West gateway and click `Delete East-West Gateway` in the operation to delete the gateway.",
How to add or delete clusters?,"After creation, you can add more clusters to the group. Select the place that needs to be added or deleted.",
How to create a new network group?,Enter a grid and click the enable switch to automatically create gateway rules for the east-west gateway. Click the `Create Network Group` button to add at least one cluster to the network group. Fill in a name for the network group and add at least one cluster.,
What terms need explanation in multi-cloud network interconnection?,"Network grouping, east-west gateway, basic settings, network grouping list, interconnection list.",
What scenarios are multi-cloud network interconnection suitable for?,"Multi-cloud network interconnection is suitable for situations where there are multiple clusters in the same grid, and the Pod networks of the multiple clusters cannot be directly connected (including Pod CIDR conflicts, etc.).",
What is multi-cloud network interconnection?,Multi-cloud network interconnection is a set of solutions provided in multi-cluster mode when the networks between multiple clusters are not connected to each other (Pods cannot directly establish communication). It can quickly connect the networks of multiple clusters and realize cross-cluster Pods. visit each other.,
How to solve the problem of service grid module in DCE5.0?,The solutions are as follows: 1) Deploy StorageClass; 2) Deploy metalLB; 3) Restart the global cluster gsc controller; 4) Modify fs.inotify.max_user_instances.,
What are the problems with the service grid module in DCE5.0?,"In the service grid module of DCE5.0, the managed grid may fail to create a high-availability ETCD because the control plane cluster does not deploy StorageClass in advance. The managed grid istiod-xxxx-hosted-xxxx component is abnormal and mspider-mcpc-ckube-remote-xxxx Component abnormalities and other issues.",
What is the service mesh module in DCE5.0?,"The service mesh module in DCE5.0 is an inter-container communication solution based on Istio, which can automatically and intelligently manage communication between services and provide rich security, traffic management and monitoring functions.",
What error messages appear in the logs?,"""unable to get livez for cluster hosted-mesh-hosted: client rate limiter Wait returned an error: context deadline exceeded - error from a previous attempt: read tcp 192.188.110.245:52050->10.105.106.29:6443: read: connection reset by peer""\n- ""Operation cannot be fulfilled on meshclusters.discovery.mspider.io \""hosted-mesh-hosted\"": the object has been modified; please apply your changes to the latest version and try again""\n - ""mesh hosted-mesh reconcile remote-config error: create mesh hosted-mesh hosted ns error: an error on the server (\""unknown\"") has prevented the request from succeeding (post namespaces)""\n- ""unable to get livez for cluster hosted-mesh-hosted: client rate limiter Wait returned an error: context deadline exceeded - error from a previous attempt: EOF""\n- ""update cluster status error: Operation cannot be fulfilled on meshclusters.discovery.mspider. io \""hosted-mesh-hosted\"": the object has been modified; please apply your changes to the latest version and try again""",
Which module is involved in the above log?,The log involves the mesh-cluster module and the global-mesh module.,
"When creating a grid, if the mcpc-ckube-remote pod is always ""creating"", how should I troubleshoot the problem?","First, you can check the pod log through the `kubectl describe pod` command to see if there is any information about the failure to mount the volume. If the volume mount fails, you need to check whether the corresponding ConfigMap exists. In addition, you can also check the logs of the gsc controller to troubleshoot the problem.",
"In DCE5.0, what is the service mesh module?",The service grid module is an important component in DCE5.0. It is used to configure and manage service communication between Kubernetes clusters to achieve functions such as traffic management and performance optimization.,
Application Workbench is which core module of DCE 5.0?,"The application workbench is one of the core modules of DCE 5.0, which is used to provide users with unified application orchestration, deployment, pipeline management, CICD, application monitoring, log query and other capabilities.",
What expansion modules does the service grid have?,"Observable module, microservice governance module, and application workbench.",
In which clusters do the grid control and data planes run?,"The grid control plane runs in the Mesh Control Plane Cluster (MCPC), and the data plane runs in the working cluster where actual business application services and governance policies take effect.",
What is the role of the grid global management plane?,"The global grid management plane is used to manage multiple service grid instances in a unified manner, provides friendly interface operations and YAML native resource editing capabilities, unified grid instance life cycle management, permission isolation, etc., and is responsible for integrating with DCE 5.0 and other The docking work of modules will reduce the adaptation costs of other modules.",
What are the three aspects of the DCE 5.0 service mesh product?,"Grid global management plane, grid control plane, and grid data plane.",
How does Cilium define and enforce network and application layer security policies?,"Through eBPF technology, Cilium provides a method to define and implement network layer and application layer security policies based on container/container identification.",
Which software does Cilium integrate with to provide a Go-based extension framework?,Cilium is tightly integrated with Envoy and provides a Go-based extension framework.,
What features does Cilium offer?,"Based on eBPF, Cilium provides many functions such as multi-cluster routing, load balancing instead of kube-proxy, transparent encryption, network and service security, etc.",
What technology is Cilium based on to secure networks and connections?,"Cilium's underlying technology utilizes eBPF, a new technology in the Linux kernel, to dynamically implement security, visibility and network control logic in Linux systems.",
What is Cilium?,"Cilium is an open source software that transparently provides network and API connections for application services deployed on the Linux container management platform, and ensures the security of these networks and connections.",
"Which is easier to manage, Contour or Nginx Ingress?",Contour is easier to manage.,
What features does Contour support?,"Contour supports dynamic configuration updates and multi-team ingress proxies out of the box while maintaining lightweight configuration files. In addition, Contour also supports easier-to-use declarative syntax, more comprehensive monitoring indicators, and support for Gateway API.",
Which data surface does Contour use?,Contour uses Envoy as the data plane.,
What is Contour?,"Contour is an open source Kubernetes Ingress controller that uses Envoy as the data plane, supports dynamic configuration updates and multi-team ingress proxies out of the box, while maintaining lightweight configuration files.",
What is the difference between Contour and Nginx Ingress?,"The differences between Contour and Nginx Ingress are: separation of data plane and control plane, convenient management, performance advantages, easier-to-use declarative syntax, more comprehensive monitoring indicators and Gateway API support.",
What is Router-Plugin?,"Router-Plugin is a Meta plug-in. By setting some routing rules in Pod Netns, data packets from the host and within the cluster are forwarded from eth0 of the Pod, while data packets from outside the cluster are forwarded from the network card created by MacVLAN/SRIOV.",
What is Veth-Plugin?,"Veth-Plugin is a Meta plug-in, similar to ptp, by adding a pair of Veth-Peer devices in Pod NetNs and hijacking traffic from hosts and clusters to pass through the Veth devices.",
What plugins does Meta-plugins include?,Meta-plugins includes Veth and Router plug-ins.,
What components does Multus-underlay contain?,"Multus-underlay includes Multus, Meta-plugins and SR-IOV CNI (optional). Meta-plugins includes two plug-ins: Veth and Router.",
What communication problems of Underlay CNI does Multus-underlay solve?,Multus-underlay solves the problem of Underlay CNI type Pods being unable to access the cluster ClusterIP and failing health checks.,
What is Multus-underlay?,"Multus-underlay is based on Multus and combined with some Underlay-type CNI plug-ins, which can insert multiple network cards into the Pod.",
Can F5network components work under Layer 4 load balancing and Layer 7 load balancing at the same time?,"F5network components cannot work under layer 4 load balancing and layer 7 load balancing at the same time, and can only choose one of the two.",
 What scenarios are Layer 4 load balancing and Layer 7 load balancing suitable for? Which component needs to be installed?,Layer 4 load balancing is applicable to the LoadBalancer service and can be used with NodePort forwarding mode or Cluster forwarding mode. The [f5 ipam controller](https://github.com/F5Networks/f5-ipam-controller) component needs to be installed. Layer 7 load balancing is suitable for ingress controller and can be used with NodePort forwarding mode or Cluster forwarding mode. There is no need to install the [f5 ipam controller](https://github.com/F5Networks/f5-ipam-controller) component.,
What are the advantages of Cluster forwarding mode? What are the requirements?,"The advantage of the Cluster forwarding mode is that data packets are forwarded directly to the Pod without going through the NodePort of the kube proxy. The forwarding method is more efficient and the delay is lower. The requirement is to forward the Pod route to the router and F5 device in the network through the BPG protocol, or to establish a VXLAN tunnel between the nodes in the cluster and the F5 device.",
What are the advantages of NodePort forwarding mode? What are the requirements?,The advantage of NodePort forwarding mode is that it is more versatile. The requirement is that the loadBalancer service object of the cluster application must be assigned a nodePort.,
What are the two modes for F5 devices to forward traffic to the cluster?,F5 devices have NodePort forwarding mode and Cluster forwarding mode to forward traffic to the cluster.,
 Which two official F5 projects does the F5network component integrate with?,The F5network component integrates [f5 ipam controller](https://github.com/F5Networks/f5-ipam-controller) and [k8s bigip ctlr](https://github.com/F5Networks/k8s-bigip-ctlr) An official F5 project.,
What role does the Cloud Orchestrator plugin play in Calico?,"The Cloud Orchestrator plug-in converts the orchestrator API that manages the network into Calico's data model and data store. For cloud providers, Calico provides a separate plug-in for each cloud orchestration platform. This allows users to manage Calico networks using their own orchestrators.",
What is calicoctl? What hosts does it work on?,"calicoctl is Calico's command-line tool, available as a binary or container, and can be used on any host with network access to the Calico data store.",
What is Typha? What does it do?,"Typha is a daemon that scales by reducing the impact of each node on data storage. Its role is to maintain a single data store connection that can be used to cache the state of the data store and replicate events so that they can be promoted to more listeners. In a large-scale (100+ nodes) Kubernetes cluster, this is critical because the number of updates generated by the API server increases with the number of nodes.",
What are the network submodules in Calico?,"Networking submodules in Calico include the Kubernetes plugin, etcd, IPAM, kube-controller, Typha, and cloud orchestrator plugins.",
What is Calico?,"Calico is an open source container networking and security solution for connecting, securing, and managing containers, virtual machines, and bare metal hosts.",
What is the role of Dikastes in Calico?,"Dikastes enforces network policy for the Istio service mesh. Runs on the cluster as a sidecar proxy for Istio Envoy. Dikastes are optional. Calico uses Envoy's sidecar proxy Dikastes in the Linux kernel (using iptables, at layers three and four) and layers three to seven to enforce network policies for workloads and encrypt and authenticate requests.",
What does Calicoctl do?,The Calicoctl command line needs to be installed separately as a binary or container and can be used on any host with network access to Calico data storage for Calico management and operations.,
What does the Calico CNI plugin do?,"The Calico CNI plugin provides Calico networking to Kubernetes clusters, allowing the use of Calico networking using orchestration schedulers that comply with the CNI networking specification.",
What is the role of confd in Calico?,"confd is an open source, lightweight configuration management tool. Monitor Calico data storage for BGP configuration and global default log changes, such as AS number, log level and IPAM information. confd dynamically generates BIRD configuration files based on data updates in storage. When the configuration file changes, confd will trigger BIRD to load the new file.",
What data storage plugins does Calico provide?,"Calico provides two data storage plug-ins: Kubernetes API datastore and etcd. Among them, Kubernetes API datastore uses Kubernetes API data storage (kdd) to manage the network, while etcd is a consistent, highly available distributed key-value store that provides data storage for the Calico network and is used for communication between components.",
What is the role of Typha in Calico?,"Typha runs as a daemon between the data store and Felix instances. Can be used to cache the state of a data store and replicate events so they can be rolled out to more listeners. One Typha instance supports hundreds of Felix instances, which can significantly reduce the load on data storage. Because Typha can filter out updates that are not relevant to Felix, Felix's CPU usage is reduced.",
What role does Felix play in Calico?,"Felix runs as an agent on each machine endpoint. Program the routes and ACLs and host to provide the required connectivity to the endpoints on that host. In particular, ensure that the host responds to ARP requests from each workload, provides the host's MAC, and enables IP forwarding for the interfaces it manages. It also monitors the interface to ensure programming is applied at the appropriate time.",
What is the role of BIRD in Calico?,BIRD is BGP Internet Routing Daemon. It obtains routes from Felix and distributes them to BGP peers on the network for routing between hosts.,
What components does Calico consist of?,"Calico consists of multiple components such as Calico API Server, Felix, BIRD, confd, Dikastes, CNI, data storage plug-in, IPAM, kube-controller, Typha and calicoctl.",
What is Calico?,"Calico creates and manages a flat three-layer network (no overlay required), and each container is assigned a routable IP. It also provides rich and flexible network policies to ensure that Workload's multi-tenant isolation, security groups and other reachability restrictions are provided through ACLs on each node.",
What mirror traffic-related functions does ingress-nginx support?,ingress-nginx supports functions such as image caching and image prefetching. You can view the documentation (./mirror.md) for details.,
How to limit upload size?,You can check the documentation (./upload.md) to learn how to limit the upload size.,
How to configure ingress-nginx logs?,You can view the documentation (./log.md) to learn how to configure ingress-nginx logs.,
How to configure timeout?,You can check the documentation (./timeout.md) to learn how to configure the timeout.,
What load balancing algorithms does ingress-nginx support?,"ingress-nginx supports load balancing algorithms such as polling, IP hash, least_conn, etc. You can check the documentation (./proxy.md) for details.",
How to configure cross-domain?,You can check the documentation (./cors.md) to learn how to configure cross-domain.,
IngressClass Scope What resources can be controlled using this controller?,"You can control namespace, cluster, namespace in the cluster and other level resources using this controller. You can view the documentation (./scope.md) for details.",
What properties can IngressClass define?,"You can define the default controller, whether to support TLS and other properties. You can check the documentation (./ingressclass.md) for details.",
What is IngressClass?,IngressClass is a way to define the Ingress controller in Kubernetes. You can view the document (./ingressclass.md) to learn about IngressClass.,
What is the installation method of ingress-nginx?,You can view the installation document (./install.md) to learn how to install ingress-nginx.,
What does ingress-nginx use as reverse proxy and load balancing?,ingress-nginx uses Nginx as a reverse proxy and load balancing.,
What is ingress-nginx?,"ingress-nginx is an Ingress controller hosted by the Kubernetes official community. It uses Nginx as a reverse proxy and load balancing. After a lot of production practice, it is stable and reliable.",
What CNIs can be used in cloud-native networks in DCE 5.0?,"CNIs such as Cilium, Calico, MacVLAN, SpiderFlat and SR-IOV can be used in the cloud native network of DCE 5.0.",
What role does Multus play in the cloud native network of DCE 5.0?,"In the cloud-native network of DCE 5.0, Multus is the scheduling core. When paired with multiple CNIs, it realizes IP allocation of Pods to multiple CNIs and supports polymorphic network communication scenarios for applications.",
What role does Cilium play in the cloud native network of DCE 5.0?,"In the cloud native network of DCE 5.0, Cilium plays the role of high-performance Overlay CNI, providing eBPF kernel acceleration, realizing cross-cluster Pod communication and cross-cluster Service communication, and supporting flexible fine-grained network policy delivery and rich traffic observation capabilities. .",
What solutions does DCE 5.0 cloud native network provide?,"DCE 5.0 cloud native network provides two solutions, namely: Cilium + MacVLAN/SpiderFlat + SpiderPool + Multus and Calico + MacVLAN/SpiderFlat + SpiderPool + Multus.",
What is the cloud native network of DCE 5.0?,"The DCE 5.0 cloud native network is built based on multiple open source technologies. It not only provides support for a single CNI network, but also provides a combination solution of multiple CNI networks.",
What are the advantages and disadvantages of L3 mode?,"- Advantages: Better load balancing\n- Disadvantages: When a node fails, all BGP sessions will be interrupted",
What is L3 mode?,"In BGP mode, each node in the cluster will establish a BGP Peer with the router and use this session to advertise the LoadBalanceIP of the cluster service to the outside of the cluster. BGP Router selects a next hop based on each different connection (that is, a certain node in the cluster, which is different from L2 mode where all traffic reaches a certain Leader node first).",
What are the advantages and disadvantages of L2 mode?,"- Advantages: Universal, no additional hardware support required\n- Disadvantages: Single node bandwidth limit, slightly slow failover (about 10s)",
What is L2 mode?,"L2 mode is a mode of `Metallb`, which will elect a Leader node through `memberlist`. This node is responsible for announcing `LoadBalancerIP` to the local network. The biggest advantage of L2 mode is that it does not require dependencies on hardware such as routers to work.",
What does `Metallb` do?,`Metallb` is an open source software that uses standard routing protocols (ARP or BGP) to implement the load balancing function of bare metal K8s clusters.,
"In YAML usage, what annotation is used to select which IP pool to allocate an IP address from?",Use the Pod annotation `ipam.spidernet.io/ippool` to choose to allocate IPs from the corresponding IP pool.,
"In the interface operation, which page can I enter to configure the container network card?",Enter `Advanced Configuration` and click to configure `Container Network Card`.,
How to automatically create a fixed IP pool?,Select the corresponding subnet to automatically create a fixed IP pool.,
What needs to be done in advance to use manual selection of an existing IP pool?,Using manual selection of an existing IP pool requires creating an IP pool in advance.,
What does this article cover?,"This article describes how to combine Multus and the Underlay CNI plug-in to configure multiple network cards for the workload Pod, and use Spiderpool to allocate and fix the IP of the Underlay network. It mainly introduces how to set up multi-container network cards for Pods, use IP pools for workloads, use fixed IP pools for workloads, and automatically create fixed IP pools for workloads.",
How to create a TLS certificate and use https to access services provided by f5network?,"You need to create a set of TLS certificates first, and then create a secret based on the key file (key and crt). Finally, configure tls in the ingress object and specify secretName.",
How to create f5network load balancing service for ingress in the cluster?,"When the component is installed in Layer 7 load balancing mode, you can create an F5 load balancing service for the ingress in the cluster. For specific methods, please refer to the official documentation. You need to confirm the ingressClass of F5 and create service and ingress objects of nodePort type. The delivered forwarding rules can be observed in the F5 Web UI.",
What annotations does f5network support?,"f5network supports multiple annotations, including cis.f5.com/ipamLabel and cis.f5.com/health, etc.",
How to create f5network load balancing service for services in the cluster?,"When working in nodePort forwarding mode, a LoadBalancer type service must be created for the application, and then the delivered forwarding rules can be observed in the F5 Web UI.",
What load balancing modes does f5network have?,f5network has layer 4 and layer 7 load balancing modes.,
How do I share an IP address?,"Different Services can be created and share Service IP, indirectly supporting the shared IP address function. Annotations(key) The same LoadBalancer Service will have the same IP (ipv4/ipv6).",
How do I specify an IP address?,"When creating the LoadBalancer Service, you can specify the IP address through Annotations: metallb.universe.tf/loadBalancerIPs.",
How to specify an address pool?,"When creating the LoadBalancer Service, you can specify the address pool through Annotations: metallb.universe.tf/address-pool.",
What is Metallb's default address pool?,The cluster default address pool will allocate addresses from the existing address pool with autoAssign=true set.,
What is Metallb?,Metallb is a load balancer in a Kubernetes cluster. It implements load balancing by extending the Kubernetes Service.,
What are the important CRDs in Submariner?,"There are the following important CRDs in Submariner: brokers.submariner.io, clusterglobalegressips.submariner.io, clusters.submariner.io, endpoints.submariner.io, gateways.submariner.io, globalegressips.submariner.io, globalingressips.submariner.io, servicediscoveries .submariner.io, serviceexports.multicluster.x-k8s.io, serviceimports.multicluster.x-k8s.io, and submariners.submariner.io. Among them, submariners.submariner.io is used by the submariner-operator component to create all Submariner components; clusters.submariner.io stores the information of each subcluster, including the subnet information of its Pod and Service; endpoints.submariner.io records the gateway node of each subcluster Basic information, including private/public IP/tunnel mode/status, etc.; serviceexports.multicluster.x-k8s.io exports each Service, corresponding to a serviceexports object, for service discovery; serviceimports.multicluster.x-k8s.io for For each serviceexports object, Lighthouse-agent creates a corresponding serviceimports object for consumption by other clusters; clusterglobalegressips.submariner.io is the global CIDR, which is used to solve the problem of sub-cluster subnet overlap when Globalnet is enabled.",
What are the optional components for Submariner?,"Optional components of Submariner include Globalnet Controller, which supports inter-cluster interconnection of overlapping subnets.",
What are the important components of Submariner?,"Submariner includes Broker, Gateway Engine, Route Agent and Service Discover components. The Broker does not have actual Pods and Services, but only provides credentials for sub-clusters to access the Broker cluster API-Server; the Gateway Engine establishes and maintains tunnels between clusters and opens up cross-cluster network communications; the Route Agent establishes between the Gateway node and the worker node Vxlan tunnel enables cross-cluster traffic on the working node to be forwarded to the Gateway node first, and then sent from the Gateway node to the peer through the cross-cluster tunnel; Service Discover includes Lighthouse-agent and Lighthouse-dns-server components, which implements the KMCS API and provides Cross-cluster service discovery.",
What is a Submariner?,"Submariner is an open source multi-cluster network solution that implements cross-cluster Pod and Service connectivity in a secure manner, and implements KMCS through Lighthouse components to provide cross-cluster service discovery capabilities.",
How does Spiderpool prevent IP address allocation conflicts?,"Spiderpool can prevent IP address allocation conflicts through mechanisms such as staggering IP addresses in the IP Pool, non-overlapping addresses between IP Pools, strict control of additions, deletions, modifications, and IP reservation mechanisms.",
What CNI plug-ins does Spiderpool work with?,"SpiderPool is suitable for any CNI plug-in that can connect to third-party IPAM, especially for some CNIs that lack IPAM, including SRI-OV, MacVLAN, IPVLAN and OVS-CNI.",
What capabilities does Spiderpool currently support?,"Currently, SpiderPool supports the following capabilities: multi-path IP Pool usage, IP Pool node affinity, IP Pool namespace affinity, backup IP Pool, application fixed IP, mechanism to prevent IP address allocation conflicts, and Recycling mechanism for IP address leaks, dual stack support, Statefulset support, Pod multi-network card support, reserved IP and multi-level routing customization, etc.",
What were the main design goals for Spiderpool?,The main design goal of Spiderpool is to use Underlay CNI for refined management of IP.,
What is Spiderpool?,"Spiderpool is an IP address management (IPAM) plug-in that can allocate IP addresses to container cloud platforms. It is suitable for any CNI plug-in that can connect to third-party IPAM, especially for some CNIs that lack IPAM.",
How to confirm that VF configuration is successful?,`cat /sys/class/net/<sriov_netdevice>/device/sriov_numvfs` or `ip link show <sriov_netdevice>`,
Which command can be used to configure VF for a network card that supports SR-IOV?,`echo xx > /sys/class/net/<sriov_netdevice>/device/sriov_numvfs`,
Will SR-IOV work if the node is a VM or does not have a network card that supports SR-IOV?,no.,
What command can be used to check whether a node has a network card that supports the SR-IOV function?,`ip link show`,
What error status code will the client receive when uploading a file size that exceeds the limit?,"When the client requests that the Body size exceeds the limit, a Request Entity Too Large error with status code 413 will be returned to the client.",
How do I set upload limits for specific Ingress resources?,This can be achieved by adding the annotation `nginx.ingress.kubernetes.io/proxy-body-size`.,
How to configure global upload limits?,"Can be configured through the parameter `client_max_body_size`, the default is `1m`.",
How to access exported services in ClusterB?,Run the command "nslookup [service-name].[namespace].svc.clusterset.local" to find and access the service.,
How to check the exported service status?,Run the command "kubectl get serviceexports.multicluster.x-k8s.io" to view the exported service. Run the command "kubectl get serviceexports.multicluster.x-k8s.io [service-name] -o yaml" to view the detailed information of the exported service. .,
How to manually export services to other clusters?,You can export the service in ClusterA by running the command "subctl export service [service-name]" and then access the service in ClusterB.,
How to check whether the Globalnet configuration is correct?,Run the command "subctl diagnose" to diagnose and check whether the Globalnet configuration is correct.,
How to avoid warnings in subctl and ensure correct behavior?,"Add one of the following tags in the namespace ""submariner-operator"": pod-security.kubernetes.io/warn=privileged, pod-security.kubernetes.io/enforce=privileged, pod-security.kubernetes.io/audit=privileged .",
How to use Submariner for cross-cluster service discovery?,"You can use the subctl tool to export the Service and access it in other clusters. For specific operations, please refer to the examples provided in the article.",
How to troubleshoot if cross-cluster communication fails?,"Use the subctl tool to execute the `subctl diagnose all` command to troubleshoot; if the problem cannot be solved, you can use the `subctl gather` command to collect all information about the current environment for analysis.",
How do I verify that the Submariner is ready?,Use the subctl tool to execute the `subctl show all` command to view the displayed results.,
Why do I need to manually configure IPPool when using Calico CNI?,"Since the IPTables rules inserted by Calico have a higher priority than the IPTables rules inserted by Submariner, cross-cluster communication problems occur. Therefore, Calico's IPPool must be manually configured to avoid this problem.",
What is a Submariner?,Submariner is an open source cross-Kubernetes cluster connectivity and service discovery solution.,
How to deploy a tenant-level Ingress instance?,"When deploying a load balancer, after it is assigned to a workspace, the Pods in the namespace corresponding to the current cluster in this workspace can receive requests distributed by the load balancer. When deploying Ingress-Ngnix, specify to enter `kubernetes.io/metadata.name :workspace01` in the `Namespace Selector`. The created Ingress instance is exclusive to the workspace `workspace01`.",
How to create a platform/namespace level Ingress instance?,"Different instances can watch different namespaces by specifying `--watch-namespace`, or you can set `controller.scope.enabled=true` and `--set controller.scope.namespace=$NAMESPACE` to enable and Set platform/namespace level Ingress.",
What scenarios are namespace-level Ingress instances suitable for?,Namespace-level Ingress instances are suitable for situations where a single namespace has an exclusive Ingress instance to achieve load isolation.,
What scenarios are platform-level Ingress instances suitable for?,Platform-level Ingress instances are suitable for situations where the same Ingress instance needs to be shared in the same cluster.,
What is IngressClass Scope?,"IngressClass Scope is used to specify the usage scope of the Ingress instance as cluster level, namespace level and workspace level.",
How to check client source IP in Metallb + istio-ingressgateway?,"On the `Global Management` -> `Audit Log` page, click `View Details` after any event to view the obtained client source IP.",
How can Metallb be configured to announce a specified node as the next hop of LB IPs?,`spec.nodeSelectors` needs to be modified to implement binding.,
How can I get the client source IP in Metallb + istio-ingressgateway?,"By keeping the nodes deployed by istio-ingressgateway pod consistent with the nodes announced by Metallb, that is, labeling the specified nodes, schedule the istio-ingressgateway pod to the above-mentioned nodes, and configure Metallb to announce the above-mentioned nodes as the next hop of LB IPs . Modifying service: istio-ingressgateway's `spec.externalTrafficPolicy` = `Local` is also a necessary step. This mode retains the real source IP.",
How does macvlan + Whereabouts perform when doing a rebuild?,"When rebuilding, after some Pods reached the `Running` state, macvlan + Whereabouts performed poorly, and it took a long time to complete the rebuild in the test.",
"In a scenario with multiple small-scale deployments, which CNI plug-in takes the shortest time to create?","In the scenario of multiple small-scale deployments, macvlan + Spiderpool has the shortest creation time, which is 1m37s.",
Which CNI plug-in has the shortest rebuild time in a single 1000-replica deployment scenario?,"In the deployment scenario of a single 1000 replicas, Calico + calico-ipam has the shortest reconstruction time, which is 4m6s.",
Which two methods were tested in the article to launch a total of 1000 Pods?,"The article tested two ways to start a total of 1,000 Pods: creating only one Deployment with a replica count of 1,000, and creating 100 Deployments with a replica count of 10.",
Which CNI plug-ins did this article test for performance?,"The article tested the performance of four CNI plug-ins: macvlan + Spiderpool, macvlan + Whereabouts, Kube-OVN and Calico + calico-ipam.",
What were the test results?,"In a single 1000-copy Deployment scenario, macvlan + Spiderpool takes the shortest time to create and delete, and takes the longest to rebuild; macvlan + Whereabouts performs poorly in both creation and reconstruction scenarios; Kube-OVN performs better in creation and deletion scenarios. , the reconstruction is slow; Calico + calico-ipam performs well in the three scenes. In the scenario of 100 10-copy Deployments, all solutions perform better than a single 1000-copy Deployment.",
What scenarios were tested in this article?,"This article tested the following two methods to start a total of 1,000 Pods on a Kubernetes cluster, and recorded the time it took for all Pods to reach ""Running"": ① Create only one Deployment, with the number of copies being 1,000; ② Create 100 Deployments, each The number of copies of a Deployment is 10. At the same time, we tested the time it took to delete these 1,000 Pods at once and reach ""Running"" again. We also tested the time it took to delete all Deployments and record the time it took for all Pods to completely disappear.",
Which solutions are chosen for comparison in this article?,"This article selects several other solutions for connecting underlay networks in the open source community for comparison: macvlan + Whereabouts, Kube-OVN + Kube-OVN IPAM, Calico + calico-ipam.",
Why do we need to perform performance testing of the underlay IPAM CNI plug-in?,"The speed at which IPAM allocates IP addresses largely determines the speed at which applications are released. When recovering from a fault in a large-scale Kubernetes cluster, underlay IPAM often becomes a performance bottleneck, and within a limited IP address range, concurrent creation of Pods will involve IP address preemption and conflict.",
This article introduces what are the characteristics of Spiderpool?,"Spiderpool is a high-performance IPAM CNI plug-in suitable for underlay networks. It supports the entire life process of the managed IP pool, allowing it to be synchronized with workload creation, expansion and deletion, and weakening the concurrency caused by an overly large shared pool. or storage issues.",
How to confirm that the sriov-device-plugin resource exists in the SR-IOV multus network-attachment-definition object?,You can use the command "kubectl get network-attachment-definitions.k8s.cni.cncf.io -n kube-system sriov-vlan0 -o yaml" to view it.,
What is SRIOV-Device-Plugin?,SRIOV-Device-Plugin is a Kubernetes plug-in used to integrate with the SR-IOV CNI and manage the virtual functions (VF) of Intel network cards.,
What problems occur in IPtables mode when the number of rules increases to a certain level?,"When the number of rules increases to a certain level, IPtables mode can cause reduced throughput, increased CPU usage, and increased latency.",
What are the advantages of Calico compared to Kubernetes Policy?,"Calico supports policy priority, Deny rules, more flexible matching rules, and supports the control of more policy objects.",
How does Calico manage Pods in a specific namespace?,"Calico manages Pods under a specific namespace through the NetworkPolicy object, and specifies the scope of effect through an additional namespace field.",
What does the Policy implementation in Calico depend on?,Calico's Policy implementation relies on IPtables.,
What is the difference between NetworkPolicy and GlobalNetworkPolicy in Calico?,"NetworkPolicy acts on Pods in a specific namespace, while GlobalNetworkPolicy acts on the entire cluster, which is more global.",
What model does Kubernetes use by default? Why do we need to define global policies or tenant-level policies to control the ingress and egress traffic of Pods?,"Kubernetes adopts a zero-trust model by default, that is, all Pods and hosts in the cluster can access each other. \nDefining global policies or tenant-level policies to control the ingress and egress traffic of Pods can enhance communication security and network isolation within the cluster.",
How to specify priority for Calico's Policy?,"Specified through the order field, the smaller the value, the higher the priority. If the values of order are the same, sort according to the order of the name field of the Policy.",
What behaviors does Calico's policy support?,"Calico's policy supports four behaviors: Allow, Deny, Log and Pass.",
Which two directions of traffic does Calico support?,Calico supports the control of egress and ingress of traffic.,
Which objects does Calico support policy control?,"Calico supports policy management and control of Pods, Services, nodes, virtual machines, and ServiceAccounts.",
How to deploy a sample application and configure Ingress Nginx for load balancing based on request hashing?,"You can refer to the YAML file given above, which includes the definition of a sample application, Service and Ingress. Mapping based on client IP can be specified using the `nginx.ingress.kubernetes.io/upstream-hash-by` annotation.",
How to enable group-based load balancing?,You can add the annotation `nginx.ingress.kubernetes.io/upstream-hash-by-subset: "true"` and use `nginx.ingress.kubernetes.io/upstream-hash-by-subset-size` to specify each group Number of workloads.,
How to use annotations for mapping based on client IP?,You can use the `nginx.ingress.kubernetes.io/upstream-hash-by` annotation and set it to `$binary_remote_addr` or `$http_x_forwarded_for`.,
What is request hash based load balancing?,"Load balancing based on request hashing refers to using a specified hash algorithm to map the client and server to a unique value, and routing the request to the corresponding server. Common hash values include client IP and request path. wait.",
How to implement load balancing strategy based on request hash?,"You can specify the hash value for client and server mapping by using the `nginx.ingress.kubernetes.io/upstream-hash-by` annotation. Mapping based on client IP, etc. can be done using `$binary_remote_addr` or `$http_x_forwarded_for`. You can enable the grouping function by adding the `nginx.ingress.kubernetes.io/upstream-hash-by-subset: ""true""` annotation. After the traffic reaches the group, it is randomly assigned to the workloads in the group.",
How does cookie-based load balancing strategy work?,"The cookie-based load balancing strategy uses cookies to bind clients and back-end services together, thereby ensuring that each client's request is always processed by the same back-end service. The specific process is: Ingress Nginx will first use the load balancing algorithm to select a backend service to process the request, and send a cookie named `example-cookie-name` back to the client. After that, when the client sends a request again, it will bring With this cookie, Ingress Nginx forwards the request to the corresponding backend service based on the identification information in the cookie.",
How does Ingress Nginx forward traffic to different backend services through domain names?,"You can use the `host` field in the Ingress configuration to specify different domain names, and specify the corresponding backend Service in the corresponding `http.paths.backend.service`.",
How to configure global load balancing policy in Ingress Nginx?,"The default load balancing algorithm can be specified by setting the `ingress-nginx.controller.config.load-balance` parameter in the Helm installation configuration `.Values.yaml` file. Supports both `round_robin` and `ewma`, the default is `round_robin`.",
What is traffic load balancing?,"Traffic load balancing is the process of allocating traffic from clients to multiple backend services for processing to improve system performance, availability, and stability.",
What's new in Spidernet Release Notes v0.6.0?,"New features in v0.6.0 include filling in the VLAN ID when creating an IP pool, displaying the total number and used number of IPs, etc.",
What are the fixes in Spidernet Release Notes v0.6.0?,"v0.6.0 fixes issues such as pointer errors when viewing workloads when using a single stack, Controller timeout when adding a large number of IPs, Chinese characters can be filled in when filling in affinity, and the workload name does not match the namespace.",
Which Spiderpool version does v0.6.0 in Spidernet Release Notes adapt to?,v0.6.0 is adapted to Spiderpool version v0.4.1.,
What are Spidernet's Release Notes used for?,"Spidernet's Release Notes list the evolution paths and feature changes of each version, making it easier for users to understand the changes after version upgrades.",
What is Spidernet?,Spidernet is a network management tool.,
"For the planning of Worker01 and Worker02 nodes in Calico with Macvlan and SR-IOV CNI scenario, which network card is based on which VLAN sub-interface is created?","Create VLAN sub-interfaces (eth1.1, eth1.2) based on eth1.",
"In the scenario of Calico paired with Macvlan CNI, what traffic is eth1 mainly responsible for?",eth1 is mainly responsible for Underlay (Macvlan/SRI-OV) network traffic.,
"In the scenario of Calico paired with Macvlan CNI, what traffic is eth0 mainly responsible for?","eth0 is mainly responsible for Kubernetes internal management traffic, Calico traffic, and inter-node communication traffic.",
"In the scenario of Calico paired with Macvlan CNI, how many physical network cards are recommended for all nodes?",It is recommended that all nodes have multiple physical network cards and the network card names are consistent.,
What is the Overlay + Underlay CNI network solution?,"The Overlay + Underlay CNI network solution is a network plan in which the Overlay network is used for communication between Kubernetes Pods, and the Underlay network is used for communication between containers and external networks.",
How to check whether the version of NetworkManager is supported?,You can check the version of NetworkManager through the following method:\n```\n/usr/sbin/NetworkManager --version\n```,
How to install nmstate?,It can be installed through Helm. The specific command is: \n```\nhelm repo add daocloud https://daocloud.github.io/network-charts-repackage/ \nhelm install nmstate -n nmstate daocloud/nmstate --create-namespace \n```,
What does nmstate depend on?,"nmstate relies on NetworkManager, which is not supported by all Linux distributions, such as Ubuntu.",
What is nmstate?,nmstate is a project that configures the network on the node through k8s CRD.,
How to allow external access to ClusterIP Service?,External access to ClusterIP SVC can be enabled through the command --set bpf.lbExternalClusterIP=true. But you need to open the relevant routes yourself.,
How does Cilium implement neighbor discovery? In which kernel versions is this possible?,"Cilium implements neighbor discovery through the Linux kernel. In the kernel version 5.16 and above, it is implemented through the ""managed"" function and uses ""extern_learn"" to mark the arp record to prevent it from being garbage collected by the kernel. For older versions of the kernel, the IP address of the new node is regularly written into the Linux kernel through cilium-agent for dynamic resolution.",
What does the Cilium Kubernetes service Topology Aware Hints function do?,"Cilium implements the K8s service Topology Aware Hints function, which can bias requests to backend endpoints in the same region.",
How to bypass Socket LoadBalancer in Pod namespace?,Bypass Socket LB configuration in kube-proxy-free environment: --set tunnel=disabled --set autoDirectNodeRoutes=true --set socketLB.hostNamespaceOnly=true.,
How to check the driver used by the device?,You can use the command "ethtool -i eth0 | grep driver" to view the driver used by the device.,
How does Cilium detect the use of multiple network cards to expose a NodePort?,Cilium automatically detects the use of multiple network cards to expose NodePorts.,
What is XDP acceleration support? What conditions are required to use it?,"XDP acceleration support means providing XDP acceleration support for NodePort, loadBalancer and externally accessible SVC. The network card driver needs to support XDP.",
How to allow external access to ClusterIP Service?,"Cilium does not allow external access to ClusterIP SVC by default. It can be enabled through the Helm parameter ""--set bpf.lbExternalClusterIP=true"", but you need to open the relevant routes yourself.",
How to implement neighbor discovery after Cilium version 1.11? How to set neighbor discovery update interval?,"The neighbor discovery library has been deleted after Cilium version 1.11, and it relies entirely on the Linux kernel to implement neighbor discovery. In the kernel of version 5.16 and above, it is implemented through the ""managed"" function, and ""extern_learn"" is used to mark the arp record to prevent it from being garbage collected by the kernel; for the kernel of lower version, the IP address of the new node is written regularly through cilium-agent. into the Linux kernel for dynamic parsing. The neighbor discovery update interval can be set through the Helm parameter ""--set --arping-refresh-period=30s"".",
How to enable topology awareness prompts?,You can turn on topology awareness prompts through the Helm parameter "--set loadBalancer.serviceTopology=true" to make requests more biased toward backend endpoints in the same region.,
How to bypass Socket LoadBalancer in Pod namespace? What should I pay attention to?,It can be configured through the Helm parameters "--set tunnel=disabled --set autoDirectNodeRoutes=true --set socketLB.hostNamespaceOnly=true". It should be noted that this function will be disabled when relying on SVC IP for load.,
How to enable XDP acceleration? What should I pay attention to?,XDP acceleration can be turned on through the Helm parameter "--set loadBalancer.acceleration=native". It should be noted that this function can only be used when the network card driver supports XDP; also pay attention to the supported driver list and the driver used by the device.,
"In mixed DSR and SNAT mode, for which protocol is DSR performed? For which protocol is SNAT performed?","In mixed DSR and SNAT mode, DSR is performed on TCP and SNAT is performed on UDP.",
What is direct SVC return (DSR) mode? How to turn it on?,"Direct SVC return (DSR) mode is for external traffic. When the traffic reaches the LB or NodePort node, SNAT is not performed when it is transferred to the backend EP. The response traffic no longer passes through the LB or the node where the traffic comes in, but is returned directly to Client mode. It can be enabled through the Helm parameters ""--set tunnel=disabled --set autoDirectNodeRoutes=true --set loadBalancer.mode=dsr"".",
How to set the parameters of Maglev hash consistency?,"Maglev hash consistency has two parameters that can be adjusted, namely maglev.tableSize and maglev.hashSeed. maglev.tableSize should be greater than 100*N and must be a prime number; maglev.hashSeed is a base64-encoded 12-byte random number. It can be set through the Helm parameters ""--set maglev.tableSize=65521 --set maglev.hashSeed=$SEED"".",
What is Maglev hash consistency? How to turn it on?,Maglev hash consistency is a strategy for obtaining the backend Pod address by hash calculation based on the five-tuple for external traffic. It can be enabled through the Helm parameter "--set loadBalancer.algorithm=maglev".,
How to view the data storage in Cluster Mesh?,"You can enter the clustermesh APIServer Pod through the following commands, configure ETCD-related certificates, and view data through etcdctl commands: etcdctl get --prefix cilium/state/identities/v1, etcdctl get --prefix cilium/state/ip/v1/<NS> , etcdctl get --prefix cilium/state/nodes/v1, etcdctl get --prefix cilium/state/services/v1/<clusterName>/<NS>.",
How to enable bandwidth management and kube-proxy replacement in Kubespray?,"Bandwidth management can be enabled through the ""cilium_config_extra_vars"" or ""cilium_enable_bandwidth_manager"" parameters. kube-proxy replacement can be set through the ""cilium_kube_proxy_replacement"" parameter, but related advanced features require complex operations to enable, such as Maglev hash consistency and direct SVC return.",
How to configure the Service Mesh function in Cilium?,"Currently, Cilium does not support directly opening Service Mesh by modifying certain parameters. It can only be opened through Cilium CLI or Helm.",
What features does Cilium support for connecting multiple clusters?,"Cilium supports the Cluster Mesh function, which can connect multiple Cilium clusters together. This function opens up the connectivity of Pods in each cluster, supports the definition of global SVC, and performs load balancing among multiple clusters. You need to enable the ""cilium clustermesh enable"" command and meet certain prerequisites.",
What is Cilium's Egress Gateway? How to turn it on?,"Cilium's Egress Gateway is used to define which traffic can leave the cluster, and specify the corresponding node and the source IP of the cluster, but does not maintain the source IP of the egress (only supports IPv4). This feature is not compatible with L7 policy. It can be enabled through the ""enable-ipv4-egress-gateway: true"" parameter.",
How to enable the syslog function of Ingress Nginx to access logs and error logs?,You need to enable the configuration item `enable-syslog` and specify the address and port of the syslog server.,
How to filter out a list of URLs that appear in NGINX access logs?,You can use the configuration item `skip-access-log-urls` to filter out the list of URLs that appear in NGINX access logs. The best practice is to filter `/health`.,
How to set the error log level of Ingress Nginx?,"You can use the configuration item `error-log-level` to set the error log level, which supports setting to `debug`, `info`, `notice`, `warn`, `error`, `crit`, `alert`, `emerg `.",
How to turn off the access log output of Ingress Nginx?,You can use the configuration item `disable-access-log` to turn off access log output.,
How to set the output location of the error log of Ingress Nginx?,You can use the configuration item `error-log-path` to set the output location of the error log. It is recommended to output the log to `stderr`.,
How to set the access log output location of Ingress Nginx?,You can use the configuration item `access-log-path` to set the output location of the access log. It is recommended to output the log to `stdout`.,
What is a common application scenario of Macvlan in the network?,A common application scenario of Macvlan in the network is to create different Macvlan instances based on different VLANs to achieve communication between different VLANs.,
Can the value of `v1.multus-cni.io/default-network` be a macvlan-overlay type CRD?,"The value of `v1.multus-cni.io/default-network` cannot be a CRD of macvlan-overlay type, that is, macvlan-overlay cannot be used as the first network card of the Pod.",
What type of Pod can the `macvlan-overlay` type Pod communicate with normally?,Pods of the `macvlan-overlay` type must be able to communicate normally with Pods of the overlay type.,
What are the two general usage scenarios of Multus + Macvlan?,Multus + Macvlan generally has two usage scenarios: `macvlan-standalone` and `macvlan-overlay`.,
What should I do if the macvlan binary is not found under the node's `/opt/cni/bin`?,"If the macvlan binary is not found under the node's `/opt/cni/bin`, you need to manually download [cni-plugins](https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni -plugins-linux-amd64-v1.1.1.tgz) and extract it to each node.",
In what form does Macvlan exist in Kubernetes?,"In Kubernetes, Macvlan is just a binary file stored under `/opt/cni/bin` on each node, and there is no separate installation method.",
What is Macvlan?,Macvlan is a network card virtualization solution for Linux. It can virtualize one physical network card into multiple virtual network cards.,
How to collect indicators?,"Calico's monitoring indicators can be accessed through Prometheus, but the corresponding ServiceMonitor object needs to be created first.",
What are the important indicators?,"Important indicators include `felix_ipset_errors`, `felix_iptables_restore_calls`, `felix_iptables_restore_errors`, `felix_iptables_save_calls`, `felix_iptables_save_errors`, `felix_log_errors`, `ipam_allocations_per_node` and `ipam_blocks_per_node`",
How to create a `ServiceMonitor` object?,"Two ServiceMonitors, `calico-node` and `calico-kube-controllers`, need to be created.",
How to create a component's metrics service?,"Two services, `calico-felix-svc` and `calico-kube-controllers-svc`, need to be created.",
How to turn on the metrics of the Calico component?,"You can decide whether to turn it on through the `calico_felix_prometheusmetricsenabled` parameter, which defaults to false, or it can be turned on manually through patch.",
Will Host information be forwarded when mirroring a request?,You can specify the Host information used for forwarding by setting `nginx.ingress.kubernetes.io/mirror-host`.,
Is it possible to mirror the body of request traffic?,You can control whether to mirror the body of request traffic by configuring `nginx.ingress.kubernetes.io/mirror-request-body`.,
What configuration items are supported by the mirror target address?,"Mirror target addresses support Service and external addresses, and the URI of the original request can optionally be added to the end of the target URL.",
What is traffic mirroring?,Traffic mirroring is a powerful tool for verifying service functions and performance. It can copy real-time traffic and send it to the mirroring service without creating data and without affecting online access.,
What is a Submariner?,Submariner is a cross-cluster network communication solution.,
What does Spiderpool do?,Spiderpool can automatically manage IP resources.,
What components does Multus-underlay use for multiple network cards?,Multus-underlay uses Macvlan and SRIOV-CNI for multi-network card situations.,
What is MetalLB?,MetalLB is a bare metal version of the Kubernetes load balancer solution.,
Ingress-nginx is the Ingress controller hosted by which community?,"Ingress-nginx is an Ingress controller hosted by the Kubernetes community, using nginx as a reverse proxy and load balancing.",
What does f5networks do?,"f5networks can fully control F5 devices, synchronize service and ingress configurations in the cluster to F5 hardware devices, and achieve load balancing of the northbound entrance of the cluster.",
What is Contour?,Contour is an open source Kubernetes Ingress controller that uses Envoy as the data plane.,
What technology does Cilium use to build its network solution?,Cilium uses the eBPF kernel to build network solutions.,
What is Calico?,Calico is a Virtual Router implemented based on the Linux kernel and is used to complete data plane forwarding.,
What network components are supported by DCE 5.0?,"The network components currently supported by DCE 5.0 are Calico, Cilium, Contour, f5networks, Ingress-nginx, MetalLB, Multus-underlay, Spiderpool and Submariner.",
"In Calico, is different IPPoo subnet overlap supported?","not support. When creating an IPPoo, Calico will verify whether the CIDR of the IPPoo overlaps with the subnet of an existing IPPoo.",
How to delete the specified IPPool?,You can use the following command to delete the specified IPPoo:\n````shell\ncalicoctl delete ippool <ippool-name>\n```,
How to check the current status of all IPPools?,You can run the following command:\n```shell\ncalicoctl get ippool -o wide\n```,
How to disable an IPPool?,You can use the following command to set the specified IPPool to a disabled state:\n```shell\ncalicoctl patch ippool <ippool-name> -p "{"spec": {"disabled": true}}"\n```,
"In the network module DCE5.0, how to modify the settings of IPPool to migrate the Pod IP address?","1. Create a new IPPool. It is recommended that the new IPPool be in the CIDR of the Kubernetes cluster. \n2. Disable old IPPool. \n3. Re-create all Pods under the old IPPool. \n4. Verify the newly started Pod, observe whether the IP is assigned from the new pool and test connectivity. \n5. Delete the old IPPool.",
How to create a new IPPool pool?,Run command:\n````shell\ncat << EOF | calicoctl apply -f -\napiVersion: projectcalico.org/v3\nkind: IPPool\nmetadata:\nname: extra-ippool\nspec:\ncidr: 192.168.0.0 /20\nblockSize: 26\nvxlanMode: Always\nnatOutgoing: true\nEOF\n```,
How to set the size of the IP address pool?,`blockSize` defaults to 26 and can be controlled by `calico node env`: `CALICO_IPV4POOL_BLOCK_SIZE` (IPv4: 20-32; IPv6:116-128). Determined based on actual cluster size.,
How to view the default address pool?,Run command: `calicoctl get ippools default-ipv4-ippool -o yaml`,
What is IPPool?,IPPool represents the set of addresses from which Calico expects to assign IPs to Pods.,
How to use and fix the IP pool for a certain application load?,Just add the corresponding workload affinity to the IP pool at the same time.,
"Namespace affinity is added, and workload affinity or node affinity is added at the same time. What is the final effect?",Multiple affinity stacking must meet all affinities to use this IP Pool.,
How to deal with setting up a dedicated IP pool for only some applications in a certain namespace?,"You can add an IP pool with namespace affinity. When a workload in the namespace is created, the IPs in this IP pool can be used, and the IP pool with namespace affinity can be added to be shared within the specified namespace.",
How to deal with the problem that the subnets available on different Nodes are different when the cluster nodes are across subnets or across data centers?,"You can use the node affinity of the IP pool to schedule the same workload to nodes across subnets or across data centers; you can also schedule different workloads to different nodes by adding affinities in different namespaces, and use Different subnets.",
How to assign a specific fixed IP to a workload?,"You can manually create a fixed IP pool and specify the workload affinity of the IP Pool to be used, and select the corresponding fixed IP Pool when creating the workload; you can also automatically create a fixed IP pool, create a subnet and add the IP to be used, and then apply The administrator automatically creates a fixed IP Pool directly based on the created subnet.",
Introduce the usage scenarios of IP pool.,"There are two main usage scenarios for IP pools. One is that the workload uses fixed IPs, and fixed IP pools can be created manually or automatically. The other is that the node affinity and namespace affinity of the IP pool can be based on different Add affinities to workloads or nodes, namespaces, etc.",
How to create a LoadBalancer type Service in Metallb?,You can use the command kubectl create service loadbalancer or create a Service in a yaml file.,
What is the role of BGP Peer in Metallb?,"BGP Peer in Metallb is used to configure the configuration of the BGP session, including the peer BGP AS and IP.",
What does Metallb BGP Advertisement do?,"Metallb BGPAdvertisement is used to specify the address pool that needs to be announced through BGP, and can configure some properties.",
Does BGP mode in Metallb require hardware support?,"Yes, BGP mode requires hardware support to run the BGP protocol. If there is no hardware support, you can use software such as frr, bird, etc. instead.",
What is BGP Router?,BGP Router is a router that can access the cluster through the BGP protocol.,
What hardware support does MetalLB's BGP mode require?,"BGP mode requires hardware support to run the BGP protocol. If not, you can use software such as frr, bird, etc. instead.",
How to assign LoadBalancer IP to Service?,"Set `spec.type=LoadBalancer` in Service, and specify the address pool to which LoadBalancer IP belongs in annotations or `service.spec.loadBalancerIP`.",
How to create IP pool and LoadBalancerIP advertisement rules?,```yaml\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\nname: demo-pool\nnamespace: metallb-system\nlabels:\nipaddresspool: demo\nspec:\naddresses:\n- 192.168.10.0 /24\n- 192.168.9.1-192.168.9.5\n- fc00:f853:0ccd:e799::/124\nautoAssign: true\navoidBuggyIPs: false\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\ nname: demo\nnamespace: metallb-system \nspec:\nipAddressPools:\n- demo-pool\nipAddressPoolSelectors:\n- matchLabels:\nipaddresspool: demo\nnodeSelectors:\n- matchLabels:\nkubernetes.io/hostname: kind- control-plane\n```,
How to configure MetalLB's L2 mode and BGP mode?,"- Configure L2 mode: You need to create an IP pool and specify LoadBalancerIP notification rules. \n- Configure BGP mode: You need to create an IP pool and configure BGPAdvertisement to specify the address pool that needs to be advertised through BGP, and configure BGP on the node.",
How does MetalLB perform load balancing in L2 mode?,"In L2 mode, Metallb elects a node in the cluster as an externally exposed host for Service. Traffic will still be forwarded to the faulty node before the client updates the cache. The time for failover depends on how quickly the client updates its Mac address cache.",
What is the difference between MetalLB's L2 mode and BGP mode?,"The L2 mode only allocates the LoadBalancer IP in a layer 2 network, while the BGP mode can span multiple levels of networks and advertise through the BGP protocol.",
What is MetalLB?,MetalLB is a Kubernets load balancer that supports both Layer2 and BGP modes.,
How to access ClusterIP?,You can use the curl command on any computer with kubectl installed to test whether ClusterIP can be accessed normally. For example: curl [ClusterIP].,
How to access Calico Pods in the cluster?,"You can test whether the Calico Pod's IP address can be accessed normally by using the ping command in other Pods. For example: kubectl exec -it [POD] sh, and then use the ping command to test the IP address of the Calico Pod.",
How do I test whether the Multus-underlay network is accessible outside the cluster?,You can use the ping command on a computer outside the cluster to test whether the node's IP address can be accessed normally.,
How to test whether the Multus-underlay network is accessible within the cluster?,"You can create a Pod using the Multus-underlay network, and then use the ping command in the Pod to test whether the IP addresses of other nodes can be accessed normally.",
Which network module is used to assign IP addresses in DCE5.0?,"Spiderpool. In DCE5.0, you can allocate another MacVLAN network card (net1) to the Pod by specifying the MacVLAN Multus CRD, and assign an IP address to the MacVLAN network card by specifying the IPPool pool.",
How to test connectivity in DCE5.0?,"You can test connectivity within and outside the cluster, access Calico Pods in the cluster, and other operations.",
"In DCE5.0, how to create a workload to use MacVLAN?","Taking MacVLAN as an example, insert the following annotations in the Annotations of the Pod: annotations:k8s.v1.cni.cncf.io/networks or v1.multus-cni.io/default-network.",
"In DCE5.0, how to verify whether each component is running normally?","You can check whether each component is running normally, including Multus, Meta-plugins, SRIOV-CNI (if enabled), and SRIOV-Device-Plugins (if enabled).",
Which network module in DCE5.0 requires hardware support?,"SRIOV. In addition, enabling SRIOV requires hardware support. Before installation, you need to confirm whether the network card of the physical host supports SRIOV.",
How to verify whether Multus-underlay is installed successfully?,"Check whether each component is running normally, including Multus, Meta-plugins, SRIOV-CNI (if enabled), and SRIOV-Device-Plugins (if enabled).",
What are the specific steps to install Multus-underlay?,"1. Confirm that the cluster has been successfully connected to the container management platform\n2. Enter the Helm template, find and click multus-underlay\n3. Fill in the basic configuration information, including namespace, ready waiting, default CNI, cluster Service and Pod CIDR, MacVLAN Install (optional) and SRIOV (optional)\n4. Click Install",
What are the precautions for Multus-underlay?,"- Need to confirm whether there is a default CNI in the current cluster\n- Multus-underlay relies on Spiderpool as ipam. \n- If you want to install SRIOV-CNI, you need to confirm whether the node is a physical host and the node has a physical network card that supports SRIOV. \n- It is not recommended to install MacVLAN and SRIOV at the same time.",
How to do more advanced configuration?,You can configure it through YAML by clicking `YAML` in the Tab tab.,
What parameters can be configured for the ingress-nginx component?,"You can configure parameters such as the number of Ingress Controller copies, Metrics, ServiceMonitor, Ingress Class name, default Ingress Class, Election ID, Service's IP single and dual stack settings, Type, mirror warehouse, and mirror name.",
What are the specific steps to install ingress-nginx?,"1. Click `Container Management` -> `Cluster List` in the left navigation bar, and then find the name of the cluster where you want to install ingress-nginx. 2. Select `Helm Application` -> `Helm Template` in the left navigation bar, find and click `ingress-nginx`. \n3. Select the version you want to install in `Version Selection` and click `Install`. \n4. In the installation interface, fill in the required installation parameters. \n5. Click the `OK` button in the lower right corner to complete the creation.",
What conditions need to be confirmed before installing the ingress-nginx component?,It is necessary to confirm that the cluster has successfully connected to the `container management` platform.,
What is the main content of this document?,"This document introduces how to install the ingress-nginx component, and details the installation steps and installation parameters.",
What configuration parameters does Contour support?,"Multiple parameters can be configured when installing Contour, mainly including:\n- Global settings: uniformly set the image warehouse address\n- Contour control plane related configuration: number of copies, creation of CRD, Ingress Class related configuration, control plane Debug level log output, etc. \n- Envoy related configurations on the data plane: number of copies, Envoy deployment type, Host network, etc., access log level, service type, etc. \n- Soft affinity scheduling rule designation and weight\n- Metrics and Alert Configurations related configurations",
What steps are required to install Contour?,"- Select `Container Management` -> `Cluster List` in the left navigation bar of the `Container Management` platform\n- Select the cluster where you want to install Contour and enter `Helm Application` in the left navigation bar -> `Helm Template `\n- Select and click `contour`\n- Fill in the required installation parameters in the installation interface, including global and Contour control plane and data plane related configurations, and advanced configuration can be performed through YAML\n- Click the OK button to proceed create",
What is Contour?,Contour is a Kubernetes ingress controller used to direct external traffic to services in a Kubernetes cluster.,
Which tool does Kubespray call by default to install the cluster? How to check the Cilium parameters supported by Kubespray?,Kubespray calls Ansible by default to install the cluster. You can view the [Kubespray official documentation](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/cilium.md) to view the Cilium parameters supported by Kubespray.,
Does Cilium support custom parameters? How to set it up?,"Supports custom parameters, which can be set via the following parameters: cilium_config_extra_vars.",
How to turn on Hubble? What metrics are exposed by default?,"Hubble is installed and enabled by default. The exposed indicators are dns, drop, tcp, flow, icmp and http. It can be set through related parameters, such as: cilium_hubble_install, cilium_enable_hubble, cilium_enable_hubble_metrics and cilium_hubble_metrics.",
What values can be set for Cilium monitoring aggregation level?,"Supported values are ""none"", ""low"", ""medium"" and ""maximum"".",
How to set the time when Cilium DaemonSet is ready again?,Can be set via the following parameters: cilium_rolling_restart_wait_retries_count and cilium_rolling_restart_wait_retries_delay_seconds.,
What identity mode does Cilium use by default? How to change identity mode?,The "crd" mode is used by default. Can be set via the following parameter: cilium_identity_allocation_mode.,
How to set the name of Cilium cluster?,This can be set via the following parameter: cilium_cluster_name: default.,
How to enable IPV6?,"The dual-stack switch needs to be turned on through the interface, and the default IPv6 parameters are automatically turned on. It can also be set via the following parameter: cilium_enable_ipv6: true.",
What IPAM modes does Cilium support? Which one is used by default?,"Cilium supports multiple IPAM modes, including ""Cluster Scope"", ""kubernetes"" and modes customized by major public clouds. The ""Cluster Scope"" mode is used by default.",
How to set Cilium data mode?,"It can be set by the following parameters: cilium_tunnel_mode: vxlan, supported values are ""vxlan"", ""geneve"" and ""disabled"".",
What are the prerequisites for Cilium to be installed in DCE 5.0?,"It is necessary to confirm that the operating system Kernel version number is >=4.9.17, and the network CNI selected in DCE 5.0 is Cilium.",
What should we pay attention to in MetalLB ARP mode?,"The following should be noted in MetalLB ARP mode:\n- If ARP mode is enabled during installation, please enable ready waiting. \n- LoadBalancer Service, and announce all IP addresses in this pool through APR. \n-The address pool list can be configured with IPv4 and IPv6 addresses. \n-The input format of each address segment can be a legal CIDR (such as 192.168.1.0/24) or an IP range (such as 1.1.1.1-1.1.1.20). \n- Each address segment entered should belong to a real ""physical"" network segment of the cluster node, but should not conflict with an existing IP address. \n- The created IP pool has the default address pool parameter `autoAssign: true` turned on by default. Parameter details: [Instructions for use](usage.md)",
How can I get more information about this component?,You can refer to the F5 official documentation for more introduction to this component.,
"When installing in cluster forwarding mode, how do I need to configure VXLAN tunnels or BGP neighbors?","When installing in cluster forwarding mode, you need to configure VXLAN tunnels or BGP neighbors. Please refer to the configuration of the CNI component of K8S. For example, in the Calico scenario, you can refer to the F5 official documentation.",
How to enable application service to be assigned VIP by f5-ipam-controller?,"As long as the application service is annotated with the annotation cis.f5.com/ipamLabel: LabelName, it will be assigned a VIP by f5-ipam-controller, and will eventually take effect on the F5 device.",
How to set up BIGIP L4 IP Pool?,"BIGIP L4 IP Pool can be set in the F5 interface. Its format is similar to { LabelName: 172.16.1.1-172.16.1.5}, where LabelName will be used for the annotation when creating the application service.",
What parameters need to be set when installing the f5-ipam-controller component?,"When installing the f5-ipam-controller component, you need to set the image warehouse address (F5-ipam-controller Settings -> Registry), image name (F5-ipam-controller Settings -> repository) and image version (F5-ipam-controller version), as well as the storage pool size (storageSize) and storage pool name (storageClassName).",
What is the f5-ipam-controller component? How to enable it?,"f5-ipam-controller is a component for layer 4 load balancing in Kubernetes. It can be enabled in the F5 interface, controlled by the switch ""install f5-ipam-controller"".",
"In the F5 interface, which options require setting the login account and password of the F5 device?",BigIP Username and BigIP Password need to set the login account and password of the F5 device.,
What is load balancing mode?,"Load balancing mode refers to the mode of F5 forwarding traffic, including nodePort and cluster modes.",
How to set the node label selector when installing components in the cluster?,It can be set in the `Node Label Selector` of the installation parameter interface.,
How to choose the forwarding mode when installing components in the cluster?,"You can choose the 4-layer load balancing mode or the 7-layer load balancing mode, which can be set in the `Forward Method` of the installation parameter interface.",
What information needs to be filled in the installation parameter interface?,"The following information needs to be filled in:\n-Installation name, namespace, version. \n- f5-bigip-ctlr The repository address, image name and version of the image. \n- The WEBUI login address and Partition name of the F5 device. \n- When this component is installed in Layer 7 load balancing mode, an ingress entrance VIP on F5 needs to be set up. \n- When this component is installed in Layer 4 load balancing mode, you need to set up a VIP address pool for F5's Layer 4 load balancing. \n- Whether to install the f5-ipam-controller component, the f5-ipam-controller image's warehouse address, image name and version, storageClass Name, storage pool size and other information.",
What are the steps to install F5network components?,"The steps are as follows:\n1. Log in to the global management interface of the DCE cluster, enter `Container Management` -> `Cluster List`, and log in to the cluster where you want to install this component. \n2. In `Helm Application` -> `Helm Template`, select the `system` warehouse and the `Network` component, and click to install `f5network`. \n3. In the installation parameter interface, fill in the relevant information and complete the installation.",
Do I need to install [f5 ipam controller](https://github.com/F5Networks/f5-ipam-controller) when installing the 7-layer load balancing mode?,unnecessary.,
How to create an independent partition?,"Log in to the `System` -> `Users` -> `Partition List` interface on the F5 device, click `create` in the upper right corner, fill in the Partition name and complete the creation.",
How to install AS3 service on F5 device?,"The specific steps are: download the matching version of the AS3 RPM package to the local computer, log in to the management Web UI of the F5 device, in the `iApps -> Package Management Lx` interface, click `import` in the upper right corner, and upload the installation package.",
Why do you need to install AS3 and k8s bigip ctlr?,AS3 provides a configurable interface for the remote network of the F5 device. k8s bigip ctlr is the F5 control plane component on the K8S cluster. Both have version matching requirements with the versions of the F5 device and K8S.,
What are F5network components?,The F5network component is a component deployed on a Kubernetes cluster to implement load balancing and traffic management of applications.,
Does Calico support exposing Felix metrics?,"Yes, Felix metrics can be exposed by configuring `calico_felix_prometheusmetricsenabled` to true.",
How to install Calico using Kubespray?,You can add and fill in the required parameters in the custom parameters under advanced configuration.,
What parameters can be configured when installing Calico?,"Parameters such as dual stack, network card detection mode, IPtables mode, tunnel mode, IPv4 container network segment, IPv6 container network segment and service network segment can be configured.",
What two packet encapsulation modes does Calico support?,"Calico supports both IPIP and VXLAN packet encapsulation modes, and both support Always and CrossSubnet modes.",
How to install Calico in DCE 5.0?,"Under the create cluster page, select the network plug-in as `calico`.",
How to set up IPv6 ippool gateway in Cluster Default Ippool Installation?,"Set the IPv6 gateway in the Cluster Default Ippool Installation, for example `fd00::1`, the IP address should belong to the `IPv6 ippool subnet`.",
How to set up the IPv4 ippool gateway in Cluster Default Ippool Installation?,"Set the IPv4 gateway in Cluster Default Ippool Installation, for example `192.168.0.1`, this IP address should belong to `IPv4 ippool subnet`.",
How to set up IPv6 ippool subnet in Cluster Default Ippool Installation?,"Set the IPv6 subnet number in the default pool in Cluster Default Ippool Installation, for example `fd00::/112`.",
How to set up IPv4 ippool subnet in Cluster Default Ippool Installation?,"Set the IPv4 subnet number in the default pool in Cluster Default Ippool Installation, for example `192.168.0.0/16`.",
How to set the IPv6 ippool name in Cluster Default Ippool Installation?,Set the IPv6 ippool name in Cluster Default Ippool Installation.,
What is IPv4 ippool?,IPv4 ippool is a mechanism for allocating IP addresses to Pods in a Kubernetes cluster.,
What is the role of enabling IPv4 and IPv6 support in IP Family Setting?,"After IPv4 and IPv6 support is enabled, when assigning an IP to a pod, it will try to assign an address of the corresponding version, otherwise the pod will fail to start.",
What logic is Spiderpool Controller responsible for?,Spiderpool Controller is responsible for the controller logic of Spiderpool.,
How to install Spiderpool?,"1. Click `Container Management` —> `Cluster List` in the left navigation bar, and then find the name of the cluster where Spiderpool is to be installed. \n2. Select `Helm Application` -> `Helm Template` in the left navigation bar, find and click `spiderpool`. \n3. Select the version you want to install in the version selection and click Install. \n4. In the installation interface, fill in the required installation parameters. \n5. Click the OK button in the lower right corner to complete the installation.",
What are the prerequisites for Spiderpool?,"To use Spiderpool, you need to combine Macvlan+Multus+Calico/Cilium, and you need to install Multus and Underlay CNI first, and confirm the network card interface and subnet to be used.",
What drivers are supported in tunnel mode? Which driver is used by default?,"In tunnel mode, supported drivers are: `libreswan` (default), `wireguard`, `vxlan`.",
"If you need to enable the globalnet function, which field needs to be configured? What should I do if I don't use this feature?","If you need to enable the globalnet function, you need to configure the `globalCidr` field. If you do not use this feature, no configuration is required.",
What does `serviceCidr` in `Submariner` mean?,`serviceCidr` in `Submariner` refers to the CIDR of the subcluster Service.,
What does `clusterCidr` in `Submariner` mean?,`clusterCidr` in `Submariner` refers to the CIDR of the subcluster Pod.,
What is clusterId? How to fill it out?,`clusterId` is used to identify the sub-cluster and needs to meet the specifications of DNS-1123 Label to be filled in.,
"If the cluster CNI is Calico, what needs to be done to resolve compatibility issues with Calico?","If you need to do some additional operations, please refer to the instruction manual.",
Please explain how to check whether the submariner-operator component is running normally.,Execute the command "kubectl get po -n submariner-operator" to check the status of the component.,
Please explain how to add the gateway node label "submariner.io/gateway: true".,"On the node that needs to be set as a gateway node, execute the command ""kubectl label nodes <node-name> submariner.io/gateway=true"".",
Please explain what ceIPSecPSK is in submariner-operator.,ceIPSecPSK is the pre-shared key required to establish an IPsec tunnel.,
Please explain how to obtain the Token of the Broker cluster API-Server.,"On the cluster where submariner-k8s-broker is installed, execute the command ""kubectl -n submariner-k8s-broker get secrets -o jsonpath=""{.items[?(@.metadata.annotations[""\""""kubernetes\.io/ service-account\.name""\""""]==""submariner-k8s-broker-client"")].data.token}"" | base64 --decode"", please note that decode needs to be executed.",
How to configure the submariner-operator to connect to the Broker cluster?,"Find the submariner-operator in Helm and select the version to install. It is recommended to install it under the submariner-operator namespace, and then configure the configuration of `submariner-operator` to connect to the Broker cluster. You need to provide the Broker cluster API-Server address, client certificate and Token.",
How to install submariner-k8s-broker?,"To install submariner-k8s-broker, you need to find submariner-k8s-broker from the Helm application and select the version to install. It is recommended to install it under the submariner-k8s-broker namespace.",
What are the requirements for Submariner deployment?,"Submariner deployment needs to meet the following conditions: it is best not to overlap the subnets of different sub-clusters, the supported CNI list is specific, the mode of Kube-proxy must be `IPtables`, `IPvs` mode is not currently supported, and it is required within the cluster To allow `Vxlan` traffic, the udp/4500 port needs to be opened outside the cluster.",
What CNIs does Submariner support?,"Submariner supports the following CNIs: OpenShift-SDN, Weave, Flannel, Canal, Calico, OVN.",
How many clusters can Submariner be deployed on? What are the differences?,"Submariner can be deployed on at least two clusters, submariner-k8s-broker can be deployed in one of the clusters, and submariner-operator is deployed in the sub-cluster of each join.",
"If there is no Pool Member on the F5 side, how to solve it?","The Service should be checked to see if it has an Endpoint. If not, there will be no Pool Member on the F5 side.",
"When `cis.f5.com/ipamLabel: LabelName` is specified in the Service but cannot be assigned an IP, what are the possible causes of this problem?",There may be other loadbalancer components in the cluster that prevent the F5 IPAM component from assigning an IP address to the Service. Please uninstall other loadbalancer components.,
Which service will be installed when this component uses Layer 4 load balancing mode? What are the requirements for this service?,"When using the 4-layer load balancing mode, `f5 ipam controller` will be installed. This service requires the cluster to have storage components and provide PVC services.",
What are the frequently asked questions about F5network installation?,"After installing the F5network component, the `f5 ipam controller` service cannot be started. `cis.f5.com/ipamLabel: LabelName` is specified in the Service, but it cannot be assigned to an IP. There is no Pool Member on the F5 side, and F5 traffic cannot be forwarded to the cluster node. on the nodePort.",
"In the DPDK test Pod, which network card does not have any network information? Why?","In the DPDK test Pod, the net1 network card does not have any network information. This is because the characteristics of DPDK make it work without relying on the kernel network protocol stack.",
How to create a DPDK test Pod?,"You can create a Pod and specify the network to be used as the NetworkAttachmentDefinition created above. You can also specify parameters such as resources and DPDK applications used by the container. After the Pod is running, you can enter the Pod and run the dpdk-app command to view network information.",
How to create a Multus DPDK CRD?,You can configure the DPDK network by creating a NetworkAttachmentDefinition resource and specifying the sriov_netdevice_dpdk resource name and other related parameters.,
How to let sriov-device-plugin find the VF that supports dpdk?,"You can edit the configmap of sriov-device-plugin, add a resource pool named sriov_netdevice_dpdk, and specify the drivers as ""vfio-pci"". And restart sriov-device-plugin.",
How to switch the network card driver to vfio-pci?,"Binding can be done using the dpdk-devbind.py command. At the same time, you also need to set up large page memory and enable IoMMU technology.",
How to create a DPDK test Pod?,"You need to create a Pod through kubectl, specify the sriov-dpdk network, and mount the relevant volume. Containers need to run in privileged mode.",
How to set up the DPDK environment?,"You need to install Multus-underlay, enable the installation of SRIOV components, and switch the network card driver to a user-mode driver. At the same time, you need to configure SRIOV-Device-Plugin and create Multus DPDK CRD.",
In what areas does DPDK have advantages?,"DPDK has excellent performance in data packet processing, network I/O, memory management, etc.",
What is DPDK?,DPDK is a high-performance packet processing library for building fast data plane applications.,
"When using the Overlay + Underlay solution in a production environment, what is the role of network card 2?","When using the Overlay + Underlay solution in a production environment, network card 2 carries Underlay CNI (Macvlan/SRI-OV) Pod traffic.",
"When using the Overlay + Underlay solution in a production environment, how many physical network cards/Bond network cards should be deployed in the physical machine/virtual machine environment?","When using the Overlay + Underlay solution in a production environment, 2 or 3 physical network cards/Bond network cards must be deployed in the physical machine/virtual machine environment.",
"In the Overlay-only scenario, what network components need to be deployed in the physical machine/virtual machine/public cloud environment?","In the Overlay-only scenario, the component Calico/Cilium needs to be deployed.",
What is the Overlay + Underlay solution?,The Overlay + Underlay solution refers to the network solution used when cluster deployment applications need to use a combination of multiple CNIs and use Underlay CNI (Macvlan/SRI-OV) IP to provide external communication.,
What is the Overlay-only scenario?,The Overlay-only solution means that the application Pods in the cluster only need to use a single CNI and do not need to provide a network solution for external access.,
How can different tenants use different Ingres load traffic without specifying ingressClassName?,"Different instances can watch different namespaces by specifying `--watch-namespace`. ingress-nginx can be installed through helm by specifying `controller.scope.enabled=true` and `--set controller.scope.namespace=$NAMESPACE`. For more information, please refer to [scope](https://kubernetes.github. io/ingress-nginx/deploy/#scope).",
Is there a default IngressClass? How to use default IngressClass?,"Each cluster can have a default IngressClass. When a default IngresClass exists, you do not need to specify the `ingressClassName` field when creating Ingres. At the same time, when an `IngresClass` resource sets the annotation to `true`, new ingress resources without a specified class will be assigned to this default class.",
How to create an Ingress with specified IngressClass through the interface?,"When creating a route, you can directly enter the corresponding `IngressClassName` in the interface.",
How to create an Ingress specifying IngressClass through YAML?,The corresponding IngressClass can be specified through `ingressClassName` in the YAML file.,
What are the applicable scenarios for IngressClass?,"Applicable scenarios include internal and external Ingress requirements in the same cluster, different teams deploying different applications in the same namespace and using different Ingress instances, and the same team deploying different applications in the same cluster that have requirements for Ingress instance resource allocation.",
What is IngressClass?,IngressClass represents the class of Ingress instances and is used to create different Ingress rule instances to meet different needs.,
What is the role of Spiderpool in Kubernetes?,Spiderpool is used to manage network resources in Kubernetes clusters.,
What is a configuration example for MacVLAN CNI?,"Please refer to the following example:\n```json\n{\n""cniVersion"": ""0.3.1"",\n""type"": ""macvlan"",\n""mode"": ""bridge"",\n""master "": ""eth0"",\n""name"": ""macvlan-cni-default"",\n""ipam"": {\n""type"": ""spiderpool""\n}\n}\n```",
What is MacVLAN CNI?,MacVLAN CNI is a CNI plugin for connecting containers to physical networks.,
Where do I edit the CNI configuration file?,Edit the CNI configuration file in the `/etc/cni/net.d/` directory.,
What is Spiderpool?,Spiderpool is a network resource management software used to manage network resources in Kubernetes clusters.,
How do I configure the server to allow requests from origins?,"You can configure the origin of requests allowed by the server through `nginx.ingress.kubernetes.io/cors-allow-origin`. For example, `https://example.com, https://www.example.com` means that only requests from Requests from these two addresses will be allowed.",
How to configure whether sending credentials during cross-domain is allowed?,"You can configure whether the browser is allowed to send credentials (Credentials) through `nginx.ingress.kubernetes.io/cors-allow-credentials`, and you need to add `Access-Control-Allow-Credentials: true` to the server response header.",
How to configure the cache duration of preflight requests?,"You can configure the cache duration of preflight requests through `nginx.ingress.kubernetes.io/cors-max-age`, thereby reducing the number of queries to the server and improving web page performance.",
How to configure which methods are accepted across domains?,"You can control which methods are accepted across domains through `nginx.ingress.kubernetes.io/cors-allow-methods`. The default is: `GET, PUT, POST, DELETE, PATCH, OPTIONS`.",
How to configure cross-domain examples?,"After installing nginx-ingress in Kubernetes, you can handle cross-domain issues by configuring the Nginx configuration of Ingress CR. Please refer to the example above for specific methods.",
What is cross-domain?,"Cross-domain means that in the browser, requests between resources under different domain names will encounter cross-domain restrictions.",
How to create subnet and IPPool via YAML?,Define SpiderSubnet and SpiderIPPool objects respectively in YAML and fill in the corresponding parameters.,
Is creating an IP pool an optional step? Why create an IP pool?,"This is an optional step. IP pools can achieve refined management and control of IP resources, such as fixed IP resources. If coarse-grained control of IP is required, there is no need to create an IP pool.",
How to create subnets and IP pools?,"In the DCE UI, go to Container Network -> Network Configuration -> Create Subnet, enter the subnet information and IP segment, and then create an IP pool through the subnet details page.",
How to automatically create a fixed IP pool?,"When creating a custom workload, add specific Annotation and specify the network card using the fixed IP pool, the subnet to be used, and the number of elastic IPs. After deployment, check the Clonset status and check the IP Pool status.",
How to use the fixed IP pool that has been created?,"Specify the default network type, VLAN ID, subnet interface and IP pool information in the custom resource `CloneSet`, and after deploying the resource, view the status and enter the container network management interface, find the corresponding subnet and view IP usage.",
What prerequisites are required to implement the actions described in this article?,"1. SpiderPool has been successfully deployed; 2. Multus with Macvlan /SRI-OV has been successfully deployed; 3. If you use manual selection of IP pools, you need to create IP subnets and IP pools in advance.",
This article describes which tools are used to configure IP Pool for custom workloads?,This article introduces the combination of Multus and Underlay CNI plug-ins to configure IP Pool for a custom workload (this article uses the workload CloneSet created by the OpenKruise controller) Pod.,
How to manually release an IP?,Manually release by running the command calicoctl ipam release --ip=10.244.0.100.,
What can the `ipam` operation of `calicoctl` do?,"The `ipam` operation of `calicoctl` can view the currently allocated block and its usage, check whether an IP is being used, and release an IP.",
What resource operations can `calicoctl` perform?,"`calicoctl` can perform resource operations: get, create, patch, delete, apply, replace.",
How to use tcpdump to capture packets to troubleshoot Calico network?,"Use tcpdump to capture packets on the veth virtual network card, tunnel network card, host network card or inside the Pod, and observe at which stage the packets are discarded.",
How to confirm whether the tunnel interface is working properly?,"Check the status of `vxlan.calico` and whether its state is `UNKNOWN`; check whether the tunnel IP is normal and should be within the IP range of the `Block` to which this node belongs. Check whether the routing is normal: whether the target Pod IP exists in the static route of this node, and the block corresponding to the next hop corresponding to the target address is on the same node.",
How to confirm whether the IP pool address is used up?,Calculate the total number of IPs; calculate how many Blocks there are (a `Block` has 2 ^ (32 - 26) = 64 addresses); confirm whether the `Block` has been allocated.,
What are the steps to troubleshoot the Calico network?,Confirm whether the IP pool address is used up; confirm whether the tunnel interface is working properly; confirm by capturing packets with tcpdump; view `iptables` rules; use `calicoctl` command.,
What is Calico Network?,"Calico Networking is an open source container networking solution that can be used for applications and data centers across containers, hosts and cloud platforms. It uses standard routing protocols and technologies such as BGP and Linux kernel routing tables to provide high performance, high reliability and secure container networking.",
"What are SR-IOV and eBPF, and why do we need to integrate these technologies on the Kubernetes platform?","SR-IOV (Single Root I/O Virtualization) is a hardware-based virtualization acceleration network solution, while eBPF (extended Berkeley Packet Filter) is a network acceleration solution. On the Kubernetes platform, as resource-sensitive applications such as databases and machine learning begin to run, these applications have higher requirements for computing power, network performance, and latency. Therefore, technologies such as SR-IOV and eBPF need to be integrated to improve performance.",
Why are cybersecurity regulatory requirements important on containers and cloud-native platforms?,"Containers and cloud-native platforms help enterprises realize automated application deployment and bring huge business benefits. However, the Pods within the platform are all flat, and there is a lack of horizontal network security isolation. Therefore, when building a container cloud platform, more fine-grained network policies need to be considered to improve network security.",
Why is interconnection inside and outside the cluster important in the process of traditional application microservices?,"In the process of traditional application microservices, users will gradually containerize some applications. Some containerized applications require externally accessible IPs to enable interconnection within and outside the cluster. Therefore, such applications also need to use fixed and externally accessible IPs.",
"In the process of cloud-native traditional applications, what kind of IP management capabilities do users often need?","In the process of cloud-native traditional applications, users often require more flexible and efficient IP management capabilities. In the process of migrating to the cloud in some traditional industries such as manufacturing, education, and energy, applications are often not transformed by microservices, and many applications still need to be accessed through a fixed IP. Moreover, such IPs also require strong control, such as strict firewall control.",
What is multi-cloud hybrid cloud? Why is the network the key to the implementation of the overall solution?,"Multi-cloud hybrid cloud means that users gradually adopt multi-cluster and cross-cloud deployment to cope with scenarios such as high availability, disaster recovery, and surge in business traffic. The network is the key to the implementation of the overall solution, because in heterogeneous environments, interconnection of heterogeneous cluster networks is required, and the ability to discover public services across clusters is required.",
"In cloud-native container networks, why is it an opportunity and challenge to combine hardware-based virtualization acceleration network solutions such as SR-IOV and network acceleration solutions such as eBPF?","Because the combination of these new technologies brings high-performance and low-latency network requirements, it also needs to solve new challenges such as software and hardware integration and network acceleration.",
"In a multi-cloud and multi-cluster environment, how to unify the distribution of network security policies for different clusters?",Unified management of network security policies and network traffic is required.,
Why does the use of Underlay IP require strict planning and allocation?,Because IP resources in Underlay CNI are relatively scarce and require strict firewall control.,
How to prevent the waste of IP resources in Underlay CNI?,"When the IP address release and allocation fails, it needs to be recycled in time.",
"In certain scenarios, it is difficult for a single network CNI to meet business needs. What strategy should be adopted?","In specific scenarios, specific CNIs need to be used to meet business requirements. In more complex scenarios, CNI combination is also required.",
"In a multi-cloud and multi-cluster environment, why should IP management and planning be based on a multi-cluster perspective?","Avoid IP conflicts and network segment conflicts (Underlay IP, Service IP, etc.).",
"In a multi-cluster environment, how to achieve interconnection among clusters?",Issues such as Cluster IP interoperability and DNS interoperability need to be solved.,
"What challenges does the network face in a multi-cloud, multi-cluster environment?","Networks face challenges such as connectivity, IP resource management, CNI diversity, IPAM mechanisms, network security policies and unified management of network traffic under multi-cloud and multi-cluster conditions.",
Which submodule performs best when Service and nodes are located on different nodes?,Performs best in calico(ipip).,
"In the DCE5.0 network module, which sub-modules have been tested for netpref latency (short connection)?","cilium(vxlan), calico(vxlan), calico(ipip), calico(underlay), macvlan-standalone, sriov-standalone.",
"In the DCE5.0 network module, which sub-modules have been tested for netpref latency (long connections)?","cilium(vxlan), calico(vxlan), calico(ipip), calico(underlay), macvlan-standalone, sriov-standalone.",
"In the DCE5.0 network module, which sub-modules have been tested for their Service-related throughput?","cilium(vxlan), calico(vxlan), calico(ipip), calico(underlay), macvlan-standalone, sriov-standalone.",
What sub-modules are used and deployed in the DCE5.0 network module?,"There is no relevant information provided in the question, so it cannot be answered.",
What conclusions can be drawn from the test results?,"The test results show that different CNI modes perform differently in different test cases. For example, when located on the same node, macvlan-standalone mode performs best; when located on different nodes, calico-ipip, calico-underlay and macvlan-standalone modes perform better; in terms of short links, macvlan Mode performs best and so on.",
What are the test indicators? How to measure?,"Test indicators include: Pod throughput, Service throughput, latency-long connection and latency-short connection. Among them, the throughput is measured using the netperf command, and the delay is measured using the TCP_RR (long connection) and TCP_CRR (short connection) parameters in the netperf command.",
What performance testing tool was used for this test?,This test uses the netperf tool for performance testing.,
What CNI types and test cases are covered in this test report?,"This test report covers three CNI types: Cilium, Calico and Macvlan. The test cases include between nodes, between Pods and the nodes where the Pods are located, between Pods and nodes across nodes, between Pods and Pods on the same node, and between Pods and Throughput and latency of communication between Services.",
How to set the IP to be used by a Pod?,"Use the command spiderpoolctl ip set and specify the IP to be set, the Pod name, the namespace where the Pod is located, the container ID of the Pod, the name of the node where it is located, and the network interface of the effective IP.",
How to release an IP?,Use the command spiderpoolctl ip release and specify the IP to be released. You can optionally use the --force parameter to force the release of the IP.,
How to use spiderpoolctl to display the Pods using a certain IP?,Use the command spiderpoolctl ip show and specify the IP to be displayed.,
What does spiderpoolctl gc do?,spiderpoolctl gc is used to trigger a garbage collection request to spiderpool-controller.,
What is spiderpoolctl?,spiderpoolctl is a program used on the CLI command line to debug the spiderpool-controller in the DCE 5.0 network module.,
What should I pay attention to when installing Falco-exporter?,"Before installation, you need to confirm that the cluster has successfully connected to the container management platform, install and run Falco and enable gRPC output. When filling in the installation parameters, you need to pay attention to setting the image warehouse address and image name, and set Prometheus ServiceMonitor and PrometheusRules according to your needs.",
How to install Falco-exporter?,"You need to install and run Falco first and enable gRPC output, and then install it on the Kubernetes cluster through Helm. Specific steps include:\n1. Enter the cluster list interface of the container management platform and select the cluster to be installed. \n2. Select falco-exporter in the Helm template and select the version and click Install. \n3. Fill in the required installation parameters, including application name, namespace, version, etc. \n4. Click OK to complete the installation.",
What is Falco-exporter?,Falco-exporter is an exporter that converts Falco output events into Prometheus Metrics.,
What content needs to be filled in the installation parameters?,"Application name, namespace, version, as well as image address, image name, Driver Kind, Log Level, etc.",
How to install Falco?,Falco needs to be installed through the Helm application. The specific steps are: select the cluster -> select the Helm template -> select the Falco version -> fill in the installation parameters -> click OK.,
What is Falco?,Falco is an open source cloud-native security tool.,
What parts does Falco consist of?,"Falco consists of user space programs, configurations, drivers and plug-ins.",
What is a Falco alarm?,"Alerts are downstream operations that can send alerts to standard output, a file, the system log, the HTTP[s] endpoint, etc.",
What are Falco rules?,Falco rules define behaviors and events that should be monitored and can be written in a rules file or a general configuration file.,
What events can Falco detect?,"Falco can detect any behavior involving Linux system calls, such as running shells in containers, running containers in privileged mode, reading sensitive files, etc.",
What is Falco?,"Falco is a cloud-native runtime security tool for detecting anomalous activity in containers, applications, hosts, and networks.",
What security scanning methods are supported in the microservice security features of DCE 5.0?,"Among the microservice security features of DCE 5.0, security scanning methods such as automatic scanning, manual scanning and periodic scanning are supported for services and APIs in the cluster. It supports all traditional web scanning projects including XSS vulnerabilities, S",
What intelligent recommendation functions does the micro-isolation feature of DCE 5.0 support?,"The micro-isolation feature of DCE 5.0 supports recording historical access traffic to resources, and can intelligently make policy recommendations based on historical access traffic when configuring isolation policies for resources.",
What are the container decoy-related functions among the runtime security features of DCE 5.0?,"Among the runtime security features of DCE 5.0, it has container bait warehouse and container bait deployment capabilities. It can customize new bait containers and supports alerting on suspicious behaviors in container baits.",
How to ensure transmission security in the image security feature of DCE 5.0?,"Among the image security features of DCE 5.0, in order to achieve secure transmission of images, key pairs and signature information are required to ensure transmission security. It has the ability to select a key for image signing when transferring images.",
For which technologies does DCE 5.0 provide fully automated security implementations?,"DCE 5.0 provides comprehensive automated security implementation for containers, Pods, images, runtimes, and microservices.",
How to remove StorageClass?,`kubectl get sc -o name | grep hwameistor-storage-lvm-| xargs -t kubectl delete`.,
"What are the commands to remove CRD, Hook and RBAC?","`kubectl get crd,mutatingwebhookconfiguration,clusterrolebinding,clusterrole -o name | grep hwameistor | xargs -t kubectl delete`.",
How to delete all LocalVolumeGroup instances?,`kubectl delete localvolumegroups.hwameistor.io --all`.,
How to remove a namespace?,`kubectl delete ns hwameistor`.,
What is the command to delete a Helm instance?,`helm delete -n hwameistor hwameistor`.,
How to uninstall HwameiStor?,"1. Delete the Helm instance: `helm delete -n hwameistor hwameistor`. \n2. Remove the namespace: `kubectl delete ns hwameistor`. \n3. Delete the `LocalVolumeGroup` instance: `kubectl delete localvolumegroups.hwameistor.io --all`. \n4. Remove CRD, Hook and RBAC: `kubectl get crd,mutatingwebhookconfiguration,clusterrolebinding,clusterrole -o name | grep hwameistor | xargs -t kubectl delete`. \n5. Remove StorageClass: `kubectl get sc -o name | grep hwameistor-storage-lvm-| xargs -t kubectl delete`.",
How to specify a new configuration file for upgrade?,Use `-f new.values.yaml` in the command `helm upgrade -n hwameistor hwameistor -f new.values.yaml` to specify a new configuration file.,
Do I need to manually restart the Pod to upgrade?,"There is no need to manually restart Pods, Helm will automatically restart each HwameiStor Pod in a rolling manner.",
Will upgrading HwameiStor affect services?,"During the upgrade, although the Pods will be restarted, these volumes will continue to serve the Pods uninterrupted, so there will be no impact on service.",
What happens during the HwameiStor upgrade process?,"During the upgrade process, each HwameiStor Pod will be restarted in a rolling manner, but the volumes will continue to serve the Pods uninterrupted.",
How does Helm upgrade HwameiStor?,Run the command `helm upgrade -n hwameistor hwameistor -f new.values.yaml` to upgrade.,
How many different types of volumes can a single Pod use simultaneously?,A single Pod can use any number of volumes of different types simultaneously.,
How to use volumes in Pods?,"Set the volumes provided for the Pod in the `.spec.volumes` field, and declare the mounting locations of the volumes in the container in the `.spec.containers[*].volumeMounts` field.",
What are persistent volumes? How is it different from scratch volumes?,Persistent volumes can outlive the Pod and are not destroyed even when the Pod no longer exists. The temporary volume has the same life cycle as the Pod and is destroyed when the Pod is destroyed.,
What types of volumes does Kubernetes support?,"Kubernetes supports many types of volumes, including: empty directory volumes, host path volumes, Git repository volumes, NFS volumes, iSCSI volumes, etc.",
What is the purpose of Kubernetes volumes?,Kubernetes volumes are used to solve the problem of file loss in containers and the problem of multiple containers sharing files.,
"Can HwameiStor provide non-low-latency, high-throughput data services?","Yes, HwameiStor can aggregate HDD, SSD, and NVMe type disks to provide low-latency, high-throughput data services.",
Does HwameiStor support data synchronization across node replicas? Why?,"Yes, HwameiStor uses cross-node replicas to synchronize data to achieve high availability. When a problem occurs, the application will be automatically scheduled to a high-availability data node to ensure application continuity.",
What features does HwameiStor have?,#NAME?,
What is HwameiStor?,HwameiStor is a Kubernetes-native container-attached storage (CAS) solution that provides distributed local data volume services and data persistence capabilities for stateful cloud-native applications or components.,
How to observe VG?,This can be observed using the vgdisplay command on a Kubernetes Worker node.,
How to verify StorageClass?,You can use the kubectl get sc command to verify.,
How to verify LocalDiskClaim object?,You can use the kubectl get ldc command to verify.,
How to create a storage pool?,"It can be created by creating a LocalDiskClaim object. For specific methods, please refer to the storageNodes parameter in the yaml file.",
What are the data import and export methods of TiDB on hwameiStor?,"Supports logical export and import at table, schema, and database levels, including using SELECT INTO OUTFILE and LOAD DATA INFILE for logical import and export, and using the online tool TiDB Lightning for physical full backup and incremental backup.",
What are the slow log query steps for TiDB on hwameiStor?,Adjust the slow query threshold to the specified value and execute S,
What sub-modules does the storage module of TiDB on hwameiStor include?,It includes two sub-modules: storage docking and HwameiStor.,
What security features does TiDB on hwameiStor support?,"Supports security features such as password security policy, SSL/TLS encryption, IP whitelist, and operation logging.",
What data backup and recovery methods does TiDB on hwameiStor support?,"It supports full backup and incremental backup through the online backup/restore tool TiDB Lightning, and also supports logical backup and restore through mysqldump.",
How to obtain slow query information in TiDB on hwameiStor?,You can obtain it by adjusting the slow query threshold to a specified time and then checking the slow query information in the log/system table/dashboard.,
Does TiDB on hwameiStor support data import and export?,"Supports logical export and import at table, schema, and database levels.",
Does TiDB on hwameiStor support operation logging?,Record key operations or error operations performed by users through the operation and maintenance management console or API.,
Does TiDB on hwameiStor support whitelist?,Supports IP whitelist function and IP segment wildcard operation.,
What is the access control method of TiDB on hwameiStor?,"Database data is controlled by granting basic add, delete, modify, and query access rights.",
How is the account management and permission testing of TiDB on hwameiStor?,"It supports the creation, modification and deletion of accounts, configuration and passwords, and supports the separation of security, auditing and data management. According to different accounts, the permission control for each level of the database includes: instance/library/table/column level.",
What distributed complex query capabilities does TiDB on hwameiStor support?,"It supports distributed complex queries and operations such as cross-node join, and supports window functions and hierarchical queries.",
What is the lock support of TiDB on hwameiStor?,"Describes the implementation of locks, blocking situations in RR/RW/WW situations, and deadlock handling methods.",
What is the character set support of TiDB on hwameiStor?,Currently only UTF-8 mb4 character set is supported.,
What transaction isolation levels does TiDB on hwameiStor support?,Supports si isolation level and rc isolation level (4.0 GA version).,
What common functions does TiDB on hwameiStor support?,Supports standard database functions.,
What execution plan binding functions does TiDB on hwameiStor support?,Supports binding features.,
What execution plan analysis functions does TiDB on hwameiStor support?,Supports execution plan parsing.,
What expressions does TiDB on hwameiStor support?,"Supports if, casewhen, forloop, whileloop, loop exit when and other statements.",
What index types does TiDB on hwameiStor support?,"Supports unique, clustered, partitioned, Bidirectional indexes, Expression-based indexes, hash indexes and other types of indexes.",
How to mount HwameiStor storage after configuring it?,You can use the `kubectl get po` command to view the basic instance and the `kubectl get po basic-tikv-0 -oyaml` command to view the mounting status.,
How to test the execution plan parsing and execution plan binding of a distributed database?,"You can execute the following statements for testing:\n1. explain analyze select * from t_test where id NOT IN (1,2,4);\n2. explain analyze select * from t_test a where EXISTS (select * from t_test b where a.id =b.id and b.id<3);\n3. explain analyze SELECT IF(id>2,""int2+"",""int2-"") from t_test.",
How to test expression support for a distributed database?,"You can execute the following statement for testing:\nSELECT CASE id WHEN 1 THEN ""first"" WHEN 2 THEN ""second"" ELSE ""OTHERS"" END AS id_new FROM t_test;\nSELECT IF(id>2,""int2+"",""int2-"") from t_test;",
"After completing the deployment of the database cluster, what basic capability tests need to be done?",The following are the basic capability tests that need to be done:\n1. Distributed transactions\n2. Object isolation\n3. Table operation support\n4. Index support\n5. Expression support\n6. Execution plan analysis and execution plan binding.,
How to configure HwameiStor storage?,PVC can be created for tidb-tikv and tidb-pd respectively from `storageClass local-storage-hdd-lvm`.,
How to connect to TiDB cluster?,You can connect to the TiDB cluster through the following steps:\n1. Install mysql-client\n2. Execute the command port-forward to direct the local 4000 port to the service port\n3. Use mysql-client to connect,
How to deploy TiDB cluster on Kubernetes?,TiDB can be deployed on Kubernetes using TiDB Operator. The specific steps are as follows:\n1. Install TiDB CRDs\n2. Install TiDB Operator\n3. Check TiDB Operator components\n4. Deploy TiDB cluster,
How does TiKV ensure that data is not lost or errors are made?,"TiKV chose the Raft algorithm to ensure that no data is lost or errors occur when a single machine fails. Simply put, it means copying data to multiple machines, so that when a certain machine cannot provide services, the copies on other machines can still provide services. This data replication solution is reliable and efficient, and can handle replica failures.",
Which storage model has TiKV chosen? And what key points are provided?,"TiKV chooses the Key-Value model and provides an ordered traversal method. Two key points of TiKV data storage: 1. This is a huge Map (can be compared to C++'s std::map), which stores Key-Value Pairs. 2. The Key-Value pairs in this Map are ordered according to the binary order of the Key, that is, you can Seek to the location of a certain Key, and then continuously call the Next method to obtain Key-Values larger than this Key in increasing order.",
What is the role of PD (Placement Driver) Server?,"PD (Placement Driver) Server is the meta-information management module of the entire TiDB cluster. It is responsible for storing the real-time data distribution of each TiKV node and the overall topology of the cluster, providing the TiDB Dashboard management and control interface, and assigning transaction IDs to distributed transactions.",
What are the three core modules of TiDB?,"The three core modules of TiDB are: TiDB Server, PD (Placement Driver) Server and storage nodes (including TiKV Server and TiFlash).",
What does the overall architecture diagram of TiDB look like?,"TiDB distributed database splits the overall architecture into multiple modules, and each module communicates with each other to form a complete TiDB system. The corresponding architecture diagram is as follows: ![TiDB Architecture Diagram](https://docs.daocloud.io/daocloud-docs-images/docs/storage/hwameistor/application/img/architecture.png)",
How to verify data persistence?,"You can delete and restart the Mysql or WordPress Pod, and then check whether the previously created blogs or comments still exist.",
How to deploy Mysql on Cephfs?,"Mysql can be deployed using a YAML file, such as `kubectl apply -f mysql2-cephfs.yaml`.",
How to deploy Mysql and WordPress on Kubernetes?,"Mysql and WordPress can be deployed using YAML files, such as `kubectl apply -f mysql.yaml` and `kubectl apply -f wordpress.yaml`.",
How to create RBD pools and placement groups?,RBD pools and placement groups can be created using the command `ceph osd pool create {pool_name} {pg_num}`.,
How to use Rook tool to view Ceph storage cluster status?,"You can use the command `kubectl exec -it rook-ceph-tools-{id} -n rook-ceph -- bash` to enter the Rook tool container, and then use the `ceph -s` command to view the Ceph storage cluster status.",
How to check Ceph storage usage?,You can use the command ceph df to view Ceph storage usage.,
How do I check available services?,You can use the command ceph mgr services to view available services.,
How to check Ceph cluster status?,You can use the command ceph -s to view the Ceph cluster status.,
How to view the Rook tool?,"You can use the command kubectl exec -it rook-ceph-tools-<pod-id> -n rook-ceph -- bash to enter the pod where the Rook tool is located, and then use the ceph command to perform related operations.",
How to deploy dashboard?,You can use the command kubectl apply -f dashboard-external-https.yaml to deploy the dashboard.,
What operations can be performed after Rook-ceph is deployed?,"You can perform operations such as viewing pods, storage resource allocation, and data paths.",
What environment needs to be prepared for Rook-ceph test verification?,"This test uses 4 virtual machine nodes to deploy a Kubernetes cluster, in which the kubelet version is 1.23.6.",
How to deploy Rook-ceph?,"Rook-ceph can be cloned from the GitHub source code, and then deployed by deploying Operator, CRD and parameters, as well as cluster and toolbox.",
What is the architecture of Rook-ceph?,"The Rook-ceph architecture includes components such as Ceph storage pools, placement groups (PGs), objects, and OSDs. It also supports features such as CSI storage resource provisioning and distributed architecture.",
What is Ceph?,"Ceph is a distributed, scalable, open source storage solution for block, object, and shared file system storage.",
How do Rook and Kubernetes work together?,"When used with Kubernetes, Rook uses the capabilities provided by the Kubernetes scheduling and orchestration platform to provide a seamless experience for scheduling, lifecycle management, resource management, security, monitoring, and user experience.",
"How does Rook implement self-managing, self-scaling, and self-healing storage services?","This is achieved by automating storage provider deployment, bootstrapping, configuration, provisioning, scaling, upgrades, migrations, disaster recovery, monitoring and resource management.",
What storage providers does Rook support?,Rook supports multiple storage providers.,
What is Rook?,"Rook is an open source cloud-native storage orchestrator that provides a platform, framework and support for various storage solutions and native integration with cloud-native environments.",
What if only the development environment needs to deploy the scheduler?,"You can quickly deploy through YAML files. For specific commands, please refer to the YAML deployment chapter in the document.",
How to deploy scheduler through Helm Chart?,"It is recommended to install through Helm Chart. For specific commands, please refer to the Helm deployment chapter in the document.",
How to deploy scheduler?,"The scheduler should be deployed in HA mode and is recommended for installation via Helm Chart, but can also be deployed via a YAML file. For details, please refer to the installation process and commands in the documentation.",
What storage volumes does the scheduler support?,The scheduler can handle LVM and Disk storage volumes.,
What is a scheduler?,"The scheduler is one of the important components of HwameiStor. It automatically schedules Pods to the correct nodes equipped with HwameiStor storage volumes, so that Pods do not need to use the NodeAffinity or NodeSelector fields to select nodes.",
Where to find LocalStorage CR to modify the storage network card address?,"Select `Custom Resources` in the left navigation bar, find `localdisknodes.hwameistor.io` and modify it.",
What prerequisite configuration is recommended before installing the HwameiStor system?,The configuration of the storage network card is a pre-configuration of the storage system. It is recommended to configure it in advance before installing the HwameiStor system.,
"If I want to modify the storage network cards of multiple nodes, can I configure them in batches?","Currently, batch configuration is not possible and needs to be configured one by one.",
How to mark the storage network card address on the node through comments?,Use the command "kubectl annotate node <your_storage_node> localstorage.hwameistor.io/storage-ipv4=172.30.46.12" to mark the storage network card address on the node as an annotation.,
Which method does HwameiStor support to avoid traffic congestion caused by using communication network cards?,Use a separate network card for data volume synchronization.,
What does StorageClass do?,"The function of StorageClass is to mark storage resources and performance, and dynamically supply appropriate PV resources according to the needs of PVC.",
How to implement the different requirements of applications for the characteristics and performance of storage devices?,"Use the resource object StorageClass to mark storage resources and performance, and dynamically supply appropriate PV resources according to the needs of PVC.",
What storage space attributes can PVC apply for?,"PVC can apply for storage space size (Size) and access mode (such as ReadWriteOnce, ReadOnlyMany or ReadWriteMany).",
Which object is the life cycle of PV independent of?,The life cycle of a PV is independent of the Pod using it.,
What are PV and PVC?,"PV (PersistentVolume, persistent volume) is an abstraction of storage resources; PVC (PersistentVolumeClaim, persistent volume claim) is a user's application for storage resources.",
What features are new in Hwameistor Release Notes?,"- The v0.9.0 version adds new functions such as disk owner, Grafana Dashboard, Operator installation method, and UI application Label. \n- The v0.7.1 version adds Hwameistor DashBoard UI, interface management of Hwameistor storage nodes, local disks, migration records, storage pool management and other functions.",
What optimization measures are mentioned in the Hwameistor Release Notes?,"Populate disk owner information during usage phase. \n- Merge the disk's own attributes when the udev event is triggered. \n- Add a label to svc. \n- Separate disk allocation and disk status update processes. \n- Rename the Exportor port to http-metrics. \n- Add the port name in the exporter service. \n-The used capacity of the disk. \n- When no available disk is found, the scoring mechanism is skipped. \n- The DRDB port defaults to 43001. \n- Chinese document optimization. \n- value.yaml file optimization. \n- Roadmap updated. \n- Sets the default failure policy when installation fails.",
In which version is Grafana Dashboard added?,The v0.9.0 version adds Grafana Dashboard.,
List the contents of the latest version of Hwameistor Release Notes.,"Version number: v0.9.3\n- Optimization:\n- Populate disk owner information during usage phase. \n- Merge the disk's own attributes when the udev event is triggered. \n- Add a label to svc. \n- Separate disk allocation and disk status update processes. \n- Rename the Exportor port to http-metrics. \n- Add the port name in the exporter service. \n- Defects: \n- LD bound, but the problem of no capacity in LSN is fixed. \n- Metrics port listening issue fixed. \n- May occur `not found` issue fixed. \n- Fixed the problem of UI tag in Helm.",
What are the functions of Hwameistor Release Notes?,"Hwameistor Release Notes are used to list the feature changes and evolution paths of each version, making it easier for users to understand and grasp the development direction and functional changes of the software.",
How to reserve disk operation?,"Enter the corresponding cluster, select Storage --> Hwameistor; click the node to enter the node details page, find the corresponding disk; click..., select the reserved disk; click Confirm to reserve.",
What are the usage statuses of idle disks?,Idle disks have two usage states: Available and Reserved.,
Which disk will Hwameistor automatically detect and mark as Reserved when it starts?,Hwameistor will automatically detect the system disk during the initial startup phase and mark it as Reserved.,
What is the usage status and reservation status of Local Disk?,"Local Disk has two statuses: Pending and Available, while LD has only three statuses: Pending, Available, and Bound. LD in any status can be Reserved/Unreserved.",
What is the Reserved Disk feature?,The reserved disk function is a function in the Hwameistor storage module that allows the operation and maintenance administrator to reserve some disks not to be used by Hwameistor.,
What is LocalVolume?,LocalVolume is an LVM logical volume created by Hwameistor and allocated to PersistentVolume.,
What is LocalDiskClaim?,"LocalDiskClaim filters and registers available local data disks in the specified node, and binds the successfully registered data disks to the PersistentVolume.",
What is the role of LocalStorageNode in Hwameistor?,"The role of LocalStorageNode in Hwameistor is to automatically create storage pools, which are LVM logical volume groups.",
What is the role of LocalDiskNode in Hwameistor?,The role of LocalDiskNode in Hwameistor is to register nodes.,
What Kubernetes object classes does Hwameistor support?,"Hwameistor supports Kubernetes object classes such as LocalDiskNode, LocalDisk, LocalDiskClaim, LocalStorageNode, LocalVolume and LocalDiskExpand.",
"How to verify data persistence? After deleting and restarting the mysql/wordpress Pod, are blogs and comments still accessible?",One way to verify data durability is to delete and restart the mysql/wordpress Pod. Blogs and comments will remain accessible after the Pod is restarted.,
What is the IP address of LoadBalancer corresponding to wordpress?,The IP of LoadBalancer corresponding to wordpress is 10.109.11.61.,
"What are the names, capacities, access modes, and storage classes of PVCs and PVs?","The PVC names are mysql-pv-claim, mysql-pv-claim2, and wp-pv-claim. The PV names are pvc-b07feec8-adc8-4e22-abd6-177b5db4fdb1, pvc-19515505-cd8f-41d1-ae91-bb42c3eb64f3 and pvc-7647bc80-febc-4299-a62a-8446d2c364c6. Their capacity is 20Gi, and their access mode is RWO. The storage class used by mysql-pv-claim is rook-ceph-block, the storage class used by mysql-pv-claim2 is rook-cephfs, and the storage class used by wp-pv-claim is also rook-ceph-block.",
What storage pools are there in the cluster?,"There are three storage pools in the cluster: replicapool, myfs-metadata and myfs-data0.",
What services are in this Kubernetes cluster? What are their types and port numbers?,"There are three services in the Kubernetes cluster: kubernetes, nginx and wordpress. kubernetes is a ClusterIP type, the port number is 443/TCP; nginx is a NodePort type, the port number is 80:31090/TCP; wordpress is a LoadBalancer type, the port number is 80:32249/TCP.",
Does HwameiStor support changing a single-copy data volume to multiple copies?,"Yes, it can be achieved by changing the data volume type, and it also supports cross-node hot backup.",
What technology is used to achieve data rebalancing?,"Data rebalancing is achieved through data volume migration technology, which moves data online to nodes with more abundant space.",
How to ensure data synchronization of cross-node hot backup?,"By planning the HA dedicated network logical interface dce-storage, storage traffic is synchronized between nodes and data is replicated synchronously across nodes to ensure hot backup of data.",
How many sets of disk failures can Raid 5 tolerate?,Raid 5 can tolerate a set of disk failures.,
What applications with high availability features are LVM local data volumes suitable for?,"Databases, message middleware, key-value storage systems, etc. with high availability features.",
What two types of local data volumes does HwameiStor provide?,LVM and Disk.,
How to check the installation status of DRBD?,Run the commands `lsmod | grep ^drbd` and `cat /proc/drbd` on each worker node.,
How to check unused data disks?,Run the command `kubectl get localdisks`. Unused data disks are displayed as `PHASE: Unclaimed`.,
How to view each node with `LocalDisk`?,Run the command `kubectl get localdisknodes`.,
How to create API through HwameiStor CRD?,Run the command `kubectl api-resources --api-group hwameistor.io`.,
How to check the created `StorageClass`?,Run the command `kubectl get storageclass hwameistor-storage-disk-hdd`.,
What are `local-disk-manager` and `local-storage` respectively?,"They are `DaemonSet`, and there should be a DaemonSet Pod on each Kubernetes node.",
How to check the installed HwameiStor related Pods?,Run the command `kubectl -n hwameistor get pod`.,
"In a production environment, which network is recommended to enable the high availability mode of HwameiStor?","In a production environment, after turning on high availability mode, it is recommended to use a `10 Gigabit TCP/IP` network with redundant protection.",
"In a production environment, what is the minimum size of data disk recommended for each host to use HwameiStor?","In a production environment, it is recommended that each host have at least one free `200GiB` data disk, and it is recommended to use a solid state drive (SSD).",
"In the test environment, what is the minimum size of the data disk required for each host to use HwameiStor?","In the test environment, each host must have at least one free `10GiB` data disk.",
What types of disks does HwameiStor support?,"HwameiStor supports physical hard disks (HDD), solid state drives (SSD) and NVMe flash disks.",
What system conditions need to be met to use HwameiStor?,"The operating system must meet the following conditions:\n- CentOS 7.9, Redhat 8.4, Redhat 7.9 or Galaxy Kirin OS V10 SP2\n- The corresponding kernel version meets the requirements",
What software needs to be installed to use HwameiStor?,"`LVM2` needs to be installed, and in order to enable the high availability function, `kernel-devel` needs to be installed that is consistent with the currently running Kernel version.",
What version of the Kubernetes container platform supports HwameiStor?,HwameiStor is only supported by Kubernetes container platform version 1.18+.,
"In a production environment, why do we need to avoid deploying Hwameistor to the Master node?","In a production environment, it is necessary to avoid deploying Hwameistor to the Master node because the Master node usually takes on the main tasks of the control plane, while Hwameistor, as a storage module, takes on the important tasks of the data plane. If both tasks run on the same server at the same time, nodes, it may cause resource contention and performance issues. Therefore, it is recommended to deploy Hwameistor on the Worker node.",
" If Hwameistor is installed through the UI interface, how to manually configure the resources of the corresponding module?","If you install Hwameistor through the UI interface, you can manually configure the resource values of the corresponding modules in the `values.extra.prod.yaml` file through YAML. For specific operations, please refer to the examples in the official documentation.",
 How to create Hwameistor through Helm and install it using `values.extra.prod.yaml`?,Create Hwameistor and install it using `values.extra.prod.yaml` with the following command: \n```console\nhelm install hwameistor ./hwameistor \\nn hwameistor --create-namespace \\nf ./hwameistor/values. yaml \\nf ./hwameistor/values.extra.prod.yaml\n```,
 What module resource configurations are included in the `values.extra.prod.yaml` file?,"The `values.extra.prod.yaml` file includes resource configurations for scheduler, Admission, Evictor, Metrics, API Server, Registrar, Manager and other modules.",
" In a production environment, what resource configurations need to be paid attention to when deploying Hwameistor?","In a production environment, when deploying Hwameistor, you need to pay attention to resource configuration and avoid deploying it to the Master node. Some recommended values are provided in the `values.extra.prod.yaml` file, including resource configurations for Scheduler, Admission, Evictor, Metrics, API Server, Registrar, Manager and other modules. If created through Helm, it can be installed by specifying the `values.extra.prod.yaml` file. If installed through the UI interface, please manually configure the corresponding resource values through YAML.",
What is the name of this enclosure?,The name of the enclosure is DCE 5.0 enclosure.,
How many processes are used in this test for performance testing?,Only one process was used in this test for performance testing.,
What is the maximum IOPS in this test?,"The maximum IOPS for read operations in this test was 128, and the maximum IOPS for write operations was 69.",
"In this test, which I/O engine is used?",The libaio I/O engine is used in this test.,
What indicators of storage performance does this test test?,"This test includes storage performance indicators such as write bandwidth, read bandwidth, read latency, and write latency.",
How to check the path of pv?,You can use the command kubectl get -o yaml pv |grep "path:" or kubectl get -o yaml pv [pv-name] |grep "path:" to view the path of the pv.,
What storage mount points are used for performance testing?,The storage mount point used for performance testing is /data.,
What storage modules are involved in this log?,The storage modules involved in this log are OpenEBS Local PV and HwameiStor.,
Which tool is used for performance testing?,Performance testing uses the dbench tool.,
What does this log output?,"The log outputs the results of the performance test on the storage module, including read and write IOPS and bandwidth.",
How to install dbench performance testing tool?,You can create a fio-deploy.yaml file and install it using the kubectl create -f fio-deploy.yaml command.,
How to create a workload and verify it?,"You can verify by creating a Pod and mounting the previously created PVC. For example, you can create a Pod named hello-local-hostpath-pod and execute some commands in it to verify.",
How to create local-hostpath-pvc resource?,You can create a local-hostpath-pvc.yaml file and use the kubectl apply -f local-hostpath-pvc.yaml command to create it.,
How to view installed OpenEBS cluster resources?,You can use the kubectl get po -n openebs command to view the installed OpenEBS cluster resources.,
What is the command used to install OpenEBS with Helm?,helm install openebs --namespace openebs openebs/openebs --create-namespace.,
How many nodes does the test environment used in this article include?,This article uses three virtual machine nodes to deploy a Kubernetes cluster: 1 Master + 2 Worker nodes.,
This article explains how to deploy and verify which cloud-native storage system?,This article describes how to deploy and verify the OpenEBS cloud-native storage system.,
How does OpenEBS perform management functions?,"All management functions on OpenEBS can be performed through kubectl, and an alpha kubectl plugin has been released to help provide information about pools and volumes with a single command.",
What are the common operations of OpenEBS plug-ins?,"Plug-ins for OpenEBS can be used for application-consistent backup and recovery, monitoring and alerting, security policy enforcement, logging, visualization, and more.",
What are the primary responsibilities of a storage driver?,"The main responsibilities of the storage driver include exposing the functionality of the data engine, interacting with the data engine or data engine operators to perform volume creation and deletion operations, interfacing with standard Linux utilities to format, mount/unmount volumes to containers, etc.",
What three layers does a CSI driver consist of?,"The CSI driver consists of three layers: Kubernetes or Orchestrator function, Kubernetes CSI layer and storage driver.",
What is a data engine operator?,The Data Engine Operator is a control plane that manages the volumes and data services provided by the corresponding data engine. It can embed local volume operations into the CSI controller/configurator.,
What role does the CSI driver play in OpenEBS?,The CSI driver acts as a facilitator in OpenEBS to manage the volume lifecycle in Kubernetes and works closely with the storage driver to receive requests and process them.,
What role does the Data Engine Operator play in OpenEBS?,The Data Engine Operator plays the role in OpenEBS of managing all operations from underlying storage to creating pools and volumes.,
What states can an OpenEBS volume copy go through?,"OpenEBS volume replicas can go through states such as initializing, healthy, offline, rebuilding, and terminated.",
How does OpenEBS expose volumes for application reading and writing?,"OpenEBS exposes volumes as devices or directory paths in the case of local PVs, iSCSI targets in the case of cStor and Jiva, NVMe Targets in the case of Mayastor, etc.",
What microservices does the control plane of OpenEBS consist of?,"The OpenEBS control plane consists of a set of microservices, including configuration managers, API servers, monitors, etc.",
How does HwameiStor meet the storage system requirements of the cloud-native era?,"HwameiStor has the advantages of high performance, high availability, automation, low cost, and rapid deployment, and can meet the requirements of the cloud native era as a storage system.",
In which space does MinIO run as a single process?,MinIO runs in user space as a single process and uses lightweight coroutines to achieve high concurrency.,
How does MinIO store data and metadata?,MinIO writes data and metadata together as objects eliminating the need for a metadata database.,
What is the cluster architecture of MinIO?,"The cluster has a fully symmetric architecture, meaning that all servers function identically and there are no name nodes or metadata servers.",
How does MinIO run on a standard server with local drives?,MinIO can run on standard servers with local drives (JBOD/JBOF).,
What is HwameiStor?,"HwameiStor is a storage system that has the advantages of high performance, high availability, automation, low cost, and rapid deployment, and can replace expensive traditional SAN storage.",
What performance metrics does MinIO need to meet when used as primary storage for cloud-native applications compared to traditional object storage?,"Cloud-native applications require higher throughput and lower latency, and these are performance metrics that MinIO can achieve, with read/write speeds of up to 183 GB/sec and 171 GB/sec.",
How does MinIO compare to traditional object storage scenarios?,"MinIO excels in traditional object storage scenarios (such as secondary storage, disaster recovery and archiving), and is also unique in storage technology for machine learning, big data, private cloud, hybrid cloud, etc., including data analysis, high-performance application workloads , native cloud applications, etc.",
What are the requirements for MinIO's architectural design?,"MinIO's architectural design has been aimed at private cloud standards with high performance requirements from the beginning, pursuing ultimate performance on the basis of realizing all the functions required for object storage.",
What is MinIO?,"MinIO is a high-performance, distributed, S3-compatible multi-cloud object storage system suite.",
How to check the status of new storage nodes in the Kubernetes cluster?,"You can use the command `kubectl get node` to check the status of new storage nodes in the Kubernetes cluster. For details, see the ""Step 1"" section in the ""Node Expansion"" content above.",
How to check whether the Pods in the cluster are running normally on the newly added storage node?,"You can use the command `kubectl -n hwameistor get pod -o wide | grep [new storage node name]` to check whether the Pods in the cluster are running normally on the new storage node. For details, see the ""Step 1"" section in the ""Node Expansion"" content above.",
How to build a storage pool for new storage nodes in HwameiStor?,"You can create a resource LocalStorageClaim for adding storage nodes to build a storage pool for the new storage nodes. For specific steps, see the second step in the above ""Node Expansion"" content.",
How to check whether the newly added storage node has been successfully added to the HwameiStor system?,"After completing the steps of adding a new storage node, you can use the command `kubectl get localstoragenode` to check the status of the new storage node and its storage pool to ensure the normal operation of the node and HwameiStor system. For details, see the ""Follow-up Checks"" section in the ""Node Expansion"" content above.",
How to add a new storage node in HwameiStor?,"You can build a storage pool for the new storage node by adding a new node to the Kubernetes cluster, or selecting an existing cluster node (non-HwameiStor node) and creating a LocalDiskClaim resource. For specific steps, see the ""Node Expansion"" content above.",
 What are the random read and write IOPS in Dbench Summary?,The random read and write IOPS in Dbench Summary are 294 and 93 respectively.,
"For read and write operations via dm-0, what are its statistics?","Read and write operations are performed through dm-0. Its statistics are ios=2926/2535, merge=0/0, ticks=676178/621959, in_queue=1332944, util=99.60%, aggrios=2926/2630, aggrmerge=0 /21, aggrticks=676172/1200603, aggrin_queue=1875954 and aggrutil=100.00%.",
How many CPUs were used in the test?,One cpu was used in the test.,
 What are the random write IOPS and bandwidth in this test?,"The random write IOPS is 93, and the bandwidth is 377KiB/s (386kB/s).",
 What are the random read IOPS and bandwidth in this test?,"The random read IOPS is 294, and the bandwidth is 1185KiB/s (1214kB/s).",
How many tasks does each process handle?,Each process handles 1 task.,
Which ioengine was used for this test?,This test uses libaio as ioengine.,
What is the mixed mode of reading and writing for this test?,The mixed read-write mode for this test is randrw.,
Which version of fio was used for testing?,The test used fio-3.13 version.,
What test result is this text?,"This text is the result of a random read and write test (including read, write and mixed read and write) of the DCE 5.0 storage module.",
What is being tested in general in this log file?,"In general, this log file tests various aspects of performance for DCE 5.0 storage module, including sequential write speed and read/write mixed performance.",
What is the average read speed in the second section of the log?,The average read speed in the second section of the log is 1185 KiB/s.,
What type of test is being conducted in the second section of the log?,The second section of the log is testing read/write mixed performance.,
What is the average write speed in the first section of the log?,The average write speed in the first section of the log is 16001.91 KiB/s.,
What type of test is being conducted in the first section of the log?,The first section of the log is testing the sequential write speed.,
What is being tested in the provided log?,The log is testing the performance of the DCE 5.0 storage module.,
How does this storage module perform?,It can be analyzed and compared based on the data in the text and cannot be answered simply.,
What test metrics indicate latency?,lat (usec/msec) and slat/clat/lat (usec/msec).,
What testing methods are available?,"Test methods include: random read and write test, random write test, sequential read and write test, sequential write test, etc.",
What test indicators are there?,"Test indicators include: latency, CPU usage, IO depth, submission and completion ratio, throughput, etc.",
What does this text describe?,This text describes the performance test results of a storage module.,
What application scenarios are high-availability LVM local data volumes suitable for?,"Highly available LVM local data volumes are suitable for applications that require high data availability, such as databases.",
What application scenarios are non-highly available LVM local data volumes suitable for?,"Non-highly available LVM local data volumes are suitable for applications with high availability features such as databases, message middleware, and key-value storage systems.",
What local disk types are supported for local storage?,"Currently supported local disk types include HDD, SSD, and NVMe.",
What is local storage?,"Local storage is a module of HwameiStor, designed to provide high-performance local persistent LVM storage volumes for applications. The supported local persistent data volume type is LVM.",
What are LVM snapshots? what's the effect?,"The LVM snapshot mechanism provides the function of taking snapshots of LVs to obtain a state-consistent backup of the file system. LVM uses Copy-On-Write (COW) technology, which enables backup without stopping the service or setting the logical volume as read-only. Using the LVM snapshot function, you can obtain consistent backup without affecting server availability.",
How to expand VG?,"If the space in the VG volume group is not enough, you need to add a new disk, use pvcreate to add a physical volume, and finally use the vgextend command to add the new physical volume to the VG volume group.",
How to expand LV?,"First, you need to determine whether there is available expansion space. If the VG has no capacity, you need to expand the VG first. Then use the lvextend command to expand the LV.",
What are the basic steps to use LVM?,"The first step is to format the physical disk into a PV. The second step is to add different PVs to the same VG. The third step is to create an LV logical volume in the VG. The fourth step is to directly format and mount the LV for use. In the fifth step, the capacity can be expanded or reduced by increasing or decreasing the number of PEs that make up the LV without losing the original data.",
What are the advantages of LVM?,"Use volume groups to make multiple hard disk spaces look like one large hard disk; use logical volumes to span partitions across multiple hard disk spaces; use logical volumes to dynamically adjust its size when there is insufficient space; when adjusting the size of a logical volume, There is no need to consider the location of the logical volume on the hard disk, and there is no need to worry about no available contiguous space; operations such as creating, deleting, and resizing LVs and VGs can be performed online, and the file system on LVM also needs to be resized; snapshots are allowed to be created, Can be used to save backups of file systems.",
What are the main components of LVM?,"LVM mainly consists of physical storage media, physical volumes, volume groups, logical volumes, physical ranges and logical ranges.",
What is LVM? what's the effect?,"LVM stands for Logical Volume Manager. It can add a logical layer between the disk partition and the file system, provide an abstract disk volume for the file system to shield the underlying disk partition layout, and build a file system on the disk volume. . LVM can be used to dynamically adjust the size of the file system without repartitioning the disk, and the file system managed by LVM can span the disk.",
How do I check the migration status?,"You can enter the details to view the migration status, or execute the `kubectl get LocalVolumeMigrate -o yaml` command to view it.",
How to deploy multi-volume Pod?,Deployment can be done through interface operations or by executing the corresponding kubectl command.,
How to create multiple PVCs?,It can be created through interface operation or executing the corresponding kubectl command.,
How to create convertible StorageClass?,It can be created through interface operation or executing the corresponding kubectl command.,
What do I need to do before migrating?,You need to unmount the PVC before migrating.,
Under what circumstances is the migration function used?,"When the copy of the node where the data volume bound to the application is located is damaged, you need to use the migration function to migrate the volume copy to other nodes.",
What is LocalVolumeGroup(LVG)?,LocalVolumeGroup (LVG) is a function used in HwameiStor to manage multiple data volumes with the same attributes.,
How to delete a bucket?,"In the MinIO console, select the target bucket, click the ""Delete"" button in the upper right corner, and follow the prompts to confirm the deletion.",
How to configure the read-only user policy?,"In the MinIO console, select the ""User"" tab in the left menu, click the ""Policy"" tab of the target user, click the ""Edit"" button in the upper right corner of the page, and enter the read-only policy related configuration.",
How to create a read-only user? What permissions does this user need?,"In the MinIO console, select the ""User"" tab in the left menu, click the ""New User"" button in the upper right corner of the page, and enter the relevant information. This read-only user needs to have list and read permissions.",
How to upload a folder?,"In the MinIO console, select the folder to be uploaded, click the ""Upload"" button on the upper right, select the target bucket and path, and confirm the upload to complete.",
How to create a new bucket?,"In the MinIO console, click the ""Create Bucket"" button in the left menu, enter the new bucket name and click Confirm to create it successfully.",
How to log in to the minio-t1 tenant? What is the username and password?,"When logging in, you need to enter the username minio and password minio123.",
How many tenants does MinIO have?,"MinIO has two tenants, minio-t1 and minio-t2.",
What is the purpose of this test?,"Basic capability tests, system security tests and operation and maintenance management tests were conducted to confirm that HwameiStor can perfectly adapt to the MinIO storage solution.",
On which platform was MinIO distributed object storage deployed and connected to HwameiStor local storage in this test?,On the Kubernetes 1.22 platform.,
"After completing the configuration, how to conduct basic functional testing?","The steps are as follows: Log in to `minio console: 10.6.163.52:30401/login` from the browser; obtain JWT through `kubectl minio proxy -n minio-operator`; browse and manage the created tenant information; log in to the minio-t1 tenant (username minio, password minio123); browse bucket bk-1; create new bucket bk-1-1; create path path-1-2; upload file successfully; upload folder successfully; create read-only user.",
How to configure HwameiStor local volume?,The steps are as follows: Run the following commands in sequence to configure the local volume: kubectl get statefulset.apps/minio-t1-pool-0 -nminio-tenant -oyaml; kubectl get pvc –A; kubectl get pvc export-minio6-0 -nminio-6 -oyaml; kubectl get pv; kubectl get pvc data0-minio-t1-pool-0-0 -nminio-tenant -oyaml; kubectl get lv; kubect get lvr.,
How to create a tenant?,"The steps are as follows: Enter the `/root/operator/examples/kustomization/base` directory and modify tenant.yaml; enter the `/root/operator/helm/tenant/` directory and modify the `values.yaml` file; enter `/root/ operator/examples/kustomization/tenant-lite` directory, modify the `kustomization.yaml` file; create a tenant: kubectl apply –k .",
How to deploy minio operator?,The steps are as follows: copy the minio operator warehouse to the local; enter the helm operator directory; deploy the minio-operator instance; check the running status of the minio-operator resource.,
How many disks are used for HwameiStor local disk management?,"HwameiStor uses five disks for local disk management, namely SDB, SDC, SDD, SDE, and SDF.",
 How to define storage class?,"Cluster administrators can define multiple StorageClass objects as needed. Each object specifies a plug-in for provisioning volumes (also called an allocator), and configures a set of parameters passed to the allocator when provisioning resources. Cluster administrators can define and expose multiple storage types (from the same or different storage systems) within the cluster, each with customizable parameters.",
 What is a PersistentVolume (PV)?,"PVs are dynamically provisioned by the storage provider when users request PVCs. PV contains details of how the container consumes storage. Kubernetes and volume drivers use the details in the PV to attach/unmount storage to the node on which the container is running, and to mount/unmount storage to the container.",
 What is PersistentVolumeClaim (PVC)?,A PVC is a user storage request served by a StorageClass provided by the cluster administrator. Applications running on the container can request a certain type of storage. Administrators can also create storage classes to provide custom properties for PVs.,
 What is StorageClass?,"StorageClass is a way of describing ""classes"" of storage, and different classes may map to quality of service levels, backup policies, or arbitrary policies. Under the dynamic resource allocation function, cluster administrators do not need to allocate storage resources in advance.",
 What is Container Storage Interface (CSI)?,"CSI is a standard for exposing arbitrary file storage and block storage systems to containerized workloads on Kubernetes, enabling third-party storage providers to write and deploy new storage volume plug-ins, such as HwameiStor LocalDisk and LocalVolumeReplica, without modifying Kubernetes core code.",
How to scale a MySQL application into a three-node cluster?,"Use the `kubectl scale` command to expand `StatefulSet`, for example `kubectl scale sts/sts-mysql-local --replicas=3`, and then use the command `kubectl get po -l app=sts-mysql-local -o wide` to view The status of the Pod. You can use the command `kubectl get pvc -l app=sts-mysql-local -o wide` to check the status of the PVC and ensure that each Pod is bound to an independent local volume. Finally, use the command `kubectl get lv` to check the status of the local volume.",
How to view the `LocalVolume` object?,You can use the command `kubectl get lv pvc-accf1ddd-6f47-4275-b520-dc317c90f80b` to view the `LocalVolume(LV)` object with the same name.,
How to view MySQL containers and `PVC/PV`?,You can use the command `kubectl get po -l app=sts-mysql-local -o wide` to view MyS,
How to use HwameiStor to run stateful applications?,It is necessary to meet the prerequisites that HwameiStor has been installed successfully and the storage pool has been created. You can use a MyS,
What are Longhorn’s advanced features?,"Supports Backing Image, supports Orphaned Replica Directories, supports DCE cluster recovery, supports multiple write operations ReadWriteMany (RWX) workloads (NFSv4), and supports Longhorn Volume as an iSCSI Target.",
What are the monitoring features of Longhorn?,"Supports Prometheus and Grafana monitoring Longhorn, supports Kubelete Metrics monitoring, and supports Longhorn alarm policy.",
What support is available for high availability in Longhorn?,"Supports Replica automatic balancing settings and data locality setting: there is at least one replica copy on the node running the pod using the storage volume, and supports automatic recovery of storage volumes after cluster node failure.",
What storage volume features does Longhorn support?,"Supports thin provisioning of storage volumes, supports storage volume maintenance mode maintenance mode for snapshot reverting operation, each storage volume replica contains multiple snapshots, supports continuous repeated snapshots and backups, and supports Clone of CSI storage volumes.",
What are control plane and data plane?,Control plane: Longhorn Manager is deployed with DaemonSet; data plane: Longhorn Engine is a storage controller and can have multiple replicas.,
What is Longhorn? What infrastructure can it run on?,"Longhorn is a lightweight, cloud-native Kubernetes distributed storage platform that can run on any infrastructure.",
How does HwameiStor handle the scheduling of multi-copy application workloads? How is it different from traditional general-purpose shared storage?,"HwameiStor recommends using a stateful StatefulSet for multi-copy workloads and creating a corresponding PV data volume for each Pod copy. For traditional general-purpose shared storage, the stateful application statefulSet will preferentially deploy the replicated copies to other nodes to distribute the workload, and create a corresponding PV data volume for each Pod copy; the stateless application deployment will prioritize the replicated copies. to other nodes to distribute the workload, and all Pods share a PV data volume (currently only NFS is supported).",
In what form is HwameiStor's scheduler deployed on the Kubernetes platform?,HwameiStor's scheduler is deployed in the HwameiStor namespace in the form of a Pod.,
What types of data volumes does Hwameistor local storage support?,Hwameistor local storage supports LVM type and raw disk type data volumes.,
What are the production operation and maintenance functions of the DCE 5.0 storage module?,"Production operation and maintenance functions include non-disruptive upgrade, disk replacement, one-click/automatic expulsion of node data volumes, single disk/application load dimension data migration, unified dashboard and rich Metrics indicators, etc.",
What are the characteristics of Hwameistor local storage?,"Hwameistor local storage has the characteristics of high-performance local volumes, multi-type data volumes, CSI standards, active and backup high availability, and data volume expansion.",
What functions can storage docking achieve?,"Storage docking can realize functions such as hybrid storage access, dynamic storage management, and multiple data volume creation methods.",
What modules does the DCE 5.0 storage module include?,The DCE 5.0 storage module includes storage docking and Hwameistor modules.,
How do I view LocalVolume and LocalVolumeReplica objects?,Use the commands `kubectl get lv pvc-5236ee6f-8212-4628-9876-1b620a4c4c36` and `kubectl get lvr` to view the LocalVolume and LocalVolumeReplica objects.,
How to view MySQL Pods and PVC/PV?,Use the commands `kubectl get po -l app=sts-mysql-ha -o wide` and `kubectl get pvc -l app=sts-mysql-ha` to view MyS,
How to create a StatefulSet?,Use the command `kubectl apply -f exapmles/sts-mysql_ha.yaml` to create MyS,
What prerequisites need to be completed before using highly available volumes?,"You need to complete the installation of HwameiStor, create a high-availability storage pool, and deploy the DRBD kernel component.",
What are highly available volumes?,Highly available volumes are volumes created using the open source DRBD data synchronization technology and can provide high availability services.,
What information does the ceph -s command output include?,"The information output by the ceph -s command includes Ceph cluster ID, health status, service (MON/MGR/MDS/OSD) status, data storage status (including volumes and pools), data usage, and PG status.",
Which command is used to enter the Pod where rook-ceph-tools is located?,Enter the Pod where rook-ceph-tools is located through the kubectl exec command.,
Which Pods are listed in this text that are running rook-ceph components?,"This text lists Pods for components such as rook-ceph-mgr, rook-ceph-mon, rook-ceph-operator, rook-ceph-osd, and rook-ceph-rgw-ceph-objectstore.",
What Kubernetes resource objects are used in the deployment of rook-ceph?,"The deployment of rook-ceph uses Kubernetes resource objects such as Deployment, StatefulSet, Service, ConfigMap, Secret, and Job.",
What is this text about?,This text is the output of installing rook-ceph using Helm deployment.,
How to verify whether the Ceph tools are installed successfully?,Run the following command on the command line to verify whether the Ceph tools are installed successfully:\n```console\nkubectl exec -it rook-ceph-tools-7c8ddb978b-2mf52 -n rook-ceph -- bash\nceph -s\n ```,
How to install rook-ceph cluster and ceph tool using Helm?,Run the following command on the command line to install rook-ceph cluster and ceph tool:\n```console\nhelm install --namespace rook-ceph rook-ceph-cluster rook-release/rook-ceph-cluster --set operatorNamespace=rook-ceph --set cephClusterSpec.storage.deviceFilter="^sd." --set cephClusterSpec.cephVersion.image=quay.io/ceph/ceph:v17.2.3\nkubectl create -f toolbox.yaml\n` ``,
How to install rook operator using Helm?,Run the following command on the command line to install the rook operator:\n```console\nhelm install --namespace rook-ceph rook-ceph rook-release/rook-ceph --create-namespace --set image.repository= rook/ceph --set csi.cephcsi.image=quay.io/cephcsi/cephcsi:v3.7.2 --set csi.registrar.image=registry.k8s.io/sig-storage/csi-node-driver-registrar: v2.5.1 --set csi.provisioner.image=registry.k8s.io/sig-storage/csi-provisioner:v3.3.0 --set csi.snapshotter.image=registry.k8s.io/sig-storage/csi- snapshotter:v6.1.0 --set csi.attacher.image=registry.k8s.io/sig-storage/csi-attacher:v4.0.0 --set csi.resizer.image=registry.k8s.io/sig-storage/ csi-resizer:v1.6.0\n```,
How to add rook repo?,Run the following command in the command line to add rook repo:\n```\nhelm repo add rook-release https://charts.rook.io/release\n```,
How to install Helm in this article?,Run the following command on the command line to install Helm:\n```\n[root@k8s-10-6-162-31 ~]# wget https://get.helm.sh/helm-v3.10.1 -linux-amd64.tar.gz\n[root@k8s-10-6-162-31 ~]# tar xvfz helm-v3.10.1-linux-amd64.tar.gz\n[root@k8s-10-6 -162-31 ~]# cd linux-amd64\n[root@k8s-10-6-162-31 linux-amd64]# mv helm /usr/bin\n```,
This article explains how to use Helm to deploy what cloud-native storage system?,This article describes how to use Helm to deploy the Rook-Ceph cloud native storage system.,
"After the installation is completed, how to check the installation status of Hwameistor?",You can click the Helm application to view the Hwameistor installation status.,
What parameters need to be filled in in the fourth step of the installation steps?,"Global settings (global image Registry, K8s image Registry, Kubelet Root Dir) and configuration settings (DRDBStartPort, Storage Class configuration (AllowVolumeExpansion, DiskType, Enable HA, Replicas, ReclaimPolicy)).",
What is the third step in the installation process?,Select the version you want to install in the version selection and click Install.,
What is the first step in the installation process?,"Click Container Management -> Cluster List in the left navigation bar, and then find the name of the cluster where you plan to install Hwameistor.",
What are the prerequisites for installing Hwameistor?,"Free HDD and SSD disks have been prepared on the node to be used; the preparation work has been completed; if you need to use a high-availability data volume, please complete the DRDB installation in advance; if the deployment environment is a production environment, please read the production environment resource requirements in advance; if you Kubernetes distributions use different kubelet directories, please confirm the kubeletRootDir in advance.",
This article explains how to install Hwameistor through what platform interface?,Install via the platform interface.,
How to check the status of newly added disks and their storage pools?,You can check the status of LocalStorageNode. The specific command is as follows:\n```yaml\nkubectl describe localstoragenode k8s-worker-4\n```,
How to add a new disk to the node's storage pool?,You can add the new disk to the node's storage pool by creating the resource LocalDiskClaim. The specific command is as follows:\n```console\n$ kubectl apply -f - <<EOF\napiVersion: hwameistor.io/v1alpha1\nkind: LocalDiskClaim\ nmetadata:\nname: k8s-worker-4-expand\nspec:\nnodeName: k8s-worker-4\ndescription:\ndiskType: SSD\nEOF\n````,
How to check whether HwameiStor has correctly created the resource LocalDisk for the newly added disk and the status is Unclaimed?,You can check it with the following command: \nkubectl get localdisk | grep k8s-worker-4 | grep sdc,
How to check that the newly added disk is correctly recognized?,You can check whether the newly added disk is successfully inserted into the node and correctly recognized by following the following steps:\n- ssh connect to the node\n- run the command lsblk | grep sdc,
How to add disk expansion capacity to a node in the HwameiStor storage system?,"Adding a disk (data disk) to a node can be completed by following the following steps:\n- Prepare a new storage disk\n- Insert the new disk into the disk slot of the node\n- Check the status of the new disk\n- By creating a resource LocalDiskClaim, add the new disk to the node's storage pool",
Who should I contact if I need to obtain commercial DRBD products?,"If you need to obtain commercial DRBD products, you should contact Linbit. Their contact information is https://linbit.com/contact-us/.",
What does the DRBD installer do?,"The DRBD installer can directly install DRBD into the container cluster. Currently, this module is only used for testing purposes.",
What can DRBD do?,DRBD can mirror the entire device through the network to achieve the purpose of high-availability clustering. Can be regarded as a kind of network RAID.,
What is DRBD?,DRBD is the abbreviation of Distributed Replicated Block Device. It is composed of Linux kernel modules and related scripts to build high-availability clusters.,
How to enable the failed deletion option of DRBD?,Just turn on the failed deletion option on the configuration page.,
What options can be set with DRBD?,DRBD can set the following options:\n- Namespace: It is recommended to be deployed in the same namespace as Hwameistor. \n- Version: The latest version is selected by default. \n- Failed deletion: off by default. \n- Ready and waiting: off by default. \n- Verbose logging: off by default.,
What is the default namespace of DRBD?,"The default namespace of DRBD is recommended to be deployed in the same namespace as Hwameistor, which is `Hwameistor` in the example.",
How to install DRBD via Helm?,You can install it using the following methods:\n```console\nhelm repo add hwameistor https://hwameistor.io/hwameistor\nhelm repo update hwameistor\nhelm pull hwameistor/drbd-adapter --untar\nhelm install drbd-adapter. /drbd-adapter -n hwameistor --create-namespace\n```\nOr accelerate through the mirror warehouse `daocloud.io/daocloud`:\n``console\nhelm install drbd-adapter ./drbd-adapter \\nn hwameistor --create-namespace \\n--set imagePullPolicy=Always \\n--set registry=daocloud.io/daocloud\n```,
How to install DRBD through UI interface?,"Enter `Container Management`-->`Helm Application`, select `drbd-adapter`, click to install and configure accordingly.",
How to observe the status of PVC and PV after expansion is completed?,You can use the kubectl get pvc and kubectl get pv commands to check the status of PVC and PV.,
"The more capacity is added during the expansion process, the longer it will take?","Yes, the more capacity you add, the longer it will take to expand.",
How to observe the process of PVC expansion?,You can use the kubectl describe pvc command to view the PVC event log.,
How to modify the size of PVC?,You can use the kubectl get edit pvc command to edit resource files and modify the value of spec.resources.requests.storage.,
Which parameter should be checked to see whether it is true before expansion?,"Before expanding the capacity, you need to check whether the allowVolumeExpansion parameter in StorageClass is true.",
Which volumes does HwameiStor support?,HwameiStor supports CSI volume expansion.,
How to install Longhorn in DCE 5.0 App Store?,"Select ""App Store"" in DCE 5.0, search for and install Longhorn Addon. You can then access the corresponding port in the browser to enter the Longhorn interface.",
How to upload chart package to DCE 5.0 mirror warehouse?,"In DCE 5.0, select ""Image Warehouse"" and then ""Upload Image"" to upload.",
How to package and compress charts containing json files?,"Enter the directory containing the json file, use the tar command to package and compress it into tgz format.",
How to convert Longhorn helm chart format?,"First, you need to add the repo, then pull and unzip the Longhorn helm chart, and convert values.yaml into json format.",
What is Longhorn?,Longhorn is a distributed block storage system for data volume management for cloud-native storage.,
What application deployments does rook-ceph support?,"rook-ceph supports the deployment of multiple applications, such as MyS",
How to verify whether the deployment of rook-ceph is successful?,"You can use the kubectl command to view the status of related resources, such as pod, service, pv, etc.",
How to use rook-ceph for storage management?,"Resources such as StorageClass, PersistentVolume, and PersistentVolumeClaim can be created for storage management. Please refer to relevant documents for specific steps.",
What are the components of rook-ceph?,"Including Ceph Monitor, Ceph OSD, Ceph MDS, Ceph Rados Gateway and other components.",
How to deploy and install rook-ceph?,It can be deployed and installed through the DCE 5.0 Add-on application store or deployed manually. Please refer to relevant documents for specific steps.,
How to verify application deployment?,"First, you need to execute the kubectl apply -f mysql.yaml command in the corresponding path to deploy the application, and check the application deployment status through the kubectl get command and kubectl describe command.",
How to install Rook-ceph in DCE 5.0 cluster?,"Connect the cluster to DCE 5.0, and then install rook-ceph.",
How to upload chart package in DaoCloud mirror warehouse?,Enter the DaoCloud mirror warehouse and upload the chart package in the mirror warehouse.,
How to convert rook-ceph helm chart format?,"You need to add the repo, pull the rook-ceph helm chart and decompress it, then convert rook-ceph's values.yaml into json format, and finally package and compress the chart containing the json file.",
What does this article offer?,This article provides the steps and instructions for installing and deploying the Rook-ceph cloud native storage system on the graphical interface of the DCE 5.0 application store Addon.,
How to check whether the installation effect is correct?,See the next chapter [Post-installation check](./post-check.md).,
"If you can't access the mirror repository, which mirror source can you try?",You can try to use the mirror sources provided by DaoCloud: m.daocloud.io and ghcr.m.daocloud.io.,
What are the prerequisites for installing HwameiStor?,"- Free HDD and SSD disks have been prepared on the node to be used. \n- Preparatory work completed. \n- If you need to use a highly available data volume, please complete the DRDB installation in advance. \n- If the deployment environment is a production environment, please read the production environment resource requirements in advance. \n- If your Kubernetes distribution uses a different kubelet directory, please confirm the kubeletRootDir in advance.",
Which tool can be used to install any component of HwameiStor?,Helm Chart.,
 How to verify the installation effect of Hwameistor?,"You can verify the installation effect through the command line, please refer to Post-installation Check.",
" How to check the status of Hwameistor components (Local Storage, Local Disk Manager, etc.)?","You can click Workload-->Stateless Workload, select the corresponding namespace, and view the Hwameistor component status.",
What are the installation steps for Hwameistor?,"- Click Container Management -> Cluster List in the left navigation bar, and then find the name of the cluster where you want to install Hwameistor. \n- Select Helm Application -> Helm Template in the left navigation bar, find and click Hwameistor Operator. \n- Select the version you want to install in the version selection and click Install. \n- In the installation interface, fill in the required installation parameters. \n- After confirming that the parameters are correct, click OK to complete the installation. After completing the installation, you can click the Helm application to view the Hwameistor Operator installation status.",
 What are the prerequisites for Hwameistor installation?,"Free HDD and SSD disks have been prepared on the node to be used; the preparation work has been completed; if you need to use a high-availability data volume, please complete the DRDB installation in advance; if the deployment environment is a production environment, please read the production environment resource requirements in advance; if you Kubernetes distributions use different kubelet directories, please confirm the kubeletRootDir in advance.",
Can the Ceph dashboard differentiate between clusters?,"The current panel does not differentiate between clusters, and the cluster identification option will be gradually optimized and added in the future.",
How to collect monitoring information of rook-ceph-cluster?,"To collect monitoring information of rook-ceph-cluster, you need to install Insight Agent in the working cluster first, and then create CR ServiceMonitor to collect monitoring information of rook-ceph-cluster.",
How to implement Ceph Dashboard?,"You need to deploy Rook-ceph first, then deploy rook-ceph-cluster, install Insight Agent in the working cluster and create CR ServiceMonitor, and finally deploy Grafana Dashboard in the Global cluster.",
What functions can CRDs + Controllers achieve?,"In Kubernetes, CRDs + Controllers = Everything, that is, all custom logic can be implemented. Custom Controller can listen to CR's CRUD events to implement custom business logic.",
Why do you need CRD?,"CRD can register a new resource to the Kubernetes cluster to expand the capabilities of the Kubernetes cluster, provide a higher level of abstraction on the custom underlying infrastructure, and customize resource types according to business needs.",
What is CR?,"CR is the abbreviation of Custom Resource, which is a resource description that conforms to the field format definition in CRD.",
What is CRD?,CRD is the abbreviation of Custom Resource Definition. It is a native resource type built into Kubernetes and is used to describe custom resources.,
How is the CSI interface used in real user cases?,"In actual user cases, using the CSI interface requires creating a storage class for the CSI storage provider, deploying the CSI volume driver including the storage provider on the Kubernetes cluster, creating a PVC to dynamically configure the storage class and using the workload of the PVC, etc.",
What functionality is the separate storage provider management logic hard-coded for?,"The individual storage provider management logic is hard-coded into the following functions, which are clearly defined in the CSI specification: CreateVolume, DeleteVolume, ControllerPublishVolume, ControllerUnpublishVolume, ValidateVolumeCapabilities, ListVolumes, GetCapacity, ControllerGetCapabilities, RequiresFSResize, and ControllerResizeVolume.",
Which interface needs to be implemented to extend the "In-Tree" volume plugin?,Expanding "In-Tree" volume plugins requires implementing the `expandablePlugin` interface.,
Which components need to be extended to extend the CSI specification?,"Extending the CSI specification requires components such as the extended CSI specification, ""in-tree"" volume plug-in, external-provisioner and external-attacher.",
What does a high-level Kubernetes prototype of the CSI interface include?,"The high-level Kubernetes prototype includes three new external components to decouple Kubernetes and storage provider logic, blue arrows represent regular methods to make calls to the API server, and red arrows show gRPC to make calls to the Volume Driver.",
What problems does CSI aim to solve?,CSI is designed to solve the challenges of testing storage subsystems and embedding storage vendor code into Kubernetes core repositories.,
What is the CSI interface?,CSI is the abbreviation of Container Storage Interfaces. It is a container storage interface that aims to define industry standards so that storage providers that support CSI can be used in container orchestration systems that support CSI.,
"If created using Helm, how to set the `kubeletRootDir` parameter?",Use the parameter `--set kubeletRootDir=<corresponding directory>` in the Helm command to set it. For example: `--set kubeletRootDir=/var/snap/microk8s/common/var/lib/kubelet/`.,
How to modify the `kubeletRootDir` parameters through interface operations?,"On the interface, open `Global Setting`, then enter the `Kubelet Root Dir` parameter setting page, and set the parameters to the corresponding directory. For details, please refer to [Create through interface](deploy-ui.md).",
What if Kubernetes distributions use different `kubelet` directories?,The parameter kubeletRootDir must be set and set to the corresponding directory.,
What is the default `kubelet` directory?,The default `kubelet` directory is `/var/lib/kubelet`.,
How to deploy Longhorn storage system?,You can search for Longhorn in the DCE 5.0 app store and select the corresponding version for deployment.,
How to enter Longhorn UI?,You can enter the Longhorn UI by entering the IP address and port number of the Longhorn front-end service in your browser. For example: http://<IP>:<PORT>.,
How to modify the Longhorn front-end service port?,The Longhorn front-end service port can be modified by modifying the NodePort property of the Kubernetes Service.,
What features does DCE 5.0’s local storage have?,"The local storage of DCE 5.0 has the characteristics of high scalability and high availability, which is naturally in line with the characteristics of cloud native.",
What standards does DCE 5.0 cloud native storage use for interconnection?,DCE 5.0 cloud native storage is connected based on the Kubernetes CSI standard.,
What are the characteristics of cloud-native software-defined storage?,"Software-defined storage uses the disk space on every machine in the enterprise through the network, and forms these dispersed storage resources into a virtual storage device, and the data is dispersed in different storage devices.",
How does traditional storage cloud-native integration connect to the Kubernetes platform?,Traditional storage cloud-native is connected to the Kubernetes platform through the CSI standard.,
What is cloud-native storage?,"Cloud-native storage is divided into three types, including traditional storage cloud-native, software-defined storage cloud-native and pure cloud-native storage.",
What is a storage preparer?,"Storage preparer refers to the plug-in or API implementation defined in the storage class for creating PV, which can meet different storage requirements.",
Will the admission controller verify the Pods in the namespace where HwameiStor is located?,"Won't. To ensure that HwameiStor's Pods can start smoothly, the admission controller will not verify the Pods in the namespace where HwameiStor is located.",
What can an admission controller do?,"The admission controller can identify HwameiStor data volumes and obtain all PVCs used by Pods. It checks each PVC [storage provisioner](https://kubernetes.io/zh-cn/docs/concepts/storage/storage-classes/#provisioner) if the provisioner's name suffix is `*.hwameistor. io`, indicating that the Pod is using the data volume provided by HwameiStor. The admission controller only verifies Pod resources, and verifies them when they are created.",
What is an admission controller? What does it do?,The admission controller is a webhook that can automatically verify HwameiStor data volumes and assist in changing `schedulerName` to hwameistor-scheduler. Its role is to verify Pod resources and ensure that the data volumes provided by HwameiStor are used.,
How can business applications be planned and designed to reduce costs?,"Business applications are planned and designed according to business peaks, and the idle rate of computing resources and storage resources is ≥ 50% to reduce costs.",
"In terms of cost reduction and efficiency improvement challenges, how will storage performance bottlenecks restrict computing density?",Storage performance bottlenecks will become bottlenecks that restrict computing density.,
"In terms of agility challenges, what are the agility and flexibility requirements for storage in containerized application scenarios?","Containerized application scenarios have high requirements for storage agility and flexibility, and need to support functions such as cloud disk mounting, improved uninstallation efficiency, fast mounting switching, automatic repair capabilities for storage service problems, and online expansion capabilities.",
"In terms of enterprise readiness challenges, what are the new requirements for storage in the cloud?","There are new and higher requirements for cloud storage, including performance (throughput, latency), efficient operation and maintenance, and reliability monitoring to ensure data storage.",
What is the role of edge computing in cloud-edge collaboration scenarios?,Edge computing plays a certain role in data storage and data calculation in cloud-edge collaboration scenarios.,
What problems do enterprises need to face when migrating stateful applications such as middleware to the cloud?,Enterprises need to face the problem of ensuring high performance and high reliability of business applications.,
What is a DCE 5.0 storage module?,"The DCE 5.0 storage module includes storage docking and HwameiStor modules, which are used to solve the storage needs of middleware services on the cloud and storage solutions in cloud-edge collaboration scenarios.",
How does Hwameistor cloud-native local storage ensure performance?,"Hwameistor cloud-native local storage adopts IO localization to achieve high-performance local throughput, stable CPU, no major fluctuations, and low memory resource overhead.",
What operational features does Hwameistor cloud-native local storage have?,"Hwameistor cloud-native local storage supports node, disk, data volume group (VG) dimensional migration, disk replacement and other operation and maintenance behaviors.",
What expansion methods does Hwameistor cloud-native local storage support?,"Hwameistor cloud-native local storage supports independent node units, with a minimum of 1 node and no limit to the number of expansions. The control plane and data plane are separated, and node expansion does not affect business application data I/O.",
What is the data protection mechanism of Hwameistor cloud-native local storage?,Hwameistor cloud-native local storage uses two copies of data volumes for redundant backup to ensure high data availability.,
What are the advantages of Hwameistor cloud-native local storage?,"Hwameistor cloud-native local storage has the advantages of IO localization, high performance and high availability, linear expansion, low CPU and memory overhead, and production operability.",
What is one of the advantages of CAS?,"The granularity of the storage policy. Using CAS architecture, you can configure all storage policies on a per-volume basis and dynamically update storage policies to achieve the desired results for each workload.",
How does CAS avoid being bound by cloud service providers?,"The storage controller can perform data migration in the background for each workload, making live migration easier. CAS's control granularity simplifies the migration of stateful workloads from one Kubernetes cluster to another in a non-disruptive manner.",
What are the advantages of CAS?,"Agility: Resource maintenance and adjustment in CAS are truly agile, and the rolling upgrade function of Kubernetes enables seamless upgrades of storage controllers and storage replicas. \n- Granularity of storage policies: Using CAS architecture, you can configure all storage policies on a per-volume basis and dynamically update storage policies to achieve the desired results for each workload. \n- Avoid binding: CAS's control granularity simplifies the migration of stateful workloads from one Kubernetes cluster to another in a non-disruptive manner. \n- Native cloud: CAS containerizes storage software and uses Kubernetes CRD to represent underlying storage resources, which can be seamlessly integrated into other cloud host tools. \n-Smaller scope of impact: With CAS, the scope of impact is much smaller and is limited to volumes that have replicas on that node.",
What components does each storage volume in CAS have?,Each storage volume in CAS has a containerized storage controller and corresponding containerized replicas.,
What is CAS storage?,CAS storage is a software storage system based on microservice storage controllers orchestrated by Kubernetes. Its storage controllers are managed and run in containers as part of the Kubernetes cluster.,
How to use symbol H to schedule tasks?,"In order for regularly scheduled tasks to produce an even load on the system, the symbol H (standing for ""hash"") should be used whenever possible. For example, use `HH(0-7) * * *` to represent a time between 00:00 and 7:59.",
How to specify multiple values for a field in a scheduled trigger rule?,"The following operators can be used in order of precedence: `*` (matches all values in the range), `MN` (all values in the specified range), `MN/X` or `*/X` (matches all values in the specified range) Or trigger every X within the entire valid range), `A,B,...,Z` (match multiple values).",
What is the rule syntax for timed triggers?,"The syntax of timed trigger rules follows the syntax of CRON. Specifically, each line consists of 5 tab or space separated fields: `MINUTE HOUR DOM MONTH DOW`.",
What types of triggers are there in pipelines?,There are two types of pipeline triggers: code source triggers and timing triggers.,
What are triggers and how to configure them?,"Triggers can automatically trigger the execution of the pipeline periodically. When editing the pipeline, click `Edit Configuration` to configure the build trigger.",
How does the DCE 5.0 application workbench solve the problem of errors during the release process of complex business systems?,"Through the standardization and automation features of continuous integration and continuous delivery, the DCE 5.0 application workbench uses code changes as flowing units and is based on the release pipeline to connect all functions of development, testing, and operation and maintenance, and release software continuously, quickly, and highly reliably. Solve the problem of errors during the release process of complex business systems.",
How to use DCE 5.0 to achieve continuous delivery of applications under cloud native?,Use the open source software Argo CD and submit the YAML file of Kubernetes to the code warehouse to achieve continuous delivery of applications under cloud native. The entire process does not require learning Kubernetes release commands or directly operating the cluster.,
What is the GitOps philosophy?,The GitOps concept is a Git-based operation and maintenance concept that automatically applies these configuration files to the production environment by submitting application configuration files to the Git code base and using CI/CD tools in the code base.,
What are the characteristics of continuous integration and continuous delivery?,"Continuous integration and continuous delivery have the characteristics of standardization and automation. They use code changes as flowing units and are based on the release pipeline to integrate all functions of development, testing, and operation and maintenance, and release software continuously, quickly, and with high reliability.",
What scenarios is the DCE 5.0 application workbench suitable for?,DCE 5.0 application workbench is suitable for continuous delivery pipelines and continuous delivery of applications under cloud native based on pipeline + GitOps.,
What are the parameters for the collection test report task template?,"The parameters of the collection test report task template include: test report (specify the location of the generated xml report file, support filling in multiple addresses, separated by commas).",
What parameters does the SVN task template have?,"The parameters of the SVN task template include: code warehouse and credentials (for private warehouses, you need to create credentials in advance and select the corresponding credentials when using them).",
What parameters does the timeout task template have?,"The parameters of the timeout task template include: time and unit (supports seconds, minutes, hours, and days).",
What types of task templates are there for using credentials?,"There are three types of task templates using credentials: username and password, access token, and kubeconfig. And need to fill in the corresponding parameters.",
What parameters does the audit task template have?,"The parameters of the audit task template include: message (this message will be displayed in the pipeline running status, and you can select people who can be audited by inputting @ + user).",
What are the parameters for saving product task templates?,The parameters of the save product task template include: the file used for archiving (use a regular expression to specify the path where the file is expected to be stored. Please separate multiple files with English commas).,
What can be done with the print message task template?,The print message task template can output some messages to the terminal.,
What can be done using Shell task templates?,You can use the Shell task template to execute shell commands and support multi-line commands.,
What parameters does the Git Clone task template have?,"The parameters of the Git Clone task template include: code warehouse, branch, and credentials (for private warehouses, you need to create credentials in advance and select the corresponding credentials when using them).",
What are the task templates of DCE 5.0 application workbench?,"The task templates of the DCE 5.0 application workbench include: Git Clone, Shell, Print Message, Save Artifacts, Audit, Use Credentials, Timeout, SVN and Collect Test Reports.",
What is DCE 5.0 Application Workbench?,"DCE 5.0 application workbench is a visual development tool based on the DCE 5.0 platform, used to quickly build, deploy and manage applications.",
How to check the port number exposed by a deployed application?,"In the details page corresponding to the deployed application, under the ""Service"" tab, you can view the port number exposed by the service.",
How to manually trigger the continuous integration process?,"Click the corresponding application name on the continuous integration page, and click the ""Build"" button on the page to manually trigger the continuous integration process.",
How to configure automatic build and automatic deployment in continuous release mode?,1. Check automatic build and automatic deployment when creating the application. \n2. Configure trigger conditions and build commands on the continuous release page and save.,
How to view the details of a deployed application?,"In the `Application Workbench` -> `Published` page, click a deployed application name to view the details of the application.",
How to manually synchronize a continuously deployed application?,"1. In the `Application Workbench` -> `Continuous Publishing` page, click the name of an application whose synchronization status is `Unsynchronized`. \n2. On the application details page, click the `Sync` button to enter the synchronization page. \n3. Configure parameters on the synchronization page, select at least one application to be synchronized, click OK, and wait for the synchronization to be successful to view the synchronization results.",
"How can I do more like executing, editing, and copying pipelines?",Click the More Operations button on the right side of the newly created pipeline to perform related operations.,
What parameters are required during the process of creating a pipeline based on the built-in template?,"Code warehouse address, branch, credentials, test command, test report location, Dockerfile path, target image address, tag, and parameters such as warehouse credentials.",
What are the steps to create a pipeline based on built-in templates?,"1. Click `Create Pipeline` on the pipeline list page. \n2. In the pop-up dialog box, select `Template Creation` and click `OK`. \n3. Select the appropriate pipeline template and click `Next`. \n4. Refer to [Customized Creation Pipeline] (custom.md) to fill in the pipeline configuration, and then click `Next`. \n5. Fill in the template parameters according to the specified instructions, and then click OK. \n6. After the creation is completed, view the newly created pipeline in the pipeline list. Operations such as execution, editing, and copying can be performed.",
What built-in template creation pipelines does DCE 5.0 Application Workbench support?,"Supports Golang, Nodejs, and Maven templates.",
How to de-integrate?,"The toolchain instance assigned to the workspace supports the deintegration operation, and the administrator can also perform the deintegration operation.",
What can administrators do?,Administrators can perform operations such as de-integrating and assigning projects to workspaces.,
What operations are supported by platform-assigned toolchain instances?,The toolchain instance allocated by the platform only supports viewing and does not support operations such as binding and unintegration.,
What can be done with the workspace-integrated toolchain?,"The workspace integrated tool chain can unintegrate, bind the project under the instance to the current workspace and use it, unbind it, etc.",
What is a toolchain instance?,"Toolchain instances refer to integrated toolchains, which are divided into workspace-integrated toolchains and administrator-integrated toolchains.",
What are the new features in v0.12.0?,"Supports enabling traditional microservices and service mesh at the same time when creating an application. Query grayscale publishing supports fuzzy query, query continuous deployment supports fuzzy query and status retrieval, and query gitops warehouse supports fuzzy query and status retrieval. A new interface for obtaining helm information has been added, and helm chart can be deployed through argocd. Added Jenkins trace docking.",
What are the new features in v0.13.0?,"Supports cascade deletion, and you can select the resources to be deleted according to the actual situation. Grayscale publishing supports editing YAML function. Jenkins has been removed from the Chart package of the deployment application workbench, and Jenkins can be installed using a separate Helm Chart.",
What optimization features does v0.14.0 have?,Optimized the prompt when re-running a certain pipeline fails.,
What are the fixes in v0.15.1?,"Fixed a bug where the administrator could not obtain the associated tool chain instance details normally when binding a project to a specified workspace. Fixed the problem that when IMAGE_TAG is empty when creating applications based on from-jar and from-git, set it to latest in the corresponding template file.",
What are the new features in v0.16.1?,"Added independent observability-related configurations when creating applications, including indicator monitoring, link tracking and JVM monitoring. Added integrated tool chain support for Jenkins. Added support for the creation and management of multi-branch pipelines. Added support for toolchain integration from the administrator perspective. A new API has been added to support blue-green publishing, including creation, deletion, rollback, upgrade, details, etc.",
What is the latest version in Application Workbench Release Notes and when will it be released?,"The latest version is v0.16.1, released on 2022-04-30.",
Where can I view the vulnerability information of an image?,Visit `Project` → `Image Warehouse` in order in Harbor to view the vulnerability information of the image.,
How to configure the pipeline in the application workbench?,"Create a pipeline in the application workbench, enter the image warehouse address in the project configured in Harbor, and wait for the pipeline to execute successfully.",
How to enable automatic image scanning in Harbor?,"After logging in to Harbor, select a specific project and check `Automatically scan images` on the `Configuration Management` tab.",
This article describes how to integrate Harbor to achieve what functions?,Introduced how to integrate Harbor in the pipeline and implement image security scanning.,
How do I view code scan results?,"After waiting for the pipeline to run successfully, go to Sonar",
"In the above code, what does it mean when waitForQualityGate is set to false and true?",waitFor,
How to copy and paste YAML code into jenkinsfile?,Just copy the YAML code into the jenkinsfile.,
What is DCE 5.0 Application Workbench?,DCE 5.0 Application Workbench is part of DaoCloud DCE and is used to manage and deploy containerized applications.,
How to integrate SonarQube in Jenkins for code scanning?,"When creating a pipeline, you can edit the Jenkinsfile and copy and paste the relevant code into it, and finally wait for the pipeline to run successfully before going to Sonar",
How to create SonarQube Token for new project?,Available on Sonar,
How to deploy SonarQube?,Sonar can be installed via Helm,
What is SonarQube?,Sonar,
In what format are Jenkins configuration files saved?,Jenkins configuration files are saved in YAML format.,
How many parallel pipelines does the Jenkins scenario configuration recommended in this article apply to?,"The Jenkins scenario configuration recommended in this article is suitable for 50, 100, and 200 parallel pipelines.",
Which part of Jenkins does the main resource consumption come from?,The main resource consumption comes from the agent.,
What are the two parts of Jenkins?,Jenkins is divided into master and agent.,
What is Jenkins?,"Jenkins is an open source automated deployment server used to automate building, testing, and deploying software.",
How to upgrade amamba application?,Execute the following command to upgrade: `helm upgrade amamba . -n amamba-system -f ./amamba.bak.yaml`.,
What is this text used for?,This text provides an application configuration information that can be used to deploy and upgrade related applications.,
What application configuration information does this text contain?,"Redis, ArgoCD, Argo Rollouts, DevOps Server and MyS",
How to upgrade the application workbench version?,"First check whether the Helm warehouse of the application workbench exists, and update the Helm warehouse of the application workbench. Back up the `--set` parameters and select the version of the application workbench you want to install. Modify the `--set` parameters and execute the corresponding command to upgrade.",
How to load the image from the installation package?,Supports image synchronization through charts-syncer and direct image loading through Docker or containerd to load images from the installation package.,
What methods does the application workbench support for offline upgrades?,The application workbench supports offline upgrade by synchronizing images through charts-syncer and directly loading images through Docker or containerd.,
What operations does the grayscale publishing menu object in the application workbench have? What operating permissions do users with different roles have?,"Grayscale publishing menu objects include view, create, publish, continue publishing, terminate publishing, update, rollback and delete, etc. Users with different roles have different operation permissions, please refer to the table.",
What operations does the code repository menu object in the application workbench have? What operating permissions do users with different roles have?,"Code warehouse menu objects include view, import, delete, etc. Users with different roles have different operation permissions, please refer to the table.",
What actions does the Continuous Deployment menu object in the Application Workbench have? What operating permissions do users with different roles have?,"Continuous deployment menu objects include view, create, synchronize, edit, etc. Users with different roles have different operation permissions, please refer to the table.",
What actions does the Credentials menu object in the Application Workbench have? What operating permissions do users with different roles have?,"Voucher menu objects include view, create, edit, delete, etc. Users with different roles have different operation permissions, please refer to the table.",
What operations does the pipeline menu object in the application workbench have? What operating permissions do users with different roles have?,"Pipeline menu objects include viewing pipelines, viewing running records, creating, running, deleting, copying, editing and canceling runs, etc. Users with different roles have different operation permissions, please refer to the table.",
What actions does the namespace menu object in the application workbench have? What operating permissions do users with different roles have?,"Namespace menu objects include view, create, edit tags, edit resource quotas, delete, etc. Please refer to the table for specific operation permissions that users with different roles have.",
What operations does the application menu object in the application workbench have? What operating permissions do users with different roles have?,"Application menu objects include viewing application list, viewing details (jump to container management), viewing application logs (jumping to observable), viewing application monitoring (jumping to observable), and viewing RabbitM",
What three user roles does Application Workbench support? What permissions do they have?,"The application workbench supports three user roles: Workspace Admin, Workspace Editor, and Workspace Viewer. Please refer to the table for specific permissions.",
How to check if pipeline is running on specified node?,"Create a pipeline task in the application workbench, edit the Jenkinsfile and select label as base, click to execute the pipeline immediately, and then go to the container management to view the node where the Pod that executes the task is running.",
How to access Jenkins Dashboard and reload configuration?,"First, you need to expose the access address of Jenkins Dashboard through NodePort, enter the container management module, select Container Network -> Service in the left navigation bar of the cluster page, search for `amamba-jenkins`, and change the access type to NodePort. Then enter your account/password to log in to the Jenkins Dashboard, select Manage Jenkins->Configuration as Code in the left navigation bar, and click Reload existing configuration to reload the configuration.",
How to add a label to a specific node?,"Enter the container management module, navigate to the left of the target cluster details page, select Node Management, select the target worker node (for example, demo-dev-worker-03), click Modify Label, and add the `ci=base` label.",
How to modify the configuration file jenkins-casc-config?,"Go to the container management module, enter the details page of the target cluster, click Configuration and Key->Configuration Items, and set a specific Agent under the `jenkins.cloud.kubernetes.templates` location in the YAML configuration item `jenkins.yaml` Add `nodeSelector: ""ci=base""`.",
This article describes how to run the customer's pipeline tasks on specified nodes in the application workbench?,"By modifying the configuration file jenkins-casc-config, selecting the specified node to add a label, accessing Jenkins Dashbord, reloading the configuration, and finally checking whether it is on the specified node by running the pipeline.",
What common tools are built into the application workbench?,"The application workbench has built-in commonly used tools, including make, wget, gcc, etc. For a complete list, please refer to: <https://docs.daocloud.io/amamba/user-guide/pipeline/config/agent/#label_1>.",
How to create a custom image and add a custom toolchain?,The way to create a custom image (BYOI) is to package the custom toolchain into the image and select the image when creating the pipeline.,
How to add a custom toolchain through Volume mounting?,"Use the `init` container and `volumeMount` to copy the tool to the agent container, and modify the default behavior of the container by modifying the configmap of jenkins casc.",
What two methods does Application Workbench support for adding custom toolchains?,The application workbench supports adding custom tool chains through Volume mounting or building custom images.,
What is a custom toolchain?,Custom tool chain refers to the function that users can add or customize the tools in the application workbench to meet the needs of specific scenarios.,
How to configure build trigger?,"When creating a pipeline based on Jenkinsfile, you can configure code source triggering and scheduled warehouse scanning schedules, or configure scheduled triggers to run the pipeline regularly.",
How to configure build settings and build parameters?,"When creating a pipeline based on Jenkinsfile, you can configure settings such as deletion of expired pipeline records, build record retention period, maximum number of build records, disallowing concurrent builds, etc., and you can add Boolean, string, multi-line text and other types of build parameters.",
How to configure code warehouse information?,"When creating a pipeline based on Jenkinsfile, you need to fill in the address of the remote code warehouse, select the corresponding private warehouse access credentials (if it is a private warehouse), select which branch the code is based on to build the pipeline, the default is the master branch, fill in the Jenkinsfile file in the code warehouse The absolute path in .",
What are the prerequisites for creating a pipeline based on Jenkinsfile?,"You need to create a workspace, create a user and add it to the workspace and grant `workspace editor` or higher permissions. Provide a code warehouse and the source code of the code warehouse has a Jenkinsfile text file. If it is a private warehouse, you need to create warehouse access credentials in advance. .",
How to create a Jenkinsfile-based pipeline in the application workbench?,"Click `Create Pipeline` on the pipeline list page, select `Jenkinsfile based on code base to create pipeline`, fill in the basic information and code warehouse information, fill in the build settings and build parameters, fill in the build trigger, and complete the creation.",
How to cancel an executing pipeline?,"On the pipeline details page, you can cancel the executing pipeline according to the execution ID in the pipeline running record. Click `︙` on the right and select `Cancel` in the pop-up menu (applicable to the case where the status is `Executing`).",
How to rerun an already run pipeline?,"On the pipeline details page, you can rerun the pipeline that has been run according to the execution ID in the pipeline running record. Click `︙` on the right and select `Rerun` in the pop-up menu.",
How to perform immediate execution pipeline?,"Select a pipeline on the pipeline list page, click `︙`, and click `Execute Now` in the pop-up menu. Depending on whether the build parameters are configured, configure parameters in the dialog box or start execution directly.",
What are the methods to introduce manual execution of pipelines?,"The pipeline can be executed manually through three methods: immediate execution, rerun, and cancel run.",
How to view scan logs and discovered branch information?,You can view the scan log and discovered branch information on the multi-branch pipeline details page.,
How to do shallow cloning?,You can turn on the shallow cloning function and set the cloning depth to speed up pulling.,
How to delete old branch and its pipeline?,"You can enable the function of deleting old branches, and set retention days or retention quantity policies as needed to delete old branches and their corresponding pipelines.",
How to trigger execution of multi-branch pipeline?,"After the creation is completed, the corresponding pipeline for the branch that meets the conditions will be automatically triggered; you can also manually trigger the scan warehouse operation to trigger the discovery of new branches in the code warehouse and execute the corresponding pipeline.",
How to create a multi-branch pipeline?,"First, you need to create a workspace and user, add the user to the workspace and grant editing permissions; provide a code warehouse, the source code in the code warehouse has multiple branches, and all have Jenkinsfile text files. Then click ""Create Pipeline"" on the pipeline list page, select ""Create Multi-branch Pipeline"", fill in the basic information, code warehouse information, branch discovery strategy and other related configurations, and finally click OK.",
How to set resource quotas in an existing namespace?,"Select a namespace on the namespace list page, click Quota Management, and set it in the resource quota dialog box that pops up.",
What are the steps to create a namespace?,"The steps include: enter the namespace list, click Create, configure basic information, configure resource quotas, and click OK.",
Who has permission to create namespaces?,Only Workspace Admin is supported to create a namespace.,
What are the prerequisites for the creation of a namespace?,1. The current workspace already has cluster resources; 2. The current user is authorized to workspace Admin role.,
What is a namespace?,A namespace is an abstraction used in Kubernetes to isolate resources.,
How to delete an imported code repository?,"Select a code repository on the code repository list page, click `︙`, click `Delete` in the pop-up menu, and then click Confirm in the secondary confirmation pop-up window.",
How to import a warehouse using SSH?,"In the `Application Workbench` -> `GitOps` -> `Warehouse` page, click the `Import Warehouse` button, select `Use SSH`, configure the relevant parameters and click OK.",
How to import a warehouse using HTTPS?,"In the `Application Workbench` -> `GitOps` -> `Warehouse` page, click the `Import Warehouse` button, select `Use HTTPS`, configure the relevant parameters and click OK.",
What two import methods does Application Workbench support?,The application workbench supports two methods of importing the warehouse using HTTPS and SSH.,
What are the prerequisites for importing a warehouse?,"You need to create a workspace and a user. The user needs to join the workspace and be assigned the `workspace edit` role, and a Git repository needs to be prepared.",
What is an import repository?,Importing the repository is to import the Git repository to the application workbench for continuous application deployment.,
How to define a drop-down list type parameter?,"Set `type` to `choice` and provide a default value in `default`, for example:\n```yaml\ntype: choice\ndefault:\n- choice 1\n- choice 2\n````",
What parameter types can be defined in the `parameterDefinitions` area?,"Supports multiple parameter types such as Boolean values, drop-down lists, credentials, passwords, and text.",
What is a pipeline template file?,"The pipeline template file mainly contains two parts: `parameterDefinitions` and `jenkinsfileTemplate`, which are used to define which parameters are exposed by the pipeline template and the `jenkinsfile` of the Jenkins pipeline, which can reference the exposed parameters.",
Which tool only supports administrator integration?,Only support for administrator integration is the Jenkins tool.,
How to do toolchain integration in Application Workbench?,"First enter the `Toolchain Integration` page, then click the `Toolchain Integration` button, configure the relevant parameters and click `OK` to complete the integration.",
What toolchains does Application Workbench support for integration?,"The application workbench supports integration with tool chains such as GitLab, Jira and Jenkins.",
Which two perspectives of tool chain integration are supported by Application Workbench?,The application workbench supports tool chain integration from two perspectives: workspace integration and administrator integration.,
What is a DevOps toolchain?,A DevOps toolchain is a set of tools that enable DevOps teams to collaborate and solve critical DevOps foundational issues throughout the product lifecycle.,
How to view the newly created application after the application workbench is successfully created?,"After the application workbench is successfully created, click ""Overview"" on the left navigation bar and select the ""Native Applications"" tab to view the newly created application.",
How to configure the container's environment variables and data storage?,"The container's environment variables and data storage can be configured in the container configuration. Environment variables can configure the container parameters in the Pod, add environment variables or transfer configurations to the Pod, etc.; the data storage can configure the container-mounted data volume and data persistence settings.",
How to configure lifecycle and health checks for containers?,"Lifecycle and health checks can be set for the container in the container configuration. The life cycle includes commands that need to be executed when a container starts, after starting, and before stopping. Health checks can be used to determine the health status of containers and applications.",
How to set the access type and resource restrictions of the container?,"Access types and resource limits for a container can be set in the container configuration. The access type supports clusterIP, NodePort and load balancer, and the resource limit needs to be set according to the remaining CPU and memory quota of the namespace in the current workspace.",
What methods does the DCE 5.0 application workbench support for building applications? How to deploy Java application through Jar file?,"The DCE 5.0 application workbench supports four ways to build applications through Git repository, Jar package, container image, and Helm template. Steps to deploy Java applications through Jar files:\n1. Create a workspace and user. \n2. Create credentials to access the mirror repository. \n3. Prepare a mirror warehouse. \n4. Select ""Wizard"" in the application workbench, and then select ""Build based on Jar package"". \n5. Fill in the basic information and configure the pipeline as required. \n6. Fill in the container configuration as required, choose whether to enable advanced features, and then upload the Jar file. \n7. After waiting for the pipeline to execute successfully, view the newly created application in the Overview tab.",
How to edit pipeline through Jenkinsfile?,"Find the pipeline that needs to be created, click `Edit Jenkinsfile` on the right, enter or paste the prepared Jenkinsfile, and then click OK.",
How to run the pipeline?,Click `Execute Now` on the pipeline details page and set the string parameters defined in the prerequisites to run the pipeline.,
How to add deployment to cluster stage?,"Click `Add Stage` in the canvas, set the name to deploy in the stage settings on the right, select to open `Specify Container` in the step module and fill in the container name: go, select to open `Use Credentials` in the step module and fill in the Related parameters, and then click `Add step` to perform cluster deployment operation.",
How to add a review stage?,"Click `Add Stage` in the canvas, set the name to review in the stage settings on the right, then click `Add Step`, select `Review` under the step type, and fill in `@admin` in the message field.",
What is DCE 5.0 Application Workbench?,"DCE 5.0 application workbench is a cloud-native application development management platform that supports the rapid construction, continuous delivery and deployment of applications in multiple languages and frameworks.",
How to verify cluster resources?,"After each stage runs successfully, resources are automatically deployed to the cluster. You can enter the pipeline record details page to view the running process and conduct audits.",
How to run the pipeline?,"Click Execute Now on the pipeline details page, set the three string parameters defined in the prerequisites, and then click OK to run the pipeline.",
How to configure the review stage?,"When adding a stage, set the name to review, select Review in the steps module, and fill in the message field, such as @admin.",
How to configure the unit test phase?,Select to open the specified container in the step module and fill in the container name in the pop-up dialog box. Then select the shell in the add step and enter the relevant commands in the command line.,
How do I configure global settings?,"Click Global Settings in the interface, select node from the type drop-down list, and select the corresponding label from the label drop-down list, such as go 16.",
What are the two areas included in the graphical interface editing page of the application workbench?,Canvas (left) and stage settings (right).,
What are the prerequisites for editing a pipeline?,"You need to create a workspace, user, credentials, and create a custom pipeline and add two string parameters.",
What editing pipeline methods does the DCE 5.0 application workbench support?,Execution stages can be defined for the pipeline by editing the Jenkinsfile or through a graphical interface form.,
What should I do if I add an existing warehouse to the GitOps module in the workspace?,"First unbind the repository from its currently bound workspace, and then re-add it to the target's workspace.",
How to solve the problem of import failure when importing GitHub repository through HTTP?,Import the GitHub repository using SSH.,
What should I do if the container cannot access the private image repository when building an image in Jenkins?,You can add the following command to the Jenkinsfile of the pipeline: `cat > /etc/containers/registries.conf << EOF [registries.insecure] registries = ["temp-registry.daocloud.io"] EOF`.,
How to solve the error when the application workbench executes the pipeline?,"In the Jenkinsfile of the pipeline, change the deployment command from `kubectl apply -f` to `kubectl apply -f . --request-timeout=30m`.",
What advanced publishing strategies does grayscale publishing support?,"Canary release, blue-green deployment, A/B Testing.",
What concepts does continuous deployment use to implement application deployment?,Introduce the GitOps concept to achieve continuous application deployment.,
What are the ways to create pipeline orchestration?,"Custom creation, creation based on Jenkinsfile, creation based on template, creation of multi-branch pipeline.",
What does application management include?,"Supports multi-form cloud native application management, full life cycle management, microservice governance, and integration with microservice engines and service grids.",
What features does the DCE 5.0 Application Workbench provide?,"Application management, pipeline orchestration, credential management, continuous deployment, warehouse management, and grayscale release.",
What should I do if I need to update or delete a deployed Helm application?,"You need to click on the application name to jump to the container management module, where you can update, delete and perform more operations on the application details page.",
How to check the Helm applications installed in the current cluster?,"The page automatically jumps to the Helm application list under the overview page, where you can view the Helm applications installed in the current cluster.",
How to deploy Helm application through Helm template?,"After entering the application workbench module, click `Wizards` in the left navigation bar, and then select `Based on Helm template`. \n- Select the cluster in which the application needs to be deployed at the top of the page, and then click the Helm Chart card that needs to be deployed. \n- Read the installation prerequisites, parameter configuration instructions and other information of the application, select the version to be installed in the upper right corner, and then click `Install`. \n- Set basic information such as application name and namespace, then configure parameters through the form or YAML below, and finally click OK at the bottom of the page.",
What are the prerequisites for deploying Helm applications using Helm templates?,You need to create a workspace and a user. The user needs to join the workspace and be assigned the `workspace edit` role.,
What methods of building applications does Application Workbench support?,"Supports four ways to build applications through Git repository, Jar package, container image and Helm template.",
How to manually sync the Argo CD app?,"On the application details page, select the manual synchronization method and wait for the synchronization to be successful to view the synchronization results.",
How can I view the details of the Argo CD application?,"After successful creation, click the application name to enter the details page to view the application details.",
How to create an Argo CD application?,"In the Application Workbench -> Continuous Publishing page, click the Create Application button. After filling in the basic information, deployment location, code repository source and synchronization strategy on the Create Application page, click OK.",
What are the prerequisites for applying the workbench?,"Create a workspace and a user, add the user to the workspace and assign the workspace edit role; prepare a Git repository.",
What is Application Workbench?,The application workbench is a platform for continuous deployment based on the open source software Argo CD.,
How to create and manage credentials?,"Click `Pipeline`->`Vouchers` in the left navigation bar to enter the voucher list, click `New Voucher` in the upper right corner, fill in the corresponding fields according to different voucher types and click `OK` to create it successfully. You can edit or delete through `︙` on the right side of the list. However, please note that if you delete the credentials used by a certain pipeline, access to the pipeline may be affected, so please operate with caution.",
What types of credentials can be created in the App Workbench?,"The following three types of credentials can be created in the application workbench: username and password, access token (Secret text), and kubeconfig.",
What is credential management?,"Credential management means that in order to interact with third-party websites or applications when the pipeline is running, users are required to create and configure credentials in the application workbench to save sensitive information, such as usernames, passwords, access tokens, etc.",
How to delete or modify a custom pipeline template?,Users can click the corresponding card on the pipeline template list page to delete or modify it.,
What are the prerequisites for customizing pipeline templates?,"Creating a custom pipeline template requires the user to first create a workspace and user, add the user to the workspace, and grant ""workspace editor"" or higher permissions. At the same time, the ""pipeline template file"" needs to be configured.",
How to create a custom pipeline template?,"Users need to click ""Create Template"" on the ""Customized Pipeline Template"" page of the application workbench, fill in the basic information and refer to the ""Pipeline Template File"" for parameter configuration, and then save it.",
What custom pipeline templates does DCE 5.0 Application Workbench support?,DCE 5.0 application workbench supports user-defined pipeline templates.,
How to set trigger parameters?,"Trigger parameters include code source trigger, scheduled warehouse scan plan and scheduled trigger.",
How to fill in basic information and build settings?,"Basic information includes name, deletion of expired pipeline records, build record retention period and maximum number of build records. Build settings include disallow concurrent builds and build parameters.",
How to create a custom pipeline?,"Prerequisites: Create a workspace, create a user, add the user to the workspace and grant workspace editor or higher permissions. The specific steps are as follows:\n1. Click Create Pipeline on the pipeline list page. \n2. In the pop-up dialog box, select custom creation pipeline and click OK. \n3. Refer to the instructions to fill in the basic information, build settings, and build parameters. \n4. Fill in the trigger parameters according to the instructions. \n5. Click the OK button to complete the creation of the custom pipeline.",
How to quickly create a pipeline in DCE 5.0?,"In the pipeline list page of the workbench, select `Create Pipeline` -> `Customize Create Pipeline`, enter the name and add the build parameters and stages, and finally save and execute.",
How to enable monitoring analysis to monitor based on predefined monitoring indicators?,"When creating a canary release task, you can turn on the monitoring and analysis option, and then automatically perform monitoring and analysis throughout the entire release process based on predefined monitoring indicators and collection intervals. If the defined rules are not met, it will automatically roll back to the old version and grayscale publishing will fail.",
How to configure traffic scheduling policy in canary release?,"In canary publishing, you can configure traffic scheduling policies, such as weight policies, blue-green policies, etc., to control traffic distribution.",
How to set the number of instances in canary release?,"In canary publishing, you can set the number of instances to control the number of copies applied when performing grayscale publishing.",
How to create a canary release task?,"You can create a canary release task by following the following steps:\n1. Enter the `Application Workbench` module, click `Grayscale Release` in the left navigation bar, and then click `Create Release Task`->`Canary Release in the upper right corner of the page Bird release`. \n2. Fill in the basic information (name, cluster, namespace, stateless load) and click Next. \n3. Configure the release rules (number of instances, traffic scheduling policy, current release traffic ratio, waiting time after reaching the traffic ratio, and monitoring analysis), and then click Next. \n4. Set the image address of the grayscale version and click OK. \n5. The system automatically jumps to the task list page of the grayscale release, indicating that the updated version is successful.",
Which open source project is the grayscale release of DCE 5.0 application workbench based on?,The grayscale release of the DCE 5.0 application workbench is based on the open source project Argo Rollout.,
How to roll back to a previous version?,"On the grayscale release task details page, click the ""Historical Version"" tab, select the target version and click ""Rollback"".",
How to update the traffic scheduling policy of the grayscale publishing process?,"On the grayscale publishing task details page, click ""ⵗ"" and select ""Update publishing task"", adjust the publishing rules and click ""OK"".",
How to update canary version?,"Click the name of the target task, click ""Update Version"" in the upper right corner, and set the image released by Canary.",
How to view the details of a Canary release task?,"Enter the application workbench module, click ""Grayscale Release"" in the left navigation bar, and click the name of the target task to view details.",
What is a canary release task?,"The canary release task means that after creating the grayscale release task and associating the workload, modifying the workload's image, resource configuration, startup parameters, etc. so that when the container group is restarted, the grayscale release task update version will be automatically triggered, and as defined The process of publishing rules for traffic scheduling.",
What are steps?,The step list describes what specifically needs to be done in a stage and what specific commands need to be executed.,
What is the parallel phase?,Parallel is used to declare some stages of parallel execution. It is usually suitable for speeding up execution when there is no dependency between stages.,
What are stages?,A phase defines a series of closely related steps. Each stage has independent and clear responsibilities in the entire pipeline.,
What is Agent?,"Agent describes the entire pipeline execution process or the execution environment of a certain stage, and must appear in the top grid of the description file or in each stage.",
What is an assembly line?,"The pipeline is a user-defined working model. The code of the pipeline defines the complete process of software delivery, which generally includes the stages of building, testing and delivering applications.",
What are the credentials? What role does it play in the assembly line?,Credentials in the application workbench refer to configuring Jenkins credentials for authentication and authorization when the pipeline interacts with third-party applications.,
What is an assembly line? What does it do?,"The pipeline is a concept in the application workbench, which provides a visual and customizable automatic delivery pipeline to help enterprises shorten the delivery cycle and improve delivery efficiency. The current pipeline is implemented based on Jenkins.",
What is a namespace? How does it relate to the workspace?,"The namespace on the platform is a smaller resource space that is isolated from each other under the workspace. It is also the workspace where users implement job production. Multiple namespaces can be created under one workspace, and the total resource quota that can be occupied cannot exceed the workspace quota. The namespace divides resource quotas in a more fine-grained manner and limits the container size (CPU, memory) under the namespace, effectively improving resource utilization.",
What is a workspace? What does it do?,"Workspace is a concept in the application workbench. It solves resource aggregation and mapping hierarchical relationships by coordinating global management and permission relationships of sub-modules. A workspace corresponds to a project, and each workspace can be assigned different resources and assigned different users and user groups.",
What is Application Workbench? What functionality does it provide?,"The application workbench is the unified entrance for DCE 5.0 application deployment, providing the entire life cycle management of cloud native applications. It provides functions such as workspaces, namespaces, pipelines, credentials, GitOps, grayscale publishing, and toolchain integration.",
What conditions need to be met to enable microservice governance?,"Enabling microservice governance requires selecting the correct framework, configuring the registration center instance and parameters, and ensuring that the registration center instance has the Sentinel or Mesh governance plug-in enabled.",
What settings are required for container configuration?,"Settings such as service configuration, resource limits, life cycle, health checks, environment variables and data storage need to be provided.",
What parameters need to be filled in the pipeline configuration?,"You need to fill in parameters such as code warehouse address, Dockerfile path, target image name and Tag.",
What is the purpose of creating a credential?,Credentials are created to use them in subsequent operations to access the code repository and image repository.,
How to build traditional microservice applications based on Git warehouse source code?,"Refer to the following steps:\n1. Create a workspace and user, and create credentials that can access the code warehouse and image warehouse. \n2. In the wizard page of the application workbench, select ""Build based on Git repository"" and fill in the basic information and pipeline configuration, including code repository address, Dockerfile path, target image name, etc. \n3. Fill in the container configuration, including resource limits, life cycle, health check, environment variables and data storage. \n4. Enable access to microservices on the advanced configuration page, and select parameters such as framework and registration center instance. \n5. Click OK to view and access microservice related information.",
What methods does App Workbench support for building apps?,"Supports four ways to build applications through Git repository, Jar package, container image, and Helm template.",
How to customize podTemplate using YAML?,"On the edit pipeline page, select the Agent type as kubernetes, click the YAML editor, fill in the YAML statement in the dialog box, and enter the default container name that needs to be run as the pipeline in Container. When using other containers in other steps of the pipeline you can optionally specify the container and fill in the required container name.",
"What environments does the built-in Label include, and what are their respective versions?","base: centos-7, podman 3.0.1, kubectl v1.22.0, unzip, which, make(GNU Make 3.82), wget, zip, bzip2, git (2.9.5)\n- maven: centos-7, openjdk -1.8.0_322, Maven 3.5.3, podman 3.0.1, kubectl v1.22.0, unzip, which, make(GNU Make 3.82), wget, zip, bzip2, git (2.9.5)\n- go: centos- 7. Go 1.12.10, GOPATH /home/jenkins/go, GOROOT /usr/local/go, podman 3.0.1, kubectl v1.22.0, unzip, which, make (GNU Make 3.82), wget, zip, bzip2, git (2.9.5)\n- go16: centos-7, Go 1.16.8, GOPATH /home/jenkins/go, GOROOT /usr/local/go, podman 3.0.1, kubectl v1.22.0, unzip, which, make(GNU Make 3.82), wget, zip, bzip2, git (2.9.5)\n- node.js: centos-7, Node v10.16.3, Yarn 1.16.0, podman 3.0.1, kubectl v1.22.0, unzip, which, make (GNU Make 3.82), wget, zip, bzip2, git (2.9.\n- python: centos-7, Python 3.7.11, podman 3.0.1, kubectl v1.22.0 , unzip, , make (GNU Make 3.82), wget, , bzip2, git (2.9.",
How to use built-in Label to specify Agent label?,"You can use go's podTemplate through `node(""go"")` in the Jenkinsfile, or you can select an Agent of type `node` and label `go` on the edit pipeline page.",
What is the role of the jnlp container in the Kubernetes plug-in?,The role of the jnlp container is to communicate between Jenkins Controller and Jenkins Agent.,
What is a pipeline node (Agent)?,"Agent describes the entire pipeline execution process or the execution environment of a certain stage, and must appear at the top of the description file or at each stage.",
"Under what circumstances will the health status of ""paused"", ""unknown"" and ""lost"" appear?","""Paused"", ""Unknown"", and ""Lost"" refer to health states that occur when a resource is paused and waiting for some external event to resume (such as a paused CronJob or a paused deployment), the current health status cannot be determined, and the resource is missing.",
How to determine the synchronization status of resources in Argo CD?,"The synchronization status of resources in Argo CD can be judged by viewing the synchronized, unsynchronized, or unknown status.",
How to determine the health status of resources in Argo CD?,"The health status of resources in Argo CD can be judged by viewing the status of the resource: Healthy, Degraded, In Progress, Paused, Unknown, or Lost.",
What is the meaning of sync status in Argo CD?,"Sync status in Argo CD includes synchronized, unsynchronized, and unknown.",
What is the meaning of health status in Argo CD?,"Health states in Argo CD include Resource Health, Degraded, In Progress, Paused, Unknown, and Lost.",
How to run a pipeline to trigger CI/CD?,Select the created pipeline and click Run Now.,
What is the initial sync state of a GitOps application?,Not synced.,
What are the steps in the pipeline?,"Pull business code, build images, and update application configuration files.",
What information is required in the prerequisites?,"Two code repositories, a Harbor mirror repository, and credentials to access these repositories.",
This article introduces which technology implements CI/CD?,Based on pipeline and GitOps.,
How to verify the effect of progressive grayscale publishing?,"Access address: `http://{istio-ingressgateway LB IP}:8082`, and get the access effect as shown below. This interface will concurrently call `http://{istio-ingressgateway LB IP}:8082/color` to fill the color information into the grid. In the grayscale publishing object, the specified colors are **blue and yellow**, which will be displayed according to the traffic ratio of 1:9 according to the defined rules.",
How to modify VirtualService?,It is necessary to delete the original host name of vs and change it to `'*'`.,
What are the steps to implement progressive grayscale release?,"The whole process is divided into four steps: build the application based on the container image, configure Istio-related resources, create grayscale publishing tasks, and finally verify the effect.",
What are the prerequisites to implement progressive grayscale release?,"The image in the example requires access to the public network, is only applicable to the DCE 5.0 platform deployed in metallb mode through the installer, and requires Istio and Argo Rollout components to be installed in the cluster.",
What does this article cover?,"This article introduces how to implement progressive grayscale release based on the open source Argo Rollout, and provides detailed operation steps.",
Can Application Workbench run on multiple operating systems and multiple architecture nodes?,"Yes, steps in a single pipeline in Application Workbench can run on multi-OS, multi-architecture nodes.",
What DevOps tools can be integrated into the application workbench?,"The application workbench integrates popular DevOps tools in the community, such as Kubernetes, GitLab, and Sonar",
What implements a standardized and easy-to-learn declarative pipeline?,A declarative pipeline is a standardized and easy-to-learn pipeline based on a declarative (syntax) implementation.,
Does Application Workbench support GitOps continuous delivery capabilities?,"Yes, Application Workbench provides declarative GitOps continuous delivery capabilities based on Kubernetes.",
"After grayscale is released, can the application workbench calculate the diversion ratio of different versions?","Yes, after the grayscale release, the application workbench can easily calculate the diversion ratio of different versions of the application.",
Does Application Workbench support multiple types of application deployment?,"Yes, Application Workbench supports multiple types of application deployments.",
How does Application Workbench's CI/CD support horizontal scaling?,Application Workbench uses centrally managed containerized workflows to support horizontal scalability and can support thousands of concurrent users and pipelines in a high availability (HA) environment.,
What are the advantages of Application Workbench?,- Enterprise-level CI/CD\n- Cloud native as the base\n- Improve R&D efficiency\n- Grayscale release\n- GitOps\n- Pipeline as code\n- Comprehensive integration\n- Years of industry experience,
"How does the cloud-edge collaboration module of DCE 5.0 solve the need to uniformly complete large-scale application delivery, operation and maintenance, and management and control on massive edge and end devices?","The cloud-edge collaboration module of DCE 5.0 extends cloud native capabilities to the edge to uniformly manage, control and schedule discrete and heterogeneous computing resources to solve the need to uniformly complete large-scale application delivery, operation and maintenance, and management and control on massive edge and end devices.",
What domestic chips and servers are DCE 5.0’s Xinchuang heterogeneous modules compatible with?,DCE 5.0’s Xinchuang heterogeneous module is compatible with domestic chips and servers.,
How can the application delivery module of DCE 5.0 implement self-service migration to the cloud?,The application delivery module of DCE 5.0 can achieve self-service cloud migration through a consistent and generalizable application delivery process.,
Which ecosystem partners’ software products are included in the DCE 5.0 app store?,"The DCE 5.0 app store includes software products from ecological partners in ten major fields such as big data, AI, and middleware.",
"Which modules of DCE 5.0 can control real-time changes in clusters, nodes, applications and services?","The observability module can control real-time changes in clusters, nodes, applications and services.",
What are the features of the observability module of DCE 5.0?,"The observability module is based on technical means such as logs, links, indicators, and eBPF to comprehensively collect service data, obtain in-depth request link information, and dynamically observe and control real-time changes in clusters, nodes, applications, and services in multiple dimensions. Through the unified control plane, all clusters and load observation data can be queried, and topology analysis technology is introduced to visually grasp the application health status and achieve second-level fault location. Modules involved include global management, container management, observability, cloud native networking and cloud native storage.",
What functions does the microservice governance module of DCE 5.0 provide?,"The microservice governance module provides non-intrusive traffic management functions, supports seamless access to traditional microservices, cloud-native microservices and open source microservice frameworks, and realizes the integrated management of the enterprise's existing microservice system and new and old microservice systems. It supports full life cycle management from development, deployment, access, observation to operation and maintenance, provides high-performance cloud native microservice gateway, and ensures the continuous availability of microservice applications. The modules involved include global management, container management, microservice governance, service grid, observability and application workbench.",
What are the characteristics of DCE 5.0’s data middleware services?,"Data middleware services are specially designed for stateful applications to meet the high I/O storage requirements of middleware and improve operation and maintenance management efficiency. It provides full life cycle middleware management capabilities such as multi-tenancy, deployment, observation, backup, and operation and maintenance operations, and realizes self-service application, elastic expansion, high concurrency processing, and stable and high availability of data services. Modules covered include global management, container management, cloud native storage, and selected middleware.",
What are the functions of the multi-cloud orchestration module of DCE 5.0?,"The multi-cloud orchestration module supports unified and centralized management of multi-cloud and hybrid clouds, provides cross-cloud resource retrieval and cross-cloud application deployment, release and operation and maintenance capabilities, and achieves efficient management and control of multi-cloud applications. It can also provide elastic scaling of applications based on cluster resources, achieve global load balancing, and have fault recovery capabilities. The modules involved include global management, container management, cloud native network and cloud native storage.",
What is DaoCloud Enterprise 5.0? What capabilities does it provide?,"DaoCloud Enterprise 5.0 is a high-performance and scalable cloud-native operating system that provides a consistent and stable experience. It supports Xinchuang heterogeneity, edge-cloud collaboration, and multi-cloud orchestration. It integrates cutting-edge service grid and microservice technologies, and will be observable. Sex runs through every flow. It provides 9 major capabilities, including container management, global management, observability, application workbench, multi-cloud orchestration, microservice engine, service mesh, middleware and mirror warehouse.",
How to expose the API and access the application?,"First, you need to create a cloud native gateway, then connect the service to the gateway and create the corresponding API route. For specific operation steps, please refer to [Create a cloud native gateway](../ms-gateway/gateway/create-gateway.md).",
How to configure flow control rules?,"Find the service for which flow control rules need to be configured in the microservice list, and then add a flow control policy in the Flow Control Rules tab.",
How to enable traditional microservice governance plug-in?,"The Sentinel management plug-in needs to be enabled in the plug-in center under the corresponding registration center. For specific steps, please refer to [Enable Sentinel Management Plug-in](../registry/managed/plugins/sentinel.md).",
How to deploy a sample application based on Helm template?,"In `Application Workbench`->`Wizard`->`Based on Helm Template`, find the opentelemetry-demo application and click the application card to install it. According to the registration center address recorded above, just update the JAVA_OPTS parameter in the Helm Install interface.",
What is the architecture of the sample application?,"The architecture of the sample application is a standard demonstration application based on OpenTelemetry, which is optimized by the DaoCloud microservices team based on the functions of DCE 5.0 to better reflect cloud native and observable capabilities and present microservice governance effects.",
What is DCE 5.0 microservice engine?,"The DCE 5.0 microservice engine is an advanced version, including registration center, configuration center, microservice management (traditional microservices, cloud native microservices), cloud native gateway and other functions.",
What important role does the microservice gateway play? What capabilities are provided?,"The microservice gateway plays an important role in managing the north-south traffic control of microservices. It provides API management, interface current limiting, multiple policy security authentication, black and white lists, routing and forwarding, MockAPI and other capabilities. It also provides enterprise-level high performance and highly scalable cloud service capabilities.",
What role can the microservice configuration center play?,"Common configurations can be extracted from different projects and managed uniformly in advance, or multiple different configurations can be applied to the same project to achieve differentiated management. At the same time, it can dynamically update configuration items, manage historical versions of configuration files, support version difference comparison and roll back to a specific version with one click, and provide sample code for novices to use.",
"At the microservice traffic management level, what solutions are used to solve the pain points in different production situations?","Using online traffic management solutions, you can quickly integrate with mainstream open source microservice frameworks, and use Sentinel and Mesh to solve pain points in different production situations.",
Which traditional microservice registries and cloud-native microservice registries are supported?,"Supports access to three traditional microservice registration centers, Eureka, Zookeeper, and Nacos, and supports access to two cloud-native microservice registration centers, Kubernetes and Service Mesh.",
What two dimensions of functionality does the microservice engine provide?,Microservice governance center and microservice gateway.,
How to update the basic information and policy configuration of a domain name?,"You can modify the basic information and policy configuration of a domain name in two ways. One is to find the domain name that needs to be updated on the domain name management page, click ⵗ on the right and select Edit Basic Information or Edit Policy Configuration. The other is to click the domain name to enter the domain name details page, click Modify Basic Configuration in the upper right corner of the page to update the basic information, and click Modify Policy to update the policy.",
What domain name management operations does the microservice gateway support?,"The microservice gateway supports full life cycle management of adding, updating, and deleting domain names, including modification of basic information and policy configurations.",
On which page is the update operation of the microservice gateway performed?,The update operation of the microservice gateway can be performed on the "Microservice Gateway List" page or on the gateway overview page.,
How many ways are there to update the configuration of a microservices gateway instance? Please describe each.,"There are two ways to update the configuration of a microservice gateway instance:\n- Select the gateway instance that needs to be updated on the ""Microservice Gateway List"" page, click **`⋯`** on the right side of the instance and select `Edit`. \n- Click `Edit` in the upper right corner of the gateway overview page.",
What should I do if I need to update the Kubernetes/Mesh registry?,"You can first [Remove the already connected registration center] (remove-registry.md), and then reconnect to other registration centers. You can also go to the container management module [Update the corresponding cluster] (../../../kpanda/user-guide/clusters/upgrade-cluster.md), or go to the service grid module [Update the corresponding grid service] ](../../../mspider/user-guide/service-mesh/README.md).",
What operations need to be done when updating the registration center configuration?,"Add or delete the registration center address, then click OK at the bottom of the page.",
How to enter the registration center configuration update page?,"On the `Access Registration Center List` page, select the registration center that needs to be updated, click **`⋯`** on the right and select `Edit`.",
The microservice engine currently only supports updating the configurations of which registration centers?,The microservice engine currently only supports updating the configuration of the Nacos/Zookeeper/Eureka registration center.,
Can the hosted registry name and deployment location be edited?,The hosted registry name and deployment location are not editable.,
What happens when the storage pool and database are modified?,"If the storage pool and database are modified, the previous data will not be migrated.",
How do I update the configuration of a hosted registry?,"1. On the `Access Registration Center List` page, select the registration center that needs to be updated, click **`⋯`** on the right, and select `Edit`. \n2. Modify the configuration that needs to be modified on the update page, and then click OK at the bottom of the page to save.",
How to practice observability when using DCE 5.0 for microservice management?,"You can view the topology structure of the application after deployment, view the log content of the application, view the access log of the cloud gateway gateway API, etc. For specific operation steps, please refer to [DCE 5.0 Observability Practice](https://docs.daocloud.io/skoala/observability/).",
How to access the application through API Gateway?,"The corresponding API route needs to be created. For specific steps, please refer to [DCE 5.0 Cloud Native Gateway](https://docs.daocloud.io/skoala/ms-gateway/gateway/). Use the **domain name** and **external API path** configured when creating the API to successfully access the application page.",
How to create a cloud native gateway and access services?,"First, you need to create a cloud native gateway. For specific steps, please refer to: [Create a cloud native gateway](https://docs.daocloud.io/skoala/ms-gateway/gateway/create-gateway), and then you can choose to connect to Nacos Registration center services. For specific steps, please refer to [DCE 5.0 Cloud Native Gateway](https://docs.daocloud.io/skoala/ms-gateway/gateway/).",
How to configure flow control policy in DCE 5.0?,"You need to view the corresponding service in the `Microservice List` under the corresponding registration center, select the flow control rule and configure it. For specific steps, please refer to [DCE 5.0 Microservices Governance](https://docs.daocloud.io/skoala/microservices/microservices-traffic/#%E6%B5%81%E9%87%8F%E6%B2%BB% E7%90%86).",
What preparations need to be done before using microservice governance?,"The corresponding governance plug-in needs to be enabled in the plug-in center under the corresponding registration center. In this practice, traditional microservice governance is used, that is, the Sentinel governance plug-in is enabled. For specific steps, please refer to [Enable Sentinel management plug-in](https://docs.daocloud.io/skoala/registry/managed/plugins/sentinel).",
How to create a microservice application in DCE 5.0?,"You can arrange YAML files or choose to deploy applications based on container images. For specific steps, please refer to [DCE 5.0 Microservice Engine Quick Start Guide] (https://docs.daocloud.io/skoala/getting-started).",
What is DCE 5.0 microservice engine?,"DCE 5.0 microservice engine is a microservice management platform launched by DaoCloud for Kubernetes applications, providing a full-process solution for application creation, deployment, and governance.",
From what dimensions does Sentinel help developers ensure the stability of microservices?,"Sentinel mainly helps developers ensure the stability of microservices from multiple dimensions such as traffic routing, traffic control, traffic shaping, circuit breaker degradation, system adaptive overload protection, and hotspot traffic protection.",
How to enable Sentinel governance plugin?,"1. Click the name of the target registration center on the hosted registration center list page. \n2. Click `Plug-in Center` in the left navigation bar, and click `Open Now` on the `Sentinel Management` card. \n3. Fill in various configuration information, and then click `OK` at the bottom of the pop-up box. \n4. If the prerequisites are met and the configuration is correct, a ""Sentinel plug-in enabled successfully"" message will pop up in the upper right corner of the page.",
What is Sentinel? What is its main function?,"Sentinel is a traffic management component for distributed, multi-language heterogeneous service architecture. It mainly takes traffic as the entry point, including traffic routing, traffic control, traffic shaping, circuit breaker degradation, system adaptive overload protection, hot spot traffic protection, etc. Multiple dimensions to help developers ensure the stability of microservices.",
What are the advantages of DCE 5.0's link tracing feature?,"The link tracking function of DCE 5.0 can quickly locate faults and view the link delay distribution of each microservice, which plays an important role in troubleshooting and performance optimization.",
Which module provides link query?,Link query is a function provided by the observability module.,
What role does link tracing play in the event of a failure?,"Link tracing can quickly locate the fault when a fault occurs, and understand the problem by viewing the link delay distribution of each microservice.",
"In DCE 5.0, how to check the link delay distribution of microservices?","In DCE 5.0, you can view the link delay distribution of each microservice through link query. For specific operations, please refer to [DCE Document](../../../insight/user-guide/data-query/trace.md).",
What type of link tracing does DCE 5.0 support?,DCE 5.0 supports service-level link tracing.,
What are the prerequisites for the link tracking function of DCE 5.0?,"The link tracing function of DCE 5.0 requires the user's application to use the OpenTracing standard for link tracing, and the insight-agent and insight-collector components need to be installed to collect and store link tracing data.",
What does link tracing do?,"Link tracking can help users track and visualize the calling relationships and delays between various microservices. When a failure occurs, it can help users quickly locate the fault point and solve the problem.",
How to check the link delay distribution of microservices?,"Users can use the link query function of DCE 5.0 to view the link delay distribution of microservices. For more details, please refer to [DCE 5.0 Observability-->Link Query](../../../../insight/user-guide/data-query/trace.md).",
"In a cluster, which component needs to be installed to use the monitoring function of DCE 5.0?",The insight-agent component needs to be installed in the cluster to use the monitoring function of DCE 5.0.,
What level of link tracking and query does DCE 5.0 support? Can faults be quickly located?,DCE 5.0 supports service-level link tracking and query. You can view the link delay distribution of each microservice and quickly locate the fault when a fault occurs.,
In what ways can grid services be governed?,"Grid services can be governed in three ways: virtual services, target rules, and gateway rules.",
"In the microservice engine, which module supports traffic management?","In the microservice engine, the [Service Grid] (../../../mspider/intro/what.md) module of DCE 5.0 is integrated to support traffic management.",
Which type of registry supports microservice governance capabilities?,Only Mesh type registries support the microservice governance function.,
Which version of the service mesh module is integrated into the microservice engine?,The service grid module of DCE 5.0 is integrated into the microservice engine.,
What is microservice governance?,"Microservice governance mainly refers to grid service traffic management based on Istio. Grid services can be governed in three ways: virtual services, target rules, and gateway rules.",
"How to update the basic configuration, policy configuration and security configuration of the API?","There are two ways to update the basic configuration, policy configuration and security configuration of the API:\n- Find the API that needs to be updated on the `API Management` page, click **`ⵗ`** on the right side of the API and select `Modify Basics' Configure`, `Modify policy configuration`, or `Modify security configuration`.\n- Click the API name to enter the API details page, and select `Modify basic information`, `Modify policy configuration` or `Modify security configuration' in the upper right corner of the page `.",
On which pages can the API update of the microservice gateway instance be performed?,You can update the API of the microservice gateway instance on the `API Management` page or click the API name to enter the API details page.,
What full life cycle management does the microservice gateway support?,"The microservice gateway supports full life cycle management of the API of the gateway instance, including adding, updating, and deleting APIs.",
How to update the API of a microservice gateway instance?,"You can update the basic configuration, policy configuration and security configuration of the API in two ways:\n- Find the API that needs to be updated on the `API Management` page, click **`ⵗ`** on the right side of the API and select `Modify Basics' Configure`, `Modify policy configuration` or `Modify security configuration`. \n- Click the API name to enter the API details page. In the upper right corner of the page, go to 'Modify basic configuration', 'Modify policy configuration' or 'Modify security configuration'.",
Which version of Microservice Engine Release Notes optimizes the configuration file structure?,v0.21.0 optimizes the configuration file structure.,
"In Microservice Engine Release Notes, which version adds custom permission points and API implementation?",v0.19.0 adds custom permission points and API implementation.,
"In the Microservice Engine Release Notes, which versions have added Sentinel-related functions?","v0.19.0, v0.17.1, v0.16.0, and v0.15.2 all have new Sentinel related functions.",
How to delete the hosted registration center and all data in it in the DCE 5.0 microservice engine?,"Select the registration center to be deleted on the hosted registration center list page, click **`⋯`** on the right and select `Delete`.",
What is the difference between an access registration center and a hosted registration center?,"The difference is:\n-Removal: It only removes the access-type registration center from the microservice engine of DCE 5.0. The original registration center and data will not be deleted. You can access the registration center again later. \n- Delete: Delete the hosted registration center and all data in it. The registration center cannot be used again in the future, and a new registration center needs to be re-created.",
How to remove the access registry?,"1. Select the registration center to be removed on the access registration center list page, click **`⋯`** on the right and select `Remove`. \n2. Enter the name of the registration center and click `Remove`.",
How to export the request log of the microservice gateway to the local?,"Support exporting log files to local. After filtering out the required request logs, click the `Export button` in the upper right corner of the page to export.",
What time range can be limited to request logs?,"You can select logs of the last 5 minutes, 10 minutes, 15 minutes, or customize the time range.",
What conditions can be used to filter request logs?,"Supports filtering logs by Request ID, request path, domain name, request method, HTTP, GRPC and other conditions.",
How to view the request log of the microservice gateway?,"Click the name of the target gateway to enter the gateway overview page, and then click `Log View`->`Request Log` in the left navigation bar.",
What microservice gateway functions can the microservice engine implement?,"The microservice engine can realize the visualization and high performance of microservice gateways, including adding interface operation capabilities based on Contour, realizing multi-gateway management, creating multiple gateway instances, and cascading functions of gateway routing.",
What two traffic management modes does the microservice engine support?,"The microservice engine supports two traffic management modes, Sentinel and Service Mesh, which are respectively suitable for east-west traffic management of traditional microservices and cloud-native microservices.",
Why do you need to use a microservices engine to manage a large number of heterogeneous microservices?,"In a large number of heterogeneous microservices, each microservice calls each other and relies on each other, affecting the whole system. It is easy for cascading effects to occur and cause a system avalanche. The use of microservice engines can simultaneously manage traditional microservices and cloud-native microservices, monitor service information, track service-level link calls, uniformly manage microservice configurations, and provide a one-stop product for microservice gateways, thereby reducing the difficulty of operation and maintenance. and cost.",
In what scenarios can the microservice engine be applied?,"The microservice engine can be used in scenarios such as microservice registration and discovery, configuration management, microservice traffic management, and microservice gateway management. Typical application scenarios are as follows:\n- One-stop management of a large number of heterogeneous microservices\n- Transformation and transition from traditional to cloud native\n- Visualization and high performance of microservice gateways",
What two dimensions of functionality does the microservice engine mainly provide?,The microservice engine mainly provides functions in two dimensions: microservice governance center and microservice gateway.,
What should you pay attention to when deleting a microservice namespace?,"All resources under the namespace must be cleaned up before deletion, otherwise they cannot be deleted. Click `Delete` under the right operation bar of the corresponding namespace, and then click `Delete Now` in the pop-up box.",
Can the microservice namespace be updated?,"Yes, click `Edit` under the right operation bar of the corresponding namespace, enter the update page and modify the name or description information.",
How to create a microservice namespace?,"Enter the `Microservice Engine` -> `Microservice Governance Center` -> `Host Registration Center` module, click the name of the target registration center, click `Microservice Namespace` in the left navigation bar, and then click `Create' in the upper right corner `, fill in the namespace ID, name and description information, and click OK in the lower right corner of the page.",
What is the default microservice namespace?,"When creating a hosted registry instance, the system automatically creates a default namespace named **public**. This namespace cannot be edited or deleted, and it is a reserved namespace that comes with the system.",
What is the role of microservice namespace?,"Microservice namespaces can be used to isolate services, configurations and other resources in different environments such as production, development, and testing. Services and configurations in different namespaces are strictly isolated and cannot reference each other.",
How to upgrade the DCE 5.0 microservice engine?,"There are two ways to upgrade: upgrading through helm repo and upgrading through chart package. Before upgrading, you need to back up the `--set` parameter of the old version. For specific operations, please refer to the step instructions in the document. It is recommended to add the microservice engine helm repository locally and update the helm repository before upgrading.",
How to synchronize the latest version of the image to the private image warehouse?,"You can use chart-syncer or Docker/containerd to synchronize images. To use chart-syncer to synchronize images, you need to create the `load-image.yaml` configuration file and execute the `charts-syncer sync --config load-image.yaml` command. If you use Docker/containerd to synchronize images, you need to decompress the `tar` compressed package and execute the `docker load -i images.tar` or `ctr image import images.tar` command on each node.",
How to upgrade the `skoala-init` component?,"A series of operations need to be performed in each working cluster, including backing up the original parameters, adding and updating the Helm warehouse of the microservice engine, and finally executing the `helm upgrade` command.",
How to upgrade `skoala` component?,"First, you need to perform a series of operations in the control plane cluster, including modifying the configuration, backing up the original data, adding and updating the Helm warehouse of the microservice engine, and finally executing the `helm upgrade` command.",
Why back up original data?,The original data is backed up so that it can be restored if an unexpected situation occurs during the upgrade process.,
What two components need to be upgraded when upgrading the microservices engine? Why?,"When upgrading the microservice engine, you need to upgrade the `skoala` component and the `skoala-init` component at the same time, because incompatible versions of the two components will cause problems.",
What two components does a microservice engine consist of?,The microservice engine consists of `skoala` component and `skoala-init` component.,
Which microservice governance scenarios are Mesh plug-ins mainly suitable for? What configurations do we need to complete when turning on the Mesh management plug-in?,"Mesh plug-ins are mainly suitable for cloud native microservice governance scenarios. We need to bind a grid instance, add microservices to the grid, and configure resources such as sidecars according to the requirements of the service grid. In addition, the Mesh plug-in provides governance rules such as virtual services, target rules, gateway rules, peer authentication, request identity authentication, and authorization policies.",
What governance rules does the Sentinel plugin support? What parameters do we need to set for the Sentinel instance?,"The Sentinel plug-in supports flow control rules, circuit breaker rules, hotspot rules, system rules and authorization rules. We need to set resource quotas for the Sentinel instance, select the deployment mode (single node/high availability) and access method.",
What two plug-ins should be paid attention to when enabling them on the registration center instance?,"The same registration center instance cannot enable both Sentinel management and Mesh management plug-ins at the same time, but they can be switched according to different scenarios.",
What two types of plug-ins are available in the plug-in center? Which microservice governance scenarios are they suitable for?,"The plug-in center provides two plug-ins: Sentinel management and Mesh management. The Sentinel plug-in is suitable for traditional microservice governance scenarios and supports multiple governance rules such as flow control rules, circuit breaker rules, hotspot rules, system rules, and authorization rules. The Mesh plug-in is suitable for cloud-native microservice governance scenarios and provides governance rules such as virtual services, target rules, gateway rules, peer authentication, request identity authentication, and authorization policies.",
What is new and fixed in version v0.12.2?,"The v0.12.2 version adds new functions such as adding Grafana template support for Sentinel's own monitoring, adding configuration information for custom configuration gateway indexes, etc., and fixes the status issues of microservice integrated observable components, pre-dependency check interface issues, etc. question.",
What is new and fixed in version v0.13.0?,The v0.13.0 version adds the docking middleware MyS,
What new features and issues are added and fixed in version v0.14.0?,"The v0.14.0 version adds offline support for the image required by Init Chart, obtains tokens for hosting Nacos, and other functions, and fixes many problems such as the naming problem of Values in Skoala Chart and the mirroring problem in the CI process.",
What is new and fixed in version v0.15.2?,"The v0.15.2 version adds new functions such as gateway API support for authentication servers, hosting registration center service access API, Sentinel cluster flow control related API and other functions. It also fixes the problem of Sentinel rule splicing errors, Nacos controller processing logic problems, etc. a question.",
What new features and issues are added and fixed in version v0.16.0?,"The v0.16.0 version adds a new token for obtaining hosted Nacos, optimizes the Grafana monitoring panel for adding Sentinel services, fixes the problem of Sentinel calling the Nacos interface with authentication, and the problem of nacos-operator frequently modifying service resources, etc. question.",
What issues are fixed in version v0.16.1?,The v0.16.1 version fixes the problem of repeatedly creating builders when building images.,
What content has been optimized in this update?,"This update optimizes the upgrade of gateway-api to v0.6.0, the independent CI step of offline chart construction, and the upgrade of Contour to v1.24.1.",
What issues are fixed in this update?,"This update fixes many problems such as Nacos Namespace creation exception, Nacos persistent storage modification exception, gateway monitoring panel data display problem, etc.",
What new features are added in this update?,"This update adds new APIs related to Sentinel statistics, a CI process that supports chart offline, adds external image security scanning capabilities in daily builds, and releases automatic updates to image versions in charts.",
What is Token transparent transmission? Is it enabled by default?,Token transparent transmission refers to whether to send JWT Token information to the back-end service. It is not enabled by default and needs to be enabled manually in the security policy of the domain name.,
"What are the JWKS name, server address, Issuer and Audiences respectively?","- JWKS name: The unique JWKS name, used to identify the specific JWT policy\n - JWKS server address: The complete F of the JWT service that returns the JWKS content",
How to create a JWKS application?,"First download the JWKS generator code to your local computer, and then run the JWKS generator. Visit http://localhost:8080 and fill in the relevant information to generate JWKS content. Next visit https://jwt.io to generate Token. Finally, create a YAML file based on the YAML template and use the kubectl apply command to install the JWKS application.",
How to enable JWT authentication in microservice engine?,"First, you need to create a domain name with the protocol https and enable JWT authentication in the security policy of the domain name. Then create the API and enable the JWT authentication security policy. Finally, bring the Token for access verification. Success indicates that the JWT policy configuration is successful.",
Which authentication method does the microservice engine gateway support?,Microservice engine gateway supports JWT authentication.,
What can be viewed in the monitoring information?,"Monitoring information allows you to view the monitoring information of microservices, including number of requests, error rate, response time, request rate, etc. Supports custom time range.",
What information can be viewed in the instance list?,"In the instance list, you can view the instance status, IP address, service port, etc. Click the instance name to further view the monitoring information and metadata of the instance.",
What type of registry is recommended to create to perform more advanced operations?,It is recommended to create a hosted registry to perform more advanced operations.,
What management operations does the access registration center support?,"The access registration center supports basic management operations, including viewing the microservice list and basic information, viewing the microservice instance list and interface list, viewing microservice monitoring information, etc.",
What is the role of microservice management?,"Microservice management mainly refers to the management of microservices through the registration center, including viewing the microservice list and basic information, viewing the microservice instance list and interface list, viewing the monitoring information of the microservice, etc. Its function is to facilitate users to understand the running status of microservices and perform corresponding management operations.",
What operations need to be done before deleting a service?,"Before deleting a service, you need to make sure that no API is using the service. If the service is being used by an API, you need to follow the page prompts and click `API Management` to delete the associated API before you can delete the service.",
How to update basic information?,"Find the service that needs to be updated on the `Service List` page, click **`ⵗ`** on the right side of the service, select `Basic Information`, and click OK after updating the basic information.",
What information do I need to fill in when adding a cluster service?,"You need to select the cluster and namespace where the target service is located, and fill in the access protocol, address and port.",
What are the prerequisites for manually accessing the service?,You need to add the corresponding service source in source management in advance before you can select the corresponding service source type when manually accessing the service.,
What two methods of adding services does the microservice gateway support?,The microservice gateway supports adding services through manual access and automatic discovery.,
How to confirm whether the Mesh plug-in is successfully enabled?,"If the prerequisites are met and the configuration is correct, a ""Mesh plug-in enabled successfully"" message will pop up in the upper right corner of the page.",
What should I do if I can’t find the service mesh I’m looking for?,You can go to the service mesh module [Create a mesh](../../../../mspider/user-guide/service-mesh/README.md).,
How to bind the service mesh to the Mesh governance plug-in?,"After enabling the Mesh management plug-in, select the service mesh you want to bind in the pop-up box, and then click OK at the bottom.",
How to enter the plug-in center of DCE 5.0?,"Click the name of the target registration center on the hosted registration center list page, and then click `Plug-in Center` in the left navigation bar.",
How to enable the Mesh management plug-in of DCE 5.0?,"1. Click the name of the target registration center on the hosting registration center list page to enter the plug-in center. \n2. Click `Plug-in Center` in the left navigation bar, and click `Open Now` on the `Mesh Management` card. \n3. Select the service mesh you want to bind, and then click OK at the bottom of the pop-up box. \n4. If the prerequisites are met and the configuration is correct, a ""Mesh plug-in enabled successfully"" message will pop up in the upper right corner of the page.",
How to further view or share microservice monitoring information?,Click the dashboard name to further view or share the monitoring information. The statistical time window and refresh cycle can be switched in the upper right corner of the page.,
What are the steps to view microservice monitoring?,"Enter the microservice engine module, click on the name of the target registration center, then click ""Monitoring"" -> ""Microservice Monitoring"" in the left navigation bar, select the corresponding microservice namespace to view the microservice monitoring data under that namespace .",
What components need to be installed to use the monitoring function of the microservice engine?,The insight-agent component needs to be installed in the cluster to use the monitoring function of the microservice engine.,
What monitoring information does microservice monitoring provide?,"Microservice monitoring can provide monitoring information for each microservice under the hosting registration center, including service response time, concurrency, and exceptions.",
What is Skoala-init? What does it do?,"Skoala-init is part of Skoala and includes some components, which are packaged and installed as separate Charts. Its role is to initialize the services required by Skoala.",
What prerequisites are required for DME?,1. Deploy the Kubernetes cluster. \n2. Install Helm. \n3. Add the specified Helm repository.,
How to uninstall DME?,Use the following command to uninstall:\n```\nhelm uninstall skoala-init -n skoala-system\nhelm uninstall skoala -n skoala-system\n```,
How to update DME?,"Supports offline upgrade and online upgrade. For details, please refer to offline upgrade or online upgrade.",
How to install DME?,1. Deploy Kubernetes cluster\n2. Install Helm\n3. Add specified Helm repository\n4. Customize and initialize database parameters\n5. Install skoala-init to the working cluster\n6. Install DME,
How to uninstall DME?,Use the helm uninstall command to uninstall;\nhelm uninstall skoala-init -n skoala-system\nhelm uninstall skoala -n skoala-system,
How to install (update) skoala-init to a working cluster?,Execute the helm command to install (update);\nhelm upgrade --install skoala-init --create-namespace -n skoala-system --cleanup-on-fail \\nskoala-release/skoala-init \\n-- version 0.13.0,
How to perform deployment (upgrade) DME?,"Just execute the helm command directly, pay attention to the corresponding version number and customized parameters;\nhelm upgrade --install skoala --create-namespace -n skoala-system --cleanup-on-fail \\n--set ui. image.tag=v0.9.0 \\n--set sweet.enable=true \\n--set hive.configMap.data.database.host= \\n--set hive.configMap.data.database.port= \\n--set hive.configMap.data.database.user= \\n--set hive.configMap.data.database.password= \\n--set hive.configMap.data.database.database= \\ nskoala-release/skoala \\n--version 0.13.0",
How to check the latest version of DME?,"In the global management cluster, obtain the version information through the helm command;\nhelm search repo skoala-release/skoala --versions\nIn the working cluster, obtain the latest through helm repo update;\nhelm search repo skoala-release/skoala-init - -versions",
What components does Skoala-init contain?,"Skoala-init includes components: skoala-agent, nacos, contour, sentinel.",
Why do you need to configure skoala helm repo?,Configuring the skoala helm repo can obtain the skoala application chart to facilitate deployment and upgrade of DME.,
How to install the `insight-agent` component?,"You can refer to the instructions in the document for installation. Specific steps include: adding the Helm repository, installing the `insight-agent` component, checking whether the component is running normally, etc.",
What dependent components need to be detected during microservice engine deployment?,"When installing the microservice engine, you need to use the `common-mysql` component to store configurations, so make sure that this component already exists. In addition, you also need to check whether there is a database named `skoala` in the `common-mysql` namespace. If you need to monitor various indicators of microservices and track links, you also need to install the `insight-agent` component. Please refer to the documentation for specific details.",
How to detect whether the microservice engine has been installed?,"You can check whether there are corresponding resources in the `skoala-system` namespace, such as: Pod, Deployment, StatefulSet, Service, etc. Please refer to the documentation for specific methods.",
What is the microservice engine deployment structure?,"The microservice engine consists of two components. The `skoala` component needs to be installed in the control plane cluster, and the `skoala-init` component needs to be installed in the working cluster. Please refer to the documentation for details.",
How to install the components of the microservice engine?,"It is recommended to use the commercial version of the installation package for installation. If you need to install it manually online separately, please note that both components `skoala` and `skoala-init` need to be installed. Please refer to the documentation for the specific process.",
What configuration information needs to be filled in to access the registration center?,"Accessing different types of registration centers requires filling in different configuration information. For the Kubernetes/Mesh registration center, just select the cluster or grid service you want to access. For the Nacos/Zookeeper/Eureka registration center, you need to fill in the name and address of the registration center, and you can click ""+ Add"" to enter multiple addresses.",
What is the governance status of microservices?,"The governance status of a microservice refers to determining whether the microservice meets the governance conditions, such as whether the governance plug-in is enabled, whether it is containerized, whether it is managed by the grid, whether it is created through the application workbench, and a series of other requirements.",
What is a protection threshold? What does it do?,"The protection threshold is a floating point number between 0 and 1, which represents the minimum proportion of healthy instances in the cluster. If the proportion of actual healthy instances is less than or equal to the threshold, threshold protection will be triggered, that is, the instance will be returned to the client regardless of whether it is healthy or not. Threshold protection is mainly to prevent all traffic from flowing into the remaining instances when there are too many failed instances, causing traffic pressure to crush the remaining instances and form an avalanche effect.",
What is the grouping of microservices?,The grouping of microservices refers to the grouping of configuration files of microservices. The purpose is to separate configuration files and microservice instances from each other so that the same configuration can be applied to different services or components.,
What information does the microservices list page provide?,"The microservice list page allows you to view information such as microservice grouping, health status, protection thresholds, request status, link tracking, and governance status. After clicking on the microservice name, you can also view the microservice instance list, monitoring information, interface list, metadata, etc.",
What configurations need to be filled in?,"Please pay attention to filling in the following configuration:\n- Enable credentials. \n- Allowed request methods. \n- Allowed request sources. \n- Preflight duration. \n- Allowed request headers. After adding keywords, you need to add the corresponding keywords to the request header to access the target service normally. \n- Exposed request headers. Controls the exposed request header keywords. You can configure multiple items.",
How to configure a cross-domain policy for a domain name?,"You need to fill in the specific configuration items of the cross-domain policy. Specific details include: \n- Enable credentials: When enabled, credential checks need to be performed on cross-domain requests. After the check passes, cross-domain requests can be processed. \n- Allowed request methods: Select the request method of HTTP protocol. \n- Allowed request sources: Limit multiple specific request sources, usually using IP. \n- Precheck duration: The time it takes to check credentials, request methods, and other matters before processing cross-domain requests. The time unit is seconds, minutes, and hours. \n- Allowed request headers: Restrict specific HTTP request header keywords. After adding keywords, you need to add the corresponding keywords to the request header to access the target service normally. \n- Exposed request headers: Control the exposed request header keywords, multiple items can be configured. For detailed instructions, please refer to the ""Configuring Domain Name Policy"" chapter in ""DCE 5.0 Microservice Engine Documentation"".",
What is cross-domain? What are the functions and effects of cross-domain?,"Cross-domain refers to initiating HTTP requests between different sources (different protocols, ports, and hosts) on the same page. For security reasons, browsers block cross-domain requests. In order to implement cross-domain requests, cross-domain technology can be used. Spanning technology can achieve the following functions and effects:\n- Share resources between different sources. \n- Implement asynchronous data exchange between client and server. \n- Enables web applications to communicate with each other.",
"If the API-level and domain-level current limiting policies conflict, which one takes precedence?","If the current limiting policy at the API level conflicts with the current limiting policy at the domain name level, the current limiting policy at the API level shall prevail.",
How to configure local traffic limiting for a domain name?,"You can set local traffic limit during the domain name creation process, or you can adjust it by modifying the domain name after the domain name is created. After configuring local traffic limiting, the configuration will be automatically applied to all APIs using this domain name. For specific local current limiting configuration instructions, please refer to the ""Local Current Limiting"" chapter in ""DCE 5.0 Microservice Engine Documentation"".",
What options are available for flow control effects?,"The flow control effect has three options: fast fail, warm up, and queue.",
How to view the created flow control rules?,Select `Flow Control Rules` on the governance page to view the list of created flow control rules.,
What content needs to be filled in the flow control rules?,"The content that needs to be filled in includes resource name, source application, direct mode/associated mode/link mode, etc.",
How to create flow control rules?,"The method of creating flow control rules is as follows:\n1. Select the microservice to be managed in the target hosting registration center and enter the management page of the service. \n2. Select `Flow Control Rules`, and then click `Create Flow Control Rules` on the right side. \n3. Fill in the rule configuration, including resource name, source application, direct mode/associated mode/link mode, etc., and click OK in the lower right corner. \n4. After creation, you can view the newly created rules in the flow control rule list. Click the More button on the right to edit the update rule or delete the rule.",
What are flow control rules?,Flow control rules monitor service traffic,
What are authorization rules? How to set authorization rules?,"Authorization rules are used to restrict requests from specific sources and can be set up as a whitelist or blacklist. In DCE 5.0, authorization rules can be set in the Sentinel plug-in's console. \n<!--",
What are system rules? What threshold types are supported by system rules?,"System rules refer to system capacity, CPU usage, average response time and entrance",
What are hotspot rules? How to set hotspot rules?,"Hotspots refer to frequently accessed data. Hotspot rules are used to count frequently accessed resources and restrict access to the resources after reaching a certain threshold. Parameters such as parameter index and parameter exceptions can be configured. In DCE 5.0, hotspot rules can be set in the Sentinel plug-in's console.",
What are circuit breaker rules? How to set up circuit breaker rules?,"circuit breaker rule is a flow control rule used to protect the system from fluctuations in traffic. You can set parameters such as the minimum number of requests, statistical duration, and interruption duration. In DCE 5.0, circuit breaker rules can be set in the Sentinel plug-in's console.",
On what basis do authorization rules allow traffic management? What types of authorization are there?,"Authorization rules allow traffic management based on the request source, such as allowing only requests initiated by callers in the whitelist. Authorization types include whitelist and blacklist.",
From what dimensions do system rules automatically select flow control rules to control request traffic? What types of thresholds are there?,"System rules automatically select flow control rules to control request traffic from an overall perspective, including system capacity, CPU usage, average response time, entrance",
What types of resources do hotspot rules target? How to set hotspot parameters and thresholds?,"Hotspot rules can limit the flow of data resources that are frequently accessed by setting hotspot parameters (that is, the target parameters that need to count the number of visits) and thresholds. Thresholds can be set individually for specified parameter values, and default to 0 if additional parameters do not match.",
What is the purpose of circuit breaker downgrade? What are the circuit breaker strategies?,"The purpose of circuit breaker downgrade is to protect the overall availability of the system and avoid the cascading effect caused by the instability of the called service, which will lengthen the caller's own response time, cause thread accumulation, and even cause the service to become unavailable. The circuit breaker strategy includes slow call ratio, exception ratio and number of exceptions.",
What are flow control rules? What flow control modes and threshold types are there?,Flow control rules are used in Sentinel governance rules to monitor and control application or service traffic.,
What are Sentinel governance rules?,"Sentinel governance rules refer to a set of rules for traffic management of the service grid or Sentinel in the microservice engine, including flow control rules, circuit breaker degradation, hotspot rules, system rules and authorization rules.",
How to enable automatic refresh to view real-time logs?,"When viewing logs, just turn on the ""Auto Refresh"" option.",
What time range can I choose for instance logs?,"The log time range can be selected: logs in the past 5 minutes, logs in the past 1 hour, 12 hours, 7 days, or a custom time range.",
What methods of filtering instance logs are supported?,Supports filtering instances to only view the logs of a certain container group. You can also refer to K,
How to view the instance logs of the microservice gateway?,"Click the name of the target gateway to enter the gateway overview page, then click `Log View`->`Instance Log` in the left navigation bar.",
How to delete the API of a microservice gateway instance in the details page?,"Click the API name that needs to be deleted to enter the API details page, click ""ⵗ"" in the upper right corner of the page and select ""Remove"".",
How to delete the API of a microservice gateway instance in the list page?,"Find the API that needs to be deleted on the list page, click ""ⵗ"" on the right side of the API and select ""Remove"".",
What full lifecycle management operations does a microservice gateway instance support?,"Microservice gateway instances support full life cycle management operations such as adding, updating, and removing APIs.",
Is deleting the API of a microservice gateway instance reversible?,"irreversible. Regardless of whether the removed API is online or not, once deleted it becomes invalid immediately and cannot be restored.",
How to remove the API of a microservice gateway instance?,"You can delete an API by clicking the delete icon next to the API on the API management page or API details page and selecting ""Remove"". However, once the deletion operation is completed, it is irreversible.",
What operations need to be performed before deleting a domain name that is in use?,You need to delete the relevant API first before you can delete the domain name in use.,
How to delete a domain name on the domain name management page?,"Find the domain name that needs to be deleted on the `Domain Name Management` page, click **`ⵗ`** and select `Delete`.",
Can a domain name be restored after deletion?,Deleting a domain name cannot be restored.,
On which pages can I delete a domain name?,Domain names can be deleted on the `Domain Name Management` page or the domain name details page.,
Under what circumstances cannot a domain name be deleted?,The domain name being used by the API cannot be deleted. You need to delete the related API first and then delete the domain name.,
Is it possible to recover after deleting the microservice gateway?,"Deleting the microservice gateway cannot be restored, so please operate with caution.",
What are the two ways to delete a microservice gateway?,"The first way is to select the instance that needs to be removed on the microservice gateway list page, click **`⋯`** on the right side of the instance and select `Delete`; the second way is to click the gateway name to enter the overview page, and then **`⋯`** in the upper right corner and select `Delete`.",
What should you pay attention to when deleting a microservice gateway?,"Before deleting the microservice gateway, you need to release all routing APIs to ensure that the service is not affected. In addition, the gateway cannot be restored after deletion, so please operate with caution.",
What is the difference between remove and delete?,"Removal: It only removes the registration center from the microservice engine of DCE 5.0. The original registration center and data will not be deleted. The registration center can be accessed again later. \n- Delete: Delete the registration center and all data in it. The registration center cannot be used again in the future, and a new registration center needs to be re-created.",
How to delete the registration center?,"- On the `Hosted Registration Center List` page, select the registration center to be deleted, click **`⋯`** on the right and select `Delete`. \n- Enter the name of the registration center and click `Remove` (access type) or `Delete` (hosted type).",
What are the two methods of deleting the registration center?,"Remove and delete respectively. Access-type registration centers only support removal operations, and hosted registration centers only support deletion operations.",
How to view node metadata?,"On the basic information page, you can select the target node in the node list and click ""View Metadata"" in the ""Operation"" column to view it.",
How to bring a node online/offline?,Select the target node in the node list and click "Offline" in the "Operation" column to modify the node instance status to "Offline"; click "Online" in the "Operation" column to modify the node instance status to "Online".,
How to restart the entire hosted registry instance?,Click "Restart" in the upper right corner of the page to restart the entire hosting registration center instance.,
What does the running status reflect?,The running status is part of the registration center instance information and is used to reflect the status of the registration center instance.,
How to view the details of the registration center?,"Find the registration center instance whose details you want to view on the managed registration center list page, and click the instance name to enter the basic information page. On the basic information page, you can view the basic information of the registration center, service governance information (the service governance function needs to be turned on), node list, data persistence information, etc.",
"When viewing monitoring information of a Nacos instance, how to view statistical details?",Pull down the page to view statistical details such as request time and number of requests.,
What dimensions of information does component monitoring provide?,"Component monitoring provides information in multiple dimensions, including the number of nodes, number of services, CPU/memory usage, number of JVM threads, total http request time, etc.",
How to view component monitoring?,"The steps to view component monitoring are as follows:\n1. Enter the microservice engine module and click on the name of the target registration center. \n2. Click `Monitoring`->`Component Monitoring` in the left navigation bar. \n3. Click the `Naocs Instance` tab to view the monitoring information of the Nacos instance. At the same time, the statistical time window and refresh cycle can be switched in the upper right corner of the page.",
What monitoring capabilities does the microservice engine provide?,"The microservice engine provides a full range of monitoring functions, covering monitoring objects such as individual microservices, system components, and service call links. Among them, the component monitoring function can provide monitoring information for Nacos and Sentinel.",
What is the insight-agent component?,The insight-agent component is a component used to provide monitoring functions in the cluster. It needs to be installed before the monitoring function can be used.,
How to update or delete microservice configuration?,"1. Enter the `Microservice Engine` -> `Microservice Governance Center` -> `Host Registration Center` module and click on the name of the target registration center. \n2. Click `Microservice Configuration List` in the left navigation bar, click the `ⵗ` button on the right side of the target configuration and select update or delete as needed.",
How to roll back the historical version of microservice configuration?,"1. Enter the `Microservice Engine` -> `Microservice Governance Center` -> `Host Registration Center` module and click on the name of the target registration center. \n2. Click `Microservice Configuration List` in the left navigation bar and click the Data ID of the target configuration. \n3. Click the `Historical Versions` tab, find the corresponding record, click `ⵗ` on the right side of the record and select `Rollback`. \n4. Compare the version differences and confirm, then click `Rollback` in the lower right corner.",
How to view microservice configuration?,"1. Enter the `Microservice Engine` -> `Microservice Governance Center` -> `Host Registration Center` module and click on the name of the target registration center. \n2. Click `Microservice Configuration List` in the left navigation bar and click the Data ID of the target configuration. \n3. View the basic information, configuration content, historical versions, listeners, sample codes and other information of the configuration.",
How to create a new microservice configuration?,"1. Enter the `Microservice Engine` -> `Microservice Governance Center` -> `Host Registration Center` module and click on the name of the target registration center. \n2. Click `Microservice Configuration List` in the left navigation bar, and then click `Create` in the upper right corner. \n3. Fill in various configuration information, including namespace, Data ID, Group, configuration format, configuration content, etc. \n4. Click `OK` in the lower right corner of the page.",
What core management capabilities does the microservice configuration list support?,"The microservice configuration list supports core management capabilities such as addition, deletion, modification and query of microservice configuration, viewing historical versions, rollback, and subscriber query.",
Which official SDK version or above can be used to display Sentinel monitoring data normally?,You need to use Sentinel official SDK v1.8.6 or above.,
How do I get the Sentinel console?,"You can download the latest version of the console jar package from the Sentinel official website, or build the Sentinel console yourself from the latest version of the source code. For details, please refer to the official documentation: [Start Console](https://sentinelguard.io/zh-cn/docs/dashboard.html).",
What happens if the project.name parameter does not conform to the specification?,"If the specification is not met, all governance rules will be pushed to the SENTINEL_GROUP configuration center under the public namespace.",
"When passing application parameters, what should be the format of the project.name parameter?","The format of the project.name parameter should be: `{{nacos_namespace_id}}@@{{nacos_group}}@@{{appName}}`, where the first part `{{nacos_namespace_id}}` refers to the ID of the Nacos namespace, rather than the name of the namespace.",
"In order to use Sentinel traffic management and view Sentinel data monitoring normally, which console does the application need to be connected to?",Sentinel console.,
What are the required fields when creating a microservices gateway?,"When creating a microservice gateway, you must fill in the following basic configuration: gateway name, deployment cluster, namespace (deployment), namespace (governance) and service entry method. Among them, the gateway name cannot exceed 63 characters in length and supports letters, numbers, and underscores. The name cannot be changed after the gateway is created.",
Does the microservice gateway support high-availability architecture?,Yes. The microservice gateway supports a high-availability architecture with multi-tenant instances.,
How to configure the number of gateway front proxy layers?,"In the advanced configuration, filling in the request needs to pass through several proxy endpoints from the client to the gateway. For example, the number of proxy layers of `Client-Nginx-Gateway` is 1, because there is only one Nginx proxy endpoint in the middle.",
Can multiple microservice gateways be deployed simultaneously in a namespace?,no. Only one microservice gateway can be deployed in a namespace.,
What entry methods does the microservice gateway support?,"Microservice gateway supports the following three entry methods: intra-cluster access, node port, and cloud service provider's load balancer. Among them, intra-cluster access can only access services within the same cluster; node ports can access services through the node's IP and static ports, supporting access to services from outside the cluster; the cloud service provider's load balancer can make services publicly accessible.",
How to set the health check address?,"In the DCE 5.0 microservice gateway, you can set the health check address to ensure that when the backend service is abnormal, the gateway automatically adjusts the load balancing. It is necessary to set parameters such as the health check path, specific health check host, check interval, check timeout, number of marked health checks, and number of marked unhealthy checks.",
How to enable local current limiting in DCE 5.0 microservice gateway?,"In DCE 5.0 microservice gateway, local current limiting capability is enabled at the API level. You can set the request rate, allowed overflow rate, limit return code and Header keyword, etc.",
Which protocol does the DCE 5.0 microservice gateway support for accessing API backend services?,DCE 5.0 microservice gateway supports access to API backend services through the Websocket protocol.,
How to rewrite request headers/response headers in DCE 5.0 microservice gateway?,"In the DCE 5.0 microservice gateway, it can support adding, modifying, and deleting request headers, response headers, and their corresponding values.",
How to configure the retry mechanism in DCE 5.0 microservice gateway?,"In the DCE 5.0 microservice gateway, after the retry mechanism is enabled, the gateway will automatically retry access when the request fails. Supports selecting different retry conditions through custom configuration, customizing retry status codes, etc. Supports two retry mechanisms, HTTP and GRPC.",
How to set timeout configuration?,"Set the maximum time for request response. If the maximum time is exceeded, the request will fail directly. In the DCE 5.0 microservice gateway, it is turned off by default, and the timeout must be configured after it is turned on. The timeout duration supports integer values of numerical type >=1, and the time unit is ""seconds (s)"".",
How to enable path rewriting in DCE 5.0 microservice gateway?,"In the DCE 5.0 microservice gateway, you can enable path rewriting and forward external request traffic to the rewritten path. You need to ensure that the rewritten path actually exists and that the path is correct and starts with ""/"".",
What load balancing strategies does DCE 5.0 microservice gateway support?,"DCE 5.0 microservice gateway supports four load balancing strategies: random, polling, weight, cookie and request hash.",
How to configure API policy?,"You can set the policy during the creation of the API, or adjust it by updating the API policy configuration after the API is created.",
How many API strategies does DCE 5.0 microservice gateway support?,DCE 5.0 microservice gateway supports nine API strategies.,
What is the format of the authentication server address and how to specify it?,"The authentication server address format is `namespace/name`, which can be specified when filling in the security configuration of the domain name. The `namespace/name` of the authentication server refers to [all-in-one-contour.yaml](https://github.com/projectsesame/envoy-authz-java/blob/main/all-in-one-contour The values of the `namespace` and `name` fields in the `metadata` section under the ExtensionService in the .yaml) file.",
What resources need to be created to access the authentication server?,"It is necessary to create the Deployment, Service and ExtensionService of the authentication server.",
How to write custom authentication logic?,"Custom authentication logic needs to be written in the check method, which is in envoy-authz-java/authz-grpc-server/src/main/java/envoy/projectsesame/io/authzgrpcserver/AuthzService.java.",
"When using the default authentication server, which image should be used?",The default images used are release.daocloud.io/skoala/demo/envoy-authz-java:0.1.0 and release-ci.daocloud.io/skoala/demo/envoy-authz-java:0.1.0.,
What are the prerequisites for the microservice gateway to support access to a third-party authentication server?,"The prerequisite is to create a cluster or connect to a cluster, and create a gateway.",
What operations does the automatically managed service support?,"For automatically managed services, only operations such as viewing and configuring policies are supported, and operations such as updating and deletion are not supported.",
How to view the service details of automatic management?,"Find the target service on the Service List->Automatic Management page and click the service name to view the service name, source, associated API and other information.",
How to configure service policy?,"Find the target service on the Service List->Automatic Management page, click ""ⵗ"" on the right, and select policy configuration. You can adjust configurations such as HTTPS certificate verification and service circuit breaker as needed, and click OK in the lower right corner of the pop-up box.",
How to automatically manage services?,"First, you need to create a gateway instance. After successful creation, the services in the service source will be automatically added to the service list of the gateway instance. You can click Automatic Management on the service list page to view it.",
What operations can be performed through the gateway details page?,"You can update the gateway configuration, delete the gateway, diagnose the gateway, etc. through the gateway details page.",
In what order are the Top 10 APIs arranged?,"Top 10 APIs are the top 10 APIs arranged from high to low according to the number of API response codes 2xx, 4xx, and 5xx. By default, they are arranged in descending order according to the number of response codes 200.",
What do the numbers on the left and the right of the number of control/worker node instances mean?,"The number on the left of the number of control/worker node instances indicates the number of instances currently online, and the number on the right indicates the total number of node instances.",
How to enter the gateway details page?,"On the gateway list page, select the name of the target gateway to enter the gateway overview page.",
What information can be viewed on the gateway details page?,"On the gateway details page, you can view basic information, network information, TOP10 popular API status, resource status, resource load, plug-in information, etc.",
In what ways can governance rules be created for microservices?,"Three governance rules, including virtual services, target rules, and gateway rules, can be created for microservices through YAML files or page forms.",
In what form is microservice metadata displayed?,"Microservice metadata is stored in the Key-Value data structure and displayed on the page in the format of k1=v1,k2=v2.",
How to manually enter the microservice API?,Click the Create button in the interface list on the microservice details page to manually enter the API.,
What indicators can be monitored with microservice monitoring?,"Microservice monitoring can monitor the number of requests, error rate, response time, request rate and other indicators.",
How to view subscriber information of a microservice?,Enter the microservice details page and click the Subscribers tab to view the subscriber list and information.,
How to bring a microservice instance online or offline?,"In the instance list on the microservice details page, click the online or offline button in the operation column.",
How to check the service traffic weight of microservices?,Enter the microservice details page and click the edit button in the instance list to view and adjust the weight of the instance.,
How to launch API?,"After the API is successfully created, it will be offline by default and cannot be accessed at this time. The API needs to be adjusted to go online before it can be accessed normally. There are two ways to bring API online:\n- Find the API that needs to be updated on the `API Management` page, click **`ⵗ`** on the right side of the API and select `API Online`. \n- Click the API name to enter the API details page, click **`ⵗ`** in the upper right corner of the page and select `API online`.",
What API strategies are supported?,"Supports 11 API strategies: load balancing, path rewriting, timeout configuration, retry mechanism, request header rewriting, response header rewriting, Websocket, local current limiting, health check, cookie rewriting, access black and white list. For configuration instructions of each policy, please refer to [API Policy Configuration] (api-policy.md).",
What should you pay attention to when filling in basic configuration information?,"Please note the following when filling in the basic configuration information: \n- API name: contains lowercase letters, numbers and special characters (- .), and cannot start or end with special characters. \n- API Group: Select the name of the group to which the API belongs. If you enter a group name that does not exist, a new group will be automatically created. \n- Associated domain name: After filling in the associated domain name, you can access the API through `domain name + port number`. If the domain name cannot be found, you can add a new domain name. Please refer to [Add Domain Name](../domain/add-domain.md). \n- Matching rule: Only requests that match this rule are allowed to pass. If multiple rules are set, all rules must be met at the same time to be released. If a request header is added, you need to add the corresponding request header when accessing the API. \n- Request method: Select the request method of HTTP protocol. For detailed descriptions of various request methods, please refer to the W3C's official document [Method Definitions] (https://www.rfc-editor.org/rfc/rfc9110.html#name-method-definitions). \n- Target service: Choose to send the request directly to the backend service, redirect to another service, or return the HTTP status code directly. \n- If you select a backend service, you need to configure the weight. The greater the weight, the more traffic the gateway distributes to it.",
How to add API?,"The steps are as follows:\n1. Click the gateway name to enter the gateway overview page, then click `API Management` in the left navigation bar, and click `Add API` in the upper right corner of the page. \n2. Refer to the instructions in the document to fill in the basic configuration and policy configuration (optional). \n3. Click `Save` in the lower right corner of the page (not online). If you click `Save and go online`, you can go online directly to the API. \n4. The API needs to be adjusted to `online` before it can be accessed normally. There are two ways to go online with the API.",
What do I need to provide to add an HTTPS domain name?,"The corresponding HTTPS certificate is required. Currently, only the selection of existing certificates is supported. The functions of automatically issuing certificates and manually uploading certificates are under development.",
How to check the added domain name?,Enter the gateway overview page and click Domain Name Management in the left navigation bar to view all domain names that have been added.,
Can I modify the added domain name?,"No, once added successfully it cannot be modified.",
What options are available when filling in domain name configuration information?,-Basic information: including required domain name and protocol (HTTP or HTTPS). \n- Policy configuration: Options include local traffic limiting and cross-domain settings.,
How to add a domain name?,"1. Enter the overview page of the target gateway, click `Domain Name Management` in the left navigation bar, and then click `Add Domain Name` in the upper right corner. \n2. Fill in the basic information and policy configuration (optional), including domain name, protocol, local traffic limit and cross-domain. \n3. Click `OK` in the lower right corner of the page to complete adding the domain name.",
How to use "DaoCloud Daoke" microservice engine to implement microservice governance?,"Users can perform registration center configuration, traffic control, API management and other operations in the visual interface provided by ""DaoCloud Daoke"" to achieve management of microservice governance. At the same time, more customized and refined functions can be achieved by connecting to modules such as multi-cloud orchestration, data service middleware, and application workbench.",
What open source microservice frameworks and gateway components does the "DaoCloud Daoke" microservice engine support?,"The ""DaoCloud Daoke"" microservice engine supports mainstream open source microservice frameworks such as Spring Cloud and Dubbo, as well as open source gateway components such as Envoy, Sentinel, and Contour.",
How does the "DaoCloud Daoke" microservice engine achieve a smooth transition?,"The ""DaoCloud Daoke"" microservice engine supports unified management of traditional microservices and cloud-native microservices, achieving a smooth transition from the traditional microservice ecosystem to the cloud-native microservice ecosystem.",
What registration center types does the "DaoCloud Daoke" microservice engine support?,"The ""DaoCloud Daoke"" microservice engine supports traditional registration centers (Zookeeper, Eureka, Nacos) and cloud native registration centers (Kubernetes, Service Mesh).",
How does the "DaoCloud Daoke" microservice engine achieve imperceptible migration?,"The ""DaoCloud Dao Ke"" microservice engine is fully compatible with the Nacos open source registration center and Envoy and contour open source gateways, and supports the migration from self-built services to the ""DaoCloud Dao Ke"" microservice engine under the premise of zero code modification. Traditional microservices can be connected to the microservice engine through the registration center without any changes, thereby realizing functions such as traffic management, configuration management, link tracking, and indicator monitoring.",
What functions does the "DaoCloud Daoke" microservice engine provide?,"The ""DaoCloud Daoke"" microservice engine provides one-stop governance functions such as service registration discovery, configuration management, traffic management, link tracking, indicator monitoring, gateway management, API management, domain name management, monitoring alarms and gateway policies.",
What does the monitoring information of the microservice gateway include?,"The monitoring information of the microservice gateway includes resource usage, Envoy monitoring details, service monitoring details, APIServe, HTTPProxy and other resource running information.",
How to view resource monitoring information in microservice gateway?,"Enter the microservice engine module, click Microservice Gateway in the left navigation bar, and click the name of the target gateway. Click `Monitoring Alarms` in the left navigation bar, and click the `Resource View Version` tab to view resource monitoring information.",
How to view business monitoring information in microservice gateway?,"Enter the microservice engine module, click Microservice Gateway in the left navigation bar, and click the name of the target gateway. Click `Monitoring Alarms` in the left navigation bar, and click the `Business Monitoring View Version` tab to view business monitoring information. You can select the monitoring time window and refresh cycle in the upper right corner of the page.",
Which tool is used to monitor the microservice gateway?,The microservices gateway is monitored through built-in Grafana dashboards.,
How does the microservice gateway view monitoring alarm information?,"1. Enter the microservice engine module, click Microservice Gateway in the left navigation bar, and click the name of the target gateway. \n2. Click `Monitoring Alarms` in the left navigation bar, and click the `Business Monitoring View Version` tab to view business monitoring information. You can select the monitoring time window and refresh cycle in the upper right corner of the page. \n3. Click the `Resource View Version` tab to view resource monitoring information.",
How can I use the notation H (hash) to make a regularly scheduled task produce an even load on the system?,"You can use the symbol H with a range. For example, `HH(0-7) * * *` represents a time between 00:00 and 7:59.",
What operators are supported in CRON expressions?,"The supported operators in CRON expressions are `*`, `MN`, `MN/X or */X` and `A,B,...,Z`.",
What are the fields in the syntax of CRON expressions? What is the value range?,"The syntax of CRON expression has five fields: MINUTE, HOUR, DOM, MONTH, and DOW. The value ranges are 0~59, 0~23, 1~31, 1~12 and 0,7 (Sunday)~6 ( Saturday).",
What are the trigger types?,Trigger types include code source triggers and timed triggers.,
Where can I configure pipeline triggers?,"When editing the pipeline, click `Edit Configuration` to configure the build trigger.",
What is a pipeline trigger?,A pipeline trigger is a configuration that automatically triggers the execution of a pipeline periodically.,
How do I install or upgrade the Application Workbench module?,You can execute the following command to install or upgrade:\n```bash\nexport VERSION=**** # Modify to the actual deployed version. \nhelm repo add mspider-release https://release.daocloud.io/chartrepo/amamba\nhelm repo update amamba\nhelm upgrade --install --create-namespace --cleanup-on-fail amamba amamba-release/amamba - n amamba-system --version=${VERSION}\n```,
What is progressive delivery? How to implement progressive delivery in Application Workbench?,"Progressive delivery is the practice of gradually exposing new versions of an application to an initially small, and then gradually larger, subset of users to mitigate the risk of negative impacts (such as bugs). The application workbench integrates the progressive delivery component Argo Rollout to support grayscale release and achieve progressive delivery.",
What are the continuous integration engines in Application Workbench?,The application workbench supports Jenkins and Tekton dual pipeline engine systems.,
What types of cloud-native applications does Application Workbench support?,"The application workbench supports ""multi-form"" cloud native applications in cloud native scenarios, including Kubernetes native applications, Helm applications, OAM applications, etc.",
What is Application Workbench?,"The application workbench is a container-based DevOps cloud-native application platform that provides a unified entrance for DCE 5.0 application creation. It can create multiple pipelines, GitOps, canary, blue-green, AB and other progressive release strategies and project management through interface forms. , tool chain integration and other functions.",
How does Argo CD automatically detect changes in Kubernetes YAML files and automatically push them to the cluster?,Argo CD cooperates with the merge request function of the code warehouse to automatically push changes to Kubernetes YAML files to the cluster. The entire process does not require learning Kubernetes release commands or directly operating the cluster.,
What open source software can be used to achieve continuous application delivery under Kubernetes?,The open source software Argo CD can be used to achieve continuous delivery of applications under Kubernetes.,
What is GitOps?,GitOps is an operations model that manages the entire system by recording infrastructure changes into Git and using Git as the single source of truth. It ensures repeatability and auditability by using code as a single source of system configuration.,
What are the advantages of continuous delivery?,"Continuous delivery has the characteristics of standardization and automation, which can well solve the problem that complex business systems require a lot of manpower and time in various stages from project creation, compilation, construction, self-verification, integration verification, production-like verification, and online, and are susceptible to human error. Problems caused by factors.",
What is a continuous delivery pipeline?,"The continuous delivery pipeline is a standardized and automated software release practice that uses code changes as flowing units. Based on the release pipeline, it integrates all functions of development, testing, and operation and maintenance to release software continuously, quickly, and with high reliability.",
"In the graphical pipeline task template, what parameters do the SVN module need to fill in?",You need to fill in the remote svn address and credentials.,
"In the graphical pipeline task template, what should we pay attention to when collecting the test report module?","When collecting test report modules, please note that the report must be in xml format and supports filling in multiple addresses separated by commas.",
What are the parameter settings of the timeout module?,The parameter settings of the timeout module include two options: time and unit.,
What types of voucher steps are there in the graphical pipeline task template?,"There are three types of credentials steps: username and password, access token, and kubeconfig.",
"In the graphical pipeline task template, what parameters do the Git Clone module need to fill in?","You need to fill in the code warehouse address, branch and credentials.",
How to view the results of manual synchronization?,"After the manual synchronization operation is successful, you can check the synchronization results.",
What happens if some resources are deleted from the manifest file during manual synchronization?,"If the cleanup option is checked, the deleted resources in the manifest file will also be cleaned up during manual synchronization; otherwise, these resources will be skipped.",
What parameters need to be configured when manually synchronizing applications?,"In the `Sync Application` page, you need to configure the following parameters:\n- Branch/Tag\n- Cleanup\n- Trial run\n- Apply only\n- Forced application\n- Synchronization settings",
How to manually synchronize a continuously deployed application?,"In the `Application Workbench` -> `Continuous Release` page, click an application name with a synchronization status of `Not Synced` to enter the application details page, click the `Sync` button, and follow the page prompts to configure parameters and synchronize operations.",
How to choose a voucher?,"If using a private repository, you will need to [Create Credentials](../credential.md) when creating the pipeline and select the credential here.",
How to choose a branch?,"When creating a pipeline, you need to enter the branch name, that is, which branch of code is based on which the pipeline is built.",
How to choose the code warehouse address?,"When creating a pipeline, you need to enter the address of the remote code warehouse as the code warehouse address.",
How to create a pipeline based on built-in templates?,"The steps are as follows:\n1. Click `Create Pipeline` on the pipeline list page. \n2. In the pop-up dialog box, select `Template Creation` and click `OK`. \n3. Select the appropriate pipeline template and click `Next`. \n4. Refer to [Customized Creation Pipeline] (custom.md) to fill in the pipeline configuration, and then click `Next`. \n5. Fill in the template parameters according to the instructions, and then click `OK`. \n6. After completion of creation, you can view the newly created pipeline in the pipeline list.",
What pipeline templates are built into the Application Workbench module?,"The application workbench module has built-in templates for Golang, Nodejs, and Maven.",
How do administrators assign projects to specific workspaces?,"Under the corresponding workspace, click the Assign Project button and select the project you want to assign. After success, an instance will be generated under the workspace, and the project can be used by the workspace.",
What operations are supported by platform-assigned toolchain instances?,"Only viewing is supported, unintegration and assigning project operations are not supported.",
How to unbind between a workspace-assigned toolchain instance and a project?,"Under the corresponding instance, click the Unbind button.",
What operations are supported by a workspace-assigned toolchain instance?,Supports de-integration operations and binding projects under the instance to the current workspace for use. Bound projects also support unbinding.,
"In the management tool chain instance, which two tool chains can be managed?",Workspace integrated toolchain and administrator integrated toolchain.,
What new features are added in Application Workbench v0.15.1?,"- Container configuration supports more options (life cycle, environment variables and data storage)\n- jira tool chain integration interface, including CRUD of jira instance and list of instance projects\n- Defect list supports fuzzy query, type, status, priority Level query\n- The list/watch mechanism for ghippo's SMTP configuration supports the email notification function of jenkins\n- Allows the creation of custom roles\n- API supports tool chain integration to add jenkins types, add access to jenkins lists and details, etc. .",
What new features are added in Application Workbench v0.16.1?,"Separate observability-related configurations when creating applications, including indicator monitoring, link tracking and JVM monitoring\n- Integrated tool chain supports Jenkins\n- Supports the creation and management of multi-branch pipelines\n- Supports administrator perspective Toolchain integration function\n- API supports blue and green releases, including creation, deletion, rollback, upgrade, details, etc.",
 How to check the vulnerability information of an image?,Visit `Project` → `Image Warehouse` in order in Harbor to view the vulnerability information of the image.,
 How to configure the pipeline in the application workbench?,"In the application workbench, create a pipeline and execute it. Enter the image warehouse address in the project configured by Harbor above and wait for the pipeline to execute successfully.",
 How to enable automatic image scanning in Harbor?,"In Harbor, after logging in, select a specific project, enter the `Configuration Management` tab, and check `Automatically scan images`.",
 This article describes how to integrate which tool in the pipeline and implement image security scanning?,This article describes how to integrate the Harbor tool in the pipeline and implement image security scanning.,
What are the values of waitForQualityGate abortPipeline?,"The value can be true or false. If true, it means that the pipeline will be stopped if the access control quality check is not passed; if it is false, it means that the pipeline can continue even if the access control quality check is not passed.",
How to view the code scanning results of SonarQube?,"After the pipeline runs successfully, go to Sonar",
"What do the registry, project, and name parameters in this code mean?",registry represents the image warehouse address used; project represents the project name used; name represents the name of the image being built.,
What pipeline agents and stages are used in this code?,"Used go agent, stages include git clone, unit test, Sonar",
How to go to SonarQube to view code scan results?,"After waiting for the pipeline to run successfully, go to Sonar",
How to edit Jenkinsfile?,"The steps to edit Jenkinsfile are as follows:\n1. Click on a pipeline to enter its details page, click `...` -> `Edit Jenkinsfile` in the upper right corner;\n2. Copy and paste the given YAML code into the Jenkinsfile.",
How to create a pipeline?,"The steps to create a pipeline are as follows:\n1. On the pipeline page, click `Create Pipeline`;\n2. Select `Custom creation`;\n3. Enter a name, use default values for other values, and click `OK`.",
How to deploy SonarQube? What are the steps?,Deploy Sonar,
What is SonarQube? What does it do?,Sonar,
How to configure Jenkins to implement 100 pipelines in parallel?,master configuration:\n```yaml\nresources:\nrequests:\ncpu: "2"\nmemory: "4Gi"\nlimits:\ncpu: "2"\nmemory: "8Gi"\nJavaOpts: |-\n -XX:MaxRAMPercentage=70.0 \n-XX:MaxRAM=6g\n-Dhudson.slaves.NodeProvisioner.initialDelay=20\n-Dhudson.slaves.NodeProvisioner.MARGIN=50\n-Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 \n-Dhudson.model.LoadStatistics.clock=5000\n-Dhudson.model.LoadStatistics.decay=0.2\n-Dhudson.slaves.NodeProvisioner.recurrencePeriod=5000\n-Dhudson.security.csrf.DefaultCrumbIssuer.EXCLUDE_SESSION_ID=true \n-Dio.jenkins.plugins.casc.ConfigurationAsCode.initialDelay=10000\n-Djenkins.install.runSetupWizard=false\n-XX:+UseG1GC\n-XX:+UseStringDeduplication\n-XX:+ParallelRefProcEnabled\n- XX:+DisableExplicitGC\n-XX:+UnlockDiagnosticVMOptions\n-XX:+UnlockExperimentalVMOptions\n-javaagent:/otel-auto-instrumentation/javaagent.jar\n``\n- agent configuration: refer to,
How to configure Jenkins to implement 50 pipelines in parallel?,master configuration:\n```yaml\nresources:\nrequests:\ncpu: "1"\nmemory: "2Gi"\nlimits:\ncpu: "2"\nmemory: "4Gi"\nJavaOpts: |-\n -XX:MaxRAMPercentage=70.0 \n-XX:MaxRAM=3g\n-Dhudson.slaves.NodeProvisioner.initialDelay=20\n-Dhudson.slaves.NodeProvisioner.MARGIN=50\n-Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 \n-Dhudson.model.LoadStatistics.clock=5000\n-Dhudson.model.LoadStatistics.decay=0.2\n-Dhudson.slaves.NodeProvisioner.recurrencePeriod=5000\n-Dhudson.security.csrf.DefaultCrumbIssuer.EXCLUDE_SESSION_ID=true \n-Dio.jenkins.plugins.casc.ConfigurationAsCode.initialDelay=10000\n-Djenkins.install.runSetupWizard=false\n-XX:+UseG1GC\n-XX:+UseStringDeduplication\n-XX:+ParallelRefProcEnabled\n- XX:+DisableExplicitGC\n-XX:+UnlockDiagnosticVMOptions\n-XX:+UnlockExperimentalVMOptions\n-javaagent:/otel-auto-instrumentation/javaagent.jar\n````\n- agent configuration:\n````yaml \nresources:\nrequests:\ncpu: "100m"\nmemory: "256Mi"\nlimits:\ncpu: "100m"\nmemory: "128Mi"\n````,
What part of the Jenkins pipeline works?,All pipeline work is done on the Jenkins agent.,
Introducing the master and agent of Jenkins.,"Jenkins is divided into master and agent. The master is mainly used to store configuration, plug-ins and coordination. The agent and master communicate through the jnlp container in the agent Pod. The work of the pipeline is all done on the agent, which is a large consumer of resources.",
Which configuration file was used when upgrading?,The amamba.bak.yaml configuration file was used during the upgrade.,
"When upgrading, which namespace does the command specify?",This command specifies the amamba-system namespace.,
Which application will be updated by this upgrade operation?,The upgrade operation updates the application named amamba.,
What is this code used for?,This code is used to perform Helm upgrade operations.,
How to upgrade application workbench?,Execute the following command to upgrade: `helm upgrade amamba . -n amamba-system -f ./amamba.bak.yaml`.,
How to backup old version of `--set` parameter?,"Before upgrading the global management version, it is recommended to execute the following command to back up the `--set` parameter of the old version: `helm get values ghippo -n ghippo-system -o yaml > amamba.bak.yaml`.",
How to check if the Helm repository of the application workbench exists?,"Execute the command `helm repo list | grep amamba`. If the return result is empty or the `Error: no repositories to show` prompt appears, you need to execute the following command to add the Helm repository of the application workbench: `helm repo add amamba http:// {harbor url}/chartrepo/{project}`.",
How to load offline upgrade image from local to Docker or containerd?,"Execute the following command to decompress the image: `tar xvf amamba.bundle.tar`; then execute the following command to load the image locally to Docker or containerd: `docker load -i images.tar # for Docker; ctr image import images.tar # for containerd `. The above operations need to be performed on each node in the cluster. After loading is complete, you need to label the image to ensure that the Registry and Repository are consistent with those during installation.",
How to synchronize offline upgrade images?,1. Create the load-image.yaml file; 2. Execute the following command: `charts-syncer sync --config load-image.yaml`.,
How to load offline upgrade image?,Supports loading images in two ways: synchronizing images through charts-syncer or loading images directly through Docker or containerd.,
What operation permissions does the Workspace Viewer role have?,"The Workspace Viewer role can view the contents of menu objects such as applications, namespaces, pipelines, credentials, continuous deployment, code warehouses, and grayscale releases, but cannot create, edit, or delete operations.",
What three user roles does Application Workbench support? What permissions does each have?,"The application workbench supports three user roles: Workspace Admin, Workspace Editor, and Workspace Viewer. Please refer to the following table for specific permissions:\n| Menu Object | Operation | Workspace Admin | Workspace Editor | Workspace Viewer |\n| -------- | -------------- -------------------------- | --------------- | ------------- --- | ------------- |\n| Apps | View app list | ✅ | ✅ | ✅ |\n| ...(part omitted) |",
"When running a pipeline, how to check if the pipeline is running on the specified node?",Check the node where the Pod that performs the task runs in the container management module.,
How to access Jenkins Dashboard?,"First, you need to expose the Jenkins Dashboard through NodePort. Then click the link on the details page to access Jenkins Dashboard, and enter your account/password (default is admin/Admin01) to log in.",
"When labeling a worker node, which menu should I choose?","Enter the details page of the target cluster in the container management module, click `Node Management` in the left navigation, select the target working node, and click `Modify Label`.",
"When modifying the configuration file jenkins-casc-config, where should nodeSelector be added?",Add nodeSelector: "ci=base" for a specific Agent under the jenkins.cloud.kubernetes.templates location in the YAML configuration item jenkins.yaml.,
This article describes how to run the customer's pipeline tasks on specified nodes in the application workbench. What are the specific steps?,"1. Enter the details page of the target cluster in the container management module, search for jenkins-casc-config, and add nodeSelector: ""ci=base"" for a specific Agent;\n2. Enter the details page of the target cluster in the container management module , select the target working node and add the ci=base tag;\n3. Access Jenkins Dashbord and reload the configuration;\n4. Run the pipeline and view the node where the Pod that performs the task runs in the container management module.",
What issues are fixed in this version?,"This version fixes the problem that the admin user does not authenticate the deployment target (cluster/namespace), the error that the gitops application creation time, synchronization start time and synchronization end time are `Invalida date`, the error in obtaining the nacos registration center list data, After the cluster is unbound and rebound, the cluster and namespace in the destination in ArgoCD are lost, as well as multiple other issues.",
What optimizations are included in this version?,"This version optimizes the application access service grid process and the performance of obtaining rollout image list, application group list, native application list, etc.",
What features are new in this version?,"This version adds the warehouse function in gitops, which supports import, deletion, and synchronization functions of gitops applications.",
How to run a pipeline containing custom images?,"When creating a pipeline, select Kubernetes as the Agent type and specify the custom image name in the YAML file to run a pipeline containing the custom image. You can refer to the examples given in the text.",
How to add a custom toolchain using Build a Custom Image?,"Custom toolchains can be packaged into images through BYOI (Build Your Own Image). When creating the pipeline, select Kubernetes as the Agent type, and specify to use a custom image. You can refer to the examples given in the text.",
How to add a custom toolchain using Volume mounting?,"Use the `init` container and `volumeMount` to copy the tool to the agent container, and modify the configmap of Jenkins Casc to modify the default behavior of the container. You can refer to the examples given in the text.",
What two methods does Application Workbench support for adding custom toolchains?,The application workbench supports adding custom tool chains through Volume mounting and building custom images.,
Why does the application workbench support the integration of user-defined tools or specific versions of tools?,"The application workbench supports the integration of user-defined tools or specific versions of tools. It can be used to upgrade/downgrade to a specific version to fix bugs in the tools, install tools that the pipeline runs on, or prepare code-dependent packages in advance to speed up compilation.",
In what ways can the build pipeline be triggered?,Code source trigger and timing trigger.,
"In addition to basic information and code warehouse information, what other information can be filled in?","Build settings and build parameters, build triggers.",
What conditions need to be met for the name of an application workbench pipeline?,Pipeline names must be unique in the same workspace.,
What are the prerequisites for creating an application workbench pipeline?,"- Create workspace and create users. \n- Add this user to the workspace and grant `workspace editor` or higher privileges. \n- Provide a code repository, and the source code of the code repository has a Jenkinsfile text file. \n- If it is a private warehouse, you need to create warehouse access credentials in advance.",
Based on what file is the application workbench pipeline created?,Create an application workbench pipeline based on Jenkinsfile.,
How to cancel an executing pipeline?,"On the pipeline details page, the executing pipeline can be canceled based on the `execution ID` in the pipeline running record. Just select Cancel in the `︙` menu.",
How to rerun an already run pipeline?,"On the pipeline details page, you can rerun the pipeline that has been run based on the `execution ID` in the pipeline running record. Just select Rerun in the `︙` menu.",
How can I execute the pipeline immediately?,"Select a pipeline on the pipeline list page, click `︙`, and click `Execute Now` in the pop-up menu. Depending on whether the pipeline is configured with `build parameters`, the parameter configuration dialog box will appear after execution or the execution will start directly.",
What is a manual execution pipeline?,"Manual execution of the pipeline refers to the manual execution of the pipeline on the graphical interface, including immediate execution, re-run and cancellation.",
Is it possible to manually trigger a code repository scan?,"Yes, it is possible to manually trigger the `Scan repository` operation to check for updates.",
How to trigger the execution of the pipeline corresponding to the branch that meets the conditions?,"After the creation is completed, the corresponding pipeline that executes the branch that meets the conditions will be automatically triggered.",
How to set up multi-branch configurations such as branch policies and scan triggers?,"When creating a multi-branch pipeline, you can refer to the instructions to fill in the branch discovery strategy, scan trigger, branch settings, and clone configuration information.",
What is the basic information for creating a multi-branch pipeline?,"Name, description information, code warehouse address, credentials, script path.",
What are the prerequisites for creating a multi-branch pipeline?,"- Create workspace and create users. \n- Add this user to the workspace and grant `workspace editor` or higher privileges. \n- Provide a code repository, and the source code of the code repository has multiple branches, and all have Jenkinsfile text files. \n- If it is a private warehouse, you need to create warehouse access credentials in advance.",
How to manage namespace quotas?,"- Select a namespace on the namespace list page and click `Quota Management`. \n- In the pop-up `Resource Quota` dialog box, you can see the resource quota information of the current namespace. \n- Click `Add` under `Application Resources`, select a resource and set the quota. \n- Click `OK` to complete the quota settings.",
How to create a namespace?,"- Click `Namespace` in the left navigation bar to enter the namespace list and click `Create` in the upper right corner. \n- Configure the basic information of the namespace, including name, cluster, label, etc. \n- Configure resource quotas for the namespace. \n- Click `OK` to complete resource creation.",
What are the prerequisites for creating a namespace?,Creating a namespace requires that the current workspace already has cluster resources and the current user is authorized to the Workspace Admin role.,
What is the role of namespace?,"Namespace is an abstraction used to isolate resources in Kubernetes. Some resources can be divided into different namespaces to achieve resource isolation, usage management and other purposes.",
How to delete a code repository?,"Select a code repository on the code repository list page, click ︙, click Delete in the pop-up menu, and click OK in the secondary confirmation pop-up window.",
How to import a warehouse using SSH?,"On the Application Workbench -> GitOps -> Warehouse page, click the Import Warehouse button, choose to use SSH, configure the relevant parameters and click OK.",
How to import a warehouse using HTTPS?,"On the Application Workbench -> GitOps -> Warehouse page, click the Import Warehouse button, choose to use HTTPS, configure the relevant parameters and click OK.",
What conditions need to be met to import a warehouse?,"You need to create a workspace and a user, add the user to the workspace and assign the workspace edit role; prepare a Git repository.",
How to import the warehouse?,"In the Application Workbench -> GitOps -> Warehouse page, click the Import Warehouse button and choose to import using HTTPS or SSH.",
"In the template file example, how to use the image warehouse credentials when pushing the Docker image?","Use the `withCredentials` function to inject credentials. The sample code is as follows:\n```\n{{- if .params.registryCredential }}\nwithCredentials([usernamePassword(credentialsId: ""{{ .params.registryCredential }}"", passwordVariable : ""PASS"", usernameVariable: ""USER"",)]) {\nsh ""docker login {{ .params.image }} -u $USER -p $PASS""\nsh ""docker push $IMG""\n}\n {{- else }}\nsh ""docker push $IMG""\n{{- end }}\n```",
How to define parameters of dropdown list type?,"Parameters of drop-down list type need to specify a default value list in the `default` field, for example:\n```\ntype: choice\ndefault:\n- choice 1\n- choice 2\n```",
What parameter types can be defined in the `parameterDefinitions` area?,"Supports multiple parameter types such as Boolean values, drop-down lists, credentials, passwords, and text.",
What is a pipeline template file? What two parts does it consist of?,The pipeline template file is a template file that defines the Jenkins pipeline and consists of two parts: `parameterDefinitions` and `jenkinsfileTemplate`.,
Which cluster is recommended for installing Jenkins?,It is recommended to deploy Jenkins in a non-global service cluster.,
What is the default account password for Jenkins deployed through Helm?,The default account password for Jenkins deployed through Helm is `admin/Admin01`.,
"If the Jenkins address is https protocol, what certificate does it need to provide?","If the Jenkins address is https protocol, a certificate is required.",
How to integrate Jenkins installed through DCE 5.0 platform?,"In the application workbench, click `Toolchain Integration` under Platform Management in the left navigation bar, select the toolchain type as `Jenkins`, and fill in the relevant information to complete the integration.",
How to check the deployment results of Jenkins?,You can go to the Helm application to view the deployment results.,
What configuration information do I need to fill in when installing Jenkins?,"It is necessary to fill in the AdminUser, AdminPassword, Deploy.JenkinsHost, JavaOpts, ServiceType, ServicePort, NodePort, resources.requests, resources.limits, image.registry and other parameters.",
How to choose the version to install Jenkins?,Select the version you want to install in `Helm Application` -> `Helm Template`.,
What should you pay attention to if you install Jenkins in a global service cluster?,"When installing Jenkins in the global service cluster, you need to make sure to select the `amamba-jenkins` instance under the `amamba-system` namespace in `Container Management->helm Application`.",
In which cluster do I need to deploy Jenkins?,Select the cluster where Jenkins needs to be installed based on the actual situation.,
In which namespace should I install Jenkins?,"When installing Jenkins, make sure you do it under the `amamba-system` namespace.",
"After integrating Jenkins, which capabilities of the application workbench will be enhanced?","After integrating Jenkins, all workspaces will gain pipeline capabilities for construction.",
What can the application workbench do after integrating Jira?,"By integrating Jira in the application workbench, tracking of Jira->Issue can be supported.",
"After integrating the GitLab repository, in which function can the application workbench be used?","After integrating the GitLab repository, the application workbench can be used in the pipeline.",
What toolchain types does Application Workbench support for integration? What are the differences?,"Application Workbench supports integration with GitLab, Jira and Jenkins.",
Which two perspectives of tool chain integration are supported by Application Workbench? What are the differences?,Application Workbench supports workspace integration and administrator integration. The administrator integration can be assigned to the workspace for use.,
How to configure health checks for containers?,Configure it in the "Health Check" option in the container configuration.,
How to label the target image?,Just set it in the "Tag" option in the configuration pipeline.,
How to set the number of Pods for an application?,Just set it in the "Number of instances" option in the basic information.,
"When creating an app, are there any restrictions on names? How to name it?","The name can be up to 63 characters long, can only contain lowercase letters, numbers and the delimiter ""-"", and must start and end with a lowercase letter or number. It can be named according to the actual situation.",
How to deploy Java application from Jar file in Application Workbench?,"You can select ""Build based on Jar package"" through the wizard, fill in the basic information, configure the pipeline, fill in the container configuration, choose whether to enable advanced features, and upload the Jar file. You can deploy the Java application after the pipeline is successfully executed.",
How to verify cluster resources?,"When each stage of the pipeline runs successfully, the resources will be automatically deployed to the cluster, and the deployment results can be verified through relevant commands and tools.",
How to run the pipeline?,"Click ""Execute Now"" on the pipeline details page, then set the three string parameters defined in the prerequisites in the pop-up dialog box and click ""OK"".",
How to build and push an image?,"In the graphical editing page, first create a new stage named ""build & push"". Under this stage, add the specified container, use credentials, perform code construction, build the Docker image according to the Dockerfile, log in to the image warehouse, and push the image. to the warehouse and other steps.",
How to add a "Unit Test" phase?,"In the graphical editing page, click ""Add Stage"", set the name to ""unit test"", then add a step and select the type as ""shell"", and fill in the command to execute the unit test.",
How to add a "pull source" stage?,"In the graphical editing page, click ""Add Phase"", set the name to ""git clone"", then add a step and select the type as ""git clone"", and fill in the relevant parameters.",
How do I configure global settings?,"Click ""Global Settings"" on the graphical editing page, select the node type as node in the drop-down list, and the label as go 16.",
How to edit the stages of a custom pipeline?,"Execution stages can be defined for the pipeline by editing the Jenkinsfile or through a graphical interface form. On the graphical editing page, you can edit the stages of the custom pipeline by adding stages and steps.",
How to solve the error problem when the GitOps module adds a GitHub warehouse?,Use SSH to import the **GitHub** repository.,
What is the reason for the error when adding a GitHub repository under the GitOps module?,"Since GitHub has removed support for username/password, importing the **GitHub** repository via HTTP will fail.",
How to solve the error problem when executing the pipeline?,"In the Jenkinsfile of the pipeline, change the deployment command from `kubectl apply -f` to `kubectl apply -f . --request-timeout=30m`",
Why do I get an error when executing the pipeline? What is the error message?,"When the cluster where Jenkins is located and the application deployment cluster cross data centers, the network communication delay will become high, and the following error message may appear:\n```bash\nE0113 01:47:27.690555 50 request.go:1058] Unexpected error when reading response body: net/http: request canceled (Client.Timeout or context cancellation while reading body)\nerror: unexpected error when reading response body. Please retry. Original error: net/http: request canceled (Client.Timeout or context cancellation while reading body)\n```",
What advanced publishing strategies does grayscale publishing support?,"Grayscale release supports canary release, blue-green deployment, and A/B Testing advanced release strategies.",
What concept is based on continuous deployment?,Continuous deployment is implemented based on the GitOps concept and is used to control the application release and deployment delivery process after code construction.,
What four modes does pipeline orchestration support for creating pipelines?,"Pipeline orchestration supports four modes of creating pipelines: custom creation, creation based on jenkinsfile, creation based on templates, and creation of multi-branch pipelines.",
What forms of cloud native applications does application management support?,"Application management supports ""multi-form"" cloud native applications in cloud native scenarios, including Kubernetes native applications, Helm applications, OAM applications, etc.",
What features does the Application Workbench offer?,"The application workbench provides functions such as application management, pipeline orchestration, credential management, continuous deployment, warehouse management, and grayscale release.",
What should I do if I need to update or delete a Helm application that is already installed in the current cluster?,"You need to click on the application name to jump to the container management module, where you can update, delete and perform more operations on the application details page.",
How to check the Helm applications installed in the current cluster?,Check it out in the list of Helm apps under the overview page. Click the application name to jump to the container management module to view application details.,
What are the steps to deploy an application using a Helm template?,"1. After entering the application workbench module, click `Wizards` in the left navigation bar, and then select `Based on Helm template`. \n2. Select the cluster in which the application needs to be deployed at the top of the page, and then click the Helm Chart card that needs to be deployed, such as ""docker-registry"". \n3. Read the installation prerequisites, parameter configuration instructions and other information of the application, select the version to be installed in the upper right corner, and then click `Install`. \n4. Set basic information such as application name and namespace, then configure parameters through the form or YAML below, and finally click OK at the bottom of the page. \n5. The page automatically jumps to the Helm application list under the overview page, where you can view the Helm applications installed in the current cluster.",
What are the prerequisites for Helm template deployment application? How to meet these conditions?,"The prerequisite is to create a workspace and a user. The user needs to join the workspace and be assigned the `workspace edit` role. You can refer to [Creating a workspace](../../../ghippo/user-guide/workspace/workspace.md), [Users and roles](../../../ghippo/user-guide /access-control/user.md) to satisfy these conditions.",
How to view synchronization results?,"After the synchronization is successful, you can view the synchronization results.",
"After creating the Argo CD application, how do I synchronize it?","You need to synchronize manually, click the ""Sync"" button.",
How to view created Argo CD applications?,"After ""After successful creation, click the application name to enter the details page and view the application details.""",
What are the prerequisites for creating an Argo CD application?,"Prerequisites include creating a workspace and a user. The user needs to join the workspace and be assigned the `workspace edit` role, and a Git repository needs to be prepared.",
What is Argo CD?,Argo CD is an open source software used to implement continuous deployment.,
How to create and manage credentials?,"The steps to create and manage credentials are as follows:\n1. Click `Pipeline`->`Credentials` in the left navigation bar, enter the voucher list, and click `New Credential` in the upper right corner. \n2. Configure the relevant parameters on the `Create Credentials` page and click `OK`. Including filling in the `certificate name`, selecting the type of voucher to be added, filling in the corresponding fields according to different types, etc. \n3. The screen prompts that the creation is successful, and the newly created credentials are placed first by default. \n4. Click `︙` on the right side of the list to select edit or delete operations in the pop-up menu. Note that deleting the credentials used by a certain pipeline may affect access to the pipeline, so please operate with caution.",
What three types of credentials can be created in Application Workbench?,"The following three types of credentials can be created in the application workbench:\n- Username and password: used to store authentication information for usernames and passwords. If third-party websites or applications support username/password access, you can choose this Types of accounts, such as GitHub, GitLab, and Docker Hub. \n- Access token (Secret text): token such as API token (such as GitHub personal access token). \n- kubeconfig: used to configure cross-cluster authentication.",
What is the role of the voucher?,"Credentials can save sensitive information, such as username and password, access token, Kubeconfig, etc., making the saved data more secure and flexible and preventing it from being exposed to the image. When the pipeline needs to interact with third-party websites or applications, it needs to provide corresponding credentials.",
How to modify or delete a custom pipeline template?,Return to the custom pipeline template list page and click on the corresponding template card to modify or delete it.,
How should I fill in the template file for a custom pipeline template?,Refer to [Pipeline Template File] (info.md) to fill in or paste the parameters prepared in advance.,
How to create a custom pipeline template?,"Click `Create Template` on the `Customized Pipeline Template` page, refer to the page prompts to fill in the basic information, including template name, description information, template file, and finally click OK.",
What are the prerequisites for customizing pipeline templates?,"Create a workspace, create a user, add the user to the workspace, grant `workspace editor` or higher permissions, and configure the pipeline template file.",
What build parameter types can be added when customizing a pipeline creation?,"Five types can be added: boolean, string, multiline text, options, password, and uploaded file.",
What is the final step in custom creation pipeline?,"After confirming that all parameters have been entered, click the OK button to complete the custom pipeline creation and automatically return to the pipeline list. Click ︙ on the right side of the list to perform various operations.",
What is the first step in creating a custom pipeline?,Click Create Pipeline on the pipeline list page.,
"After a custom pipeline is created, what steps are required to run the pipeline?","Each stage of the pipeline needs to be manually defined (i.e., edited) before the pipeline can be run. If you define a process and run the pipeline directly, a build failure error will occur.",
What are the prerequisites for creating a custom pipeline?,#NAME?,
How to save and execute pipeline?,"1. After completing editing, click `Save and Execute`.\n2. Enter the sample parameters in step 2 in the pop-up dialog box to successfully run the pipeline.",
How to edit a pipeline?,"1. Click the name of an assembly line on the assembly line list page. \n2. Click `Edit Pipeline` in the upper right corner.\n3. Click `Global Settings` in the upper right corner.\n4. Set the type to node, and set the label to go, click `OK`.\n5. Add stage - pull Get the source code; add a stage - build and push the image; add a stage - deploy to the cluster.",
What are the steps to create a credential?,"1. Create two credentials on the `Credentials` page: docker-credential and demo-dev-kubeconfig. \n2. After the creation is completed, the voucher information can be seen on the `Certificate List` page.",
What are the prerequisites to quickly create a pipeline?,"You need to create a workspace and a user, and give the user the `workspace edit` role; you need to create two credentials that can access the mirror warehouse and the cluster; you need to prepare a GitHub warehouse and a DockerHub warehouse.",
"After the publishing task is successfully created, how do I view the task list?","After the creation is successful, the system will automatically jump to the task list page published by Grayscale.",
"After the publishing task is successfully created, how to set the image address of the grayscale version?","In the page that pops up after clicking ""Create and Update Application"", set the image address of the grayscale version and click ""OK"".",
What will happen if the waiting time after setting the arrival traffic ratio is 0 or not filled in?,The publishing task will be permanently suspended when it reaches the step of setting publishing rules.,
What are the configuration items of the traffic scheduling policy?,"The number of instances, the proportion of published traffic in the current stage, the waiting time after reaching the traffic proportion, and monitoring analysis.",
What basic information needs to be filled in the canary publishing task?,"Publish task name, cluster, namespace and stateless workload.",
What prerequisites need to be met before creating a canary release task?,"1. Create a workspace and a user, add the user to the workspace and assign the Workspace Editor role. \n2. Create an application and enable grayscale publishing. \n3. The cluster where the publishing object is located has already installed the two components Istiod and Argo Rollout.",
What does grayscale publishing mean?,"Grayscale release is to release a new application version without affecting the old version. According to the predefined rules, traffic is gradually switched to the new version. When the new version runs without problems, all traffic is automatically migrated from the old version to the new version.",
Which open source project is the application workbench based on to provide grayscale publishing capabilities?,The application workbench provides grayscale publishing capabilities based on the open source project Argo Rollout.,
How to roll back to a previous version?,"On the `Grayscale Release Task` details page, select the historical version and click Rollback to roll back to a previous version.",
How to modify the traffic scheduling policy of the canary release process?,"On the `Grayscale Release Task` details page, click `ⵗ` in the upper right corner of the page and select `Update Release Task`, adjust the release rules and click OK to modify.",
How to update a canary released version?,"Click the name of the target task, and then click `Update Version` in the upper right corner. After setting the canary release image, the application will automatically trigger a new grayscale release process.",
How to view the details of a Canary release task?,"Enter the `Application Workbench` module, click `Grayscale Release` in the left navigation bar, and click the name of the target task to view the Grayscale Release task details page.",
What are the related operations for canary publishing tasks?,"View task details, update versions, update release tasks, rollback versions, and other instructions.",
What is a step list?,"The step list describes what specifically needs to be done in a stage and what specific commands need to be executed. For example, there is a step that requires the system to print a message that ""Building..."", that is, execute the command echo ""Building..."".",
How to declare parallel stages?,Parallel is used to declare some stages of parallel execution. It is usually suitable for speeding up execution when there is no dependency between stages.,
What are stages?,"A phase defines a series of closely related steps. Each stage has independent and clear responsibilities in the entire pipeline, such as the ""Build"", ""Test"" or ""Deploy"" stage. Generally speaking, all the actual building process is placed in stages.",
What is Agent?,"Agent describes the entire pipeline execution process or the execution environment of a certain stage, and must appear in the top grid of the description file or in each stage.",
What is an assembly line?,"The pipeline is a user-defined working model. The code of the pipeline defines the complete process of software delivery, which generally includes the stages of building, testing and delivering applications.",
Why integrate DevOps toolchain? What are the benefits?,"Integrating a DevOps tool chain eliminates the need to log into multiple platforms and deal with the difficulty of not having a unified view across different tools, allowing teams to bring existing tools they already know and use into the application workbench.",
What is grayscale publishing? What are the functions?,"Grayscale publishing is a tool that helps users update applications progressively. It realizes the coexistence of multiple versions, release pause, traffic percentage switching and other functions, and fully automatically realizes online grayscale traffic switching.",
What is GitOps? What is its core idea?,"The core idea of GitOps is to use a Git repository that contains a declarative description of the current desired (production environment infrastructure), and to ensure that the production environment is consistent with the desired state in the repository through automated processes. If you need to deploy a new application or update an existing application, you only need to update the corresponding repository, and the automated process will handle the rest, just like using cruise control to manage the application in production.",
What is a voucher? Where can I configure credentials?,Credentials are the configuration required by the pipeline to interact with third-party applications. You can [Configure Credentials](../user-guide/pipeline/credential.md) in Jenkins.,
What is an assembly line? What functions does it have?,"The pipeline provides a visual, customizable automatic delivery pipeline to help companies shorten delivery cycles and improve delivery efficiency. The current pipeline is implemented based on Jenkins.",
What is a namespace? what's the effect?,"In the application workbench, the namespace is a smaller resource space that is isolated from each other under the workspace. It is also the workspace where users implement job production. Multiple namespaces can be created under one workspace, and the total resource quota that can be occupied cannot exceed the workspace quota. The namespace not only divides resource quotas in a more fine-grained manner, but also limits the size (CPU, memory) of containers under the namespace, effectively improving resource utilization.",
What is Application Workbench? what's the effect?,"The application workbench is the unified entrance for DCE 5.0 application deployment, which lowers the threshold for enterprises to use cloud native applications and improves the efficiency from enterprise software development to application delivery. It can coordinate the permission relationship between global management and sub-modules, and solve resource aggregation and mapping hierarchical relationships.",
What microservice governance functions can be enabled in advanced configuration?,"Monitoring, logging and link tracking functions can be turned on.",
What information needs to be filled in to create a microservice application based on Git?,"You need to fill in the Git warehouse address, branch, credentials for accessing the code warehouse, Dockerfile path, image warehouse name and other information.",
How to create a microservice application based on Git?,"You need to click ""Build based on Git repository"" in the ""Application Workbench"" -> ""Wizard"" page, fill in the basic information, pipeline configuration, container configuration and advanced configuration, and finally click OK.",
How to create credentials?,"Credentials can be created on the ""Credentials"" page, including credentials for accessing the code warehouse and credentials for accessing the image warehouse.",
What methods does the Application Workbench support for building applications?,"Supports four ways to build applications through Git repository, Jar package, container image, and Helm template.",
How to customize podTemplate?,You can select the Agent of type `kubernetes` in the edit pipeline page and click the YAML editor to customize the YAML statement to define the required environment.,
"When using the built-in Label, how to specify a specific Agent label to use the corresponding podTemplate?","You can use the podTemplate of the corresponding label through `node(""label name"")` in the Jenkinsfile, or select the Agent of type `node` and label as `label name` on the edit pipeline page.",
Which special container does the Kubernetes plugin run in the Jenkins Agent Pod? Why?,The Kubernetes plug-in will run a special container `jnlp` in the Jenkins Agent Pod for the purpose of communicating between the Jenkins Controller and the Jenkins Agent.,
How to extend the running Jenkins Agent in Kubernetes?,You can use the Kubernetes plugin for Jenkins plugin to extend the Jenkins Agent running in Kubernetes.,
What is Agent?,"Agent describes the entire pipeline execution process or the execution environment of a certain stage, and must appear at the top of the description file or at each stage.",
How to determine the health and synchronization status of resources?,"In the Argo CD UI interface, you can view the health and synchronization status of each application. You can also use the command `argocd app get <APPNAME>` to view resource details, including health and synchronization status.",
Under what circumstances will the synchronization status become unsynchronized?,"When the resources deployed in the cluster are inconsistent with the expected status in the warehouse, the synchronization status will be displayed as unsynchronized.",
What are the synchronization statuses?,"The synchronization status in Argo CD is synchronized, unsynchronized and unknown.",
Under what circumstances will the health status be degraded?,"When an error or exception occurs in Argo CD, a resource will have a degraded health status.",
What are the health states in Argo CD?,"The health states of Argo CD are Healthy, Degraded, In Progress, Paused, Unknown and Missing.",
How to trigger CI/CD?,"Select the pipeline created above and click `Run Now`. After successful operation, continuous deployment of the application will be triggered.",
How to create a GitOps application?,"1. Import the argocd-example-apps repository using HTTPS, [reference steps](../user-guide/gitops/import-repo.md)\n2. Create a GitOps application, [reference steps](../user -guide/gitops/create-app.md)",
How to create a pipeline?,You can refer to the [Create Pipeline](../user-guide/pipeline/create/custom.md) mentioned in the article.,
What are the steps involved in the process described in this article?,The main steps of this tutorial are: pull business code → build image → update application configuration file.,
What are the prerequisites mentioned in this article?,"- Prepare two code warehouses, one as a warehouse for storing business code and the other as a warehouse for application configuration files (yaml). \n- Prepare a Harbor mirror repository\n- Prepare credentials to access the above three repositories. The three credentials used in this tutorial are named git-credentials, git-app-credentials, and harbor-credentials.",
How to access website effects?,"The access address is: `http://{istio-ingressgateway LB IP}:8082`, which is the LB IP of Istio's ingressgateway plus the service port (8082).",
What resources need to be created in Istio-related resource configuration?,"Gateway, VirtualService, and AuthorizationPolicy resources need to be created.",
What are the steps of the whole process?,"The whole process is divided into four steps, namely building an application based on the container image, configuring Istio-related resources, creating grayscale publishing tasks and verifying the effect.",
What prerequisites are mentioned in this article?,The images in the example need to access the public network: `argoproj/rollouts-demo:yellow` and `argoproj/rollouts-demo:blue`; only applicable to the DCE 5.0 platform deployed through the installer in metallb mode; using the grayscale publishing capability Istio and Argo Rollout components need to be installed in the cluster.,
This article describes how to implement progressive grayscale release based on what?,This article introduces how to implement progressive grayscale release based on the open source Argo Rollout.,
What is GitOps?,"GitOps refers to providing the declarative GitOps continuous delivery capability based on Kubernetes, through version control applications, thereby achieving automated application deployment and life cycle management.",
What integrated DevOps tools does Application Workbench support?,"The application workbench integrates popular DevOps tools in the community, such as Kubernetes, GitLab, and Sonar",
What is grayscale publishing?,Grayscale release refers to helping users progressively update applications to ensure that new features can be launched smoothly. Supports application release strategies such as canary deployment and blue-green deployment.,
What application deployment methods does Application Workbench support?,The application workbench supports multiple types of application deployment methods.,
What are the advantages of Application Workbench?,"The advantages of the application workbench include enterprise-level CI/CD, cloud native as the base, improved R&D efficiency, grayscale release, GitOps, pipeline as code, comprehensive integration and years of industry experience.",
How to deploy application workbench?,"You can install Workbench through the installation package of [DCE 5.0 Commercial Edition], or you can install or upgrade the application workbench module separately. The specific command is as follows:\n```\nexport VERSION=**** # Modify to the actual deployed version. \nhelm repo add mspider-release https://release.daocloud.io/chartrepo/amamba\nhelm repo update amamba\nhelm upgrade --install --create-namespace --cleanup-on-fail amamba amamba-release/amamba - n amamba-system --version=${VERSION}\n```",
What role does the Application Workbench play in DCE 5.0?,"As a container-based DevOps cloud-native application platform in DCE 5.0, the application workbench provides application creation, multiple pipelines, GitOps, canary, blue-green, AB and other progressive release strategies, project management, tool chain integration, etc. function. It can realize hierarchical resource management with the help of global management, use CI/CD pipeline and GitOps process to add, delete, modify and check cloud native applications to achieve progressive delivery.",
What is progressive delivery? What is the role of Argo Rollout in DCE 5.0?,"Progressive delivery is the practice of gradually exposing new versions of an application to an initially small, and then gradually larger, subset of users to mitigate the risk of negative impacts (such as bugs). Argo-Rollout Kubernetes Progressive Delivery Controller provides more powerful deployment capabilities in DCE 5.0. Including grayscale release, blue-green deployment, update testing (experimentation), progressive delivery (progressive delivery) and other features.",
"How does Application Workbench enable secure, automated and progressive delivery?","The application workbench introduces a concept of continuous deployment for cloud native applications - GitOps, fully embraces GitOps, integrates the progressive delivery component Argo Rollout, and supports grayscale release, thereby improving the stability and efficiency of application delivery.",
How does Application Workbench perform continuous integration?,The application workbench supports Jenkins and Tekton dual pipeline engine systems. It uses a graphical approach to edit the pipeline to achieve a WYSIWYG effect. Applications can be built using code from different sources.,
What cloud-native applications does Application Workbench support?,"The application workbench supports ""multi-form"" cloud native applications in cloud native scenarios, including Kubernetes native applications, Helm applications, OAM applications, etc. It can be connected to microservice applications of SpingCloud, Dubbo, and ServiceMesh frameworks to implement microservice governance and seamlessly integrate with the microservice engine and service grid of DCE 5.0.",
What is Application Workbench?,"The application workbench is a container-based DevOps cloud-native application platform that provides a unified entrance for DCE 5.0 application creation. It can create multiple pipelines, GitOps, canary, blue-green, AB and other progressive release strategies and project management through interface forms. , tool chain integration and other functions. It can effectively help enterprises achieve digital transformation and improve their IT delivery capabilities and competitiveness.",
