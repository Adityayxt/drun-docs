# 参数组

## 创建参数组

1. 点击右上角 **创建** 按钮。

    ![创建参数组](images/create-parameter-groups.png)

2. 填写参数组的基本信息：

    - 参数组名称
    - 选择微调类型：默认设置为 **SFT**
    - 选择参数组的命名空间

    ![参数组基本信息](images/basic-information.png)

3. 填写参数配置：

    - Scheduler：训练中使用的调度器类型，指如何设置学习率的变化方式，默认设置为 **cosine** 。可选择以下三种：
        - **cosine** ：通过绘制余弦曲线，将 **LearningRate** 逐渐减少到0。
        - **linear** :初始 **LearningRate** 为设置值，以直线形式减少到0.
        - **constant** ： **LearningRate** 始终保持不变。  
    - Optimizer：训练中使用的优化器类型，可选择 **adamw_torch** 、 **adamw_hf** 、 **sgd** 、 **adafactor** 或 **adagrad** ，默认设置为 **adamw_torch** 。
    - FP16：选择是否使用16位浮点数进行训练，这可以减少内存占用和加速训练过程，默认为 **False** 。
    - LoRA_Alpha：数值越大，LoRA输出对模型的影响越大。若微调数据量较小，建议调大 **alpha** 数值，反之则调小。默认数值为 **32** 。
    - LoRA_R：数值越大，LoRA的参数越多。若原始模型表现良好，可将 **r** 设置小一点，反之可以设置大一些。默认设置为 **4** 。
    - LoRA_Dropout：训练过程中会以这一概率随机选择忽略神经元以防止过拟合，数值在0到1之间，默认设置为 **0.1** 。
    - Int4/8：可选择 **Int4** 或 **Int8** 形式储存数据。前者储存空间更小，可提高储存效率和计算速率，后者可储存的整数范围更大。
    - LearningRate：每次参数更新的步长，数值越小参数更新越快。默认数值为 **0.001** 。
    - Epochs：对同一组数据重复训练次数。默认数值为 **10** 。
    - BlockSize：模型中块的大小。数值越大，模型占用内存和计算量会越大，模型表现可能会更好。默认为 **512** 。
    - BatchSize：同时学习的批次大小。数值越大，训练速度越快，占用内存越多。默认数值为 **32** 。
    - WarmupRatio：训练过程中预热的比例，数值范围大多在0到1之间。默认为 **0.1** 。
    - WeightDecay：表示模型的正则化程度。数值越小，正则化越低，模型越大可能出现过拟合。默认数值为 **0.0001** 。
    - GradAccSteps：权重更新前积累的梯度数目。在硬件条件不变的情况下，该数值越大，可使用的 **BatchSize** 越大。默认值为 **1** 。
    - TrainerType：训练器类型。默认为 **Standard** 。

    ![参数配置](images/parameter-configuration.png)

4. 点击右下角 **确认** 创建参数组。

## 编辑参数组

1. 可在左上角筛选 **命名空间** ，选择想要查看参数组的命名空间。

2. 找到想要查看的参数组，点击右侧的 **⋮** 按钮，选择 **编辑**。

3. 即可修改参数组内的所有参数配置。

## 删除参数组

找到想要删除的参数组，点击右侧的 **⋮** 按钮，选择 **删除**。
